{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26fdb704170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.manual_seed(189898) # Last 6 digits of my A# without the leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Check your Current Working Directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Batch Size\n",
    "batch_size = 20\n",
    "\n",
    "# Download the MNIST dataset to local drive. A new folder \"data\" will be created in teh current directory to store data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Use a data loader to shuffle and batch\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# Network Architecture\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 10\n",
    "\n",
    "# Fully connected neural netowrk with two hidden layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, h1, h2, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(h2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Define the Loss Function and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/10], Step [100/3000], Loss: 0.6115\n",
      "Epoch[1/10], Step [200/3000], Loss: 0.2294\n",
      "Epoch[1/10], Step [300/3000], Loss: 0.8505\n",
      "Epoch[1/10], Step [400/3000], Loss: 0.8356\n",
      "Epoch[1/10], Step [500/3000], Loss: 0.2663\n",
      "Epoch[1/10], Step [600/3000], Loss: 0.9179\n",
      "Epoch[1/10], Step [700/3000], Loss: 0.229\n",
      "Epoch[1/10], Step [800/3000], Loss: 0.1723\n",
      "Epoch[1/10], Step [900/3000], Loss: 0.4879\n",
      "Epoch[1/10], Step [1000/3000], Loss: 0.2801\n",
      "Epoch[1/10], Step [1100/3000], Loss: 0.0731\n",
      "Epoch[1/10], Step [1200/3000], Loss: 0.9669\n",
      "Epoch[1/10], Step [1300/3000], Loss: 0.1547\n",
      "Epoch[1/10], Step [1400/3000], Loss: 0.0771\n",
      "Epoch[1/10], Step [1500/3000], Loss: 0.5352\n",
      "Epoch[1/10], Step [1600/3000], Loss: 0.4816\n",
      "Epoch[1/10], Step [1700/3000], Loss: 0.34\n",
      "Epoch[1/10], Step [1800/3000], Loss: 0.6817\n",
      "Epoch[1/10], Step [1900/3000], Loss: 0.4901\n",
      "Epoch[1/10], Step [2000/3000], Loss: 0.4228\n",
      "Epoch[1/10], Step [2100/3000], Loss: 0.2214\n",
      "Epoch[1/10], Step [2200/3000], Loss: 0.0388\n",
      "Epoch[1/10], Step [2300/3000], Loss: 0.3663\n",
      "Epoch[1/10], Step [2400/3000], Loss: 0.2592\n",
      "Epoch[1/10], Step [2500/3000], Loss: 0.1682\n",
      "Epoch[1/10], Step [2600/3000], Loss: 0.0222\n",
      "Epoch[1/10], Step [2700/3000], Loss: 0.1387\n",
      "Epoch[1/10], Step [2800/3000], Loss: 0.0668\n",
      "Epoch[1/10], Step [2900/3000], Loss: 0.1177\n",
      "Epoch[1/10], Step [3000/3000], Loss: 0.8076\n",
      "Epoch[2/10], Step [100/3000], Loss: 0.077\n",
      "Epoch[2/10], Step [200/3000], Loss: 0.553\n",
      "Epoch[2/10], Step [300/3000], Loss: 0.2514\n",
      "Epoch[2/10], Step [400/3000], Loss: 0.1338\n",
      "Epoch[2/10], Step [500/3000], Loss: 0.1539\n",
      "Epoch[2/10], Step [600/3000], Loss: 0.0485\n",
      "Epoch[2/10], Step [700/3000], Loss: 0.0384\n",
      "Epoch[2/10], Step [800/3000], Loss: 0.0721\n",
      "Epoch[2/10], Step [900/3000], Loss: 0.0863\n",
      "Epoch[2/10], Step [1000/3000], Loss: 0.0955\n",
      "Epoch[2/10], Step [1100/3000], Loss: 0.1092\n",
      "Epoch[2/10], Step [1200/3000], Loss: 0.3989\n",
      "Epoch[2/10], Step [1300/3000], Loss: 0.3698\n",
      "Epoch[2/10], Step [1400/3000], Loss: 0.0313\n",
      "Epoch[2/10], Step [1500/3000], Loss: 0.1843\n",
      "Epoch[2/10], Step [1600/3000], Loss: 0.0354\n",
      "Epoch[2/10], Step [1700/3000], Loss: 0.0183\n",
      "Epoch[2/10], Step [1800/3000], Loss: 0.0862\n",
      "Epoch[2/10], Step [1900/3000], Loss: 0.2993\n",
      "Epoch[2/10], Step [2000/3000], Loss: 0.0338\n",
      "Epoch[2/10], Step [2100/3000], Loss: 0.1652\n",
      "Epoch[2/10], Step [2200/3000], Loss: 0.1064\n",
      "Epoch[2/10], Step [2300/3000], Loss: 0.0302\n",
      "Epoch[2/10], Step [2400/3000], Loss: 0.0687\n",
      "Epoch[2/10], Step [2500/3000], Loss: 0.2078\n",
      "Epoch[2/10], Step [2600/3000], Loss: 0.1631\n",
      "Epoch[2/10], Step [2700/3000], Loss: 0.0152\n",
      "Epoch[2/10], Step [2800/3000], Loss: 0.032\n",
      "Epoch[2/10], Step [2900/3000], Loss: 0.0247\n",
      "Epoch[2/10], Step [3000/3000], Loss: 0.0412\n",
      "Epoch[3/10], Step [100/3000], Loss: 0.0584\n",
      "Epoch[3/10], Step [200/3000], Loss: 0.0284\n",
      "Epoch[3/10], Step [300/3000], Loss: 0.016\n",
      "Epoch[3/10], Step [400/3000], Loss: 0.0508\n",
      "Epoch[3/10], Step [500/3000], Loss: 0.007\n",
      "Epoch[3/10], Step [600/3000], Loss: 0.2935\n",
      "Epoch[3/10], Step [700/3000], Loss: 0.031\n",
      "Epoch[3/10], Step [800/3000], Loss: 0.1039\n",
      "Epoch[3/10], Step [900/3000], Loss: 0.0157\n",
      "Epoch[3/10], Step [1000/3000], Loss: 0.1939\n",
      "Epoch[3/10], Step [1100/3000], Loss: 0.0292\n",
      "Epoch[3/10], Step [1200/3000], Loss: 0.072\n",
      "Epoch[3/10], Step [1300/3000], Loss: 0.1828\n",
      "Epoch[3/10], Step [1400/3000], Loss: 0.0815\n",
      "Epoch[3/10], Step [1500/3000], Loss: 0.0201\n",
      "Epoch[3/10], Step [1600/3000], Loss: 0.2548\n",
      "Epoch[3/10], Step [1700/3000], Loss: 0.0569\n",
      "Epoch[3/10], Step [1800/3000], Loss: 0.0962\n",
      "Epoch[3/10], Step [1900/3000], Loss: 0.0206\n",
      "Epoch[3/10], Step [2000/3000], Loss: 0.0525\n",
      "Epoch[3/10], Step [2100/3000], Loss: 0.0255\n",
      "Epoch[3/10], Step [2200/3000], Loss: 0.0916\n",
      "Epoch[3/10], Step [2300/3000], Loss: 0.0151\n",
      "Epoch[3/10], Step [2400/3000], Loss: 0.1104\n",
      "Epoch[3/10], Step [2500/3000], Loss: 0.0303\n",
      "Epoch[3/10], Step [2600/3000], Loss: 0.2253\n",
      "Epoch[3/10], Step [2700/3000], Loss: 0.0051\n",
      "Epoch[3/10], Step [2800/3000], Loss: 0.1318\n",
      "Epoch[3/10], Step [2900/3000], Loss: 0.0552\n",
      "Epoch[3/10], Step [3000/3000], Loss: 0.0059\n",
      "Epoch[4/10], Step [100/3000], Loss: 0.0763\n",
      "Epoch[4/10], Step [200/3000], Loss: 0.0378\n",
      "Epoch[4/10], Step [300/3000], Loss: 0.0516\n",
      "Epoch[4/10], Step [400/3000], Loss: 0.0631\n",
      "Epoch[4/10], Step [500/3000], Loss: 0.0899\n",
      "Epoch[4/10], Step [600/3000], Loss: 0.0735\n",
      "Epoch[4/10], Step [700/3000], Loss: 0.1684\n",
      "Epoch[4/10], Step [800/3000], Loss: 0.1525\n",
      "Epoch[4/10], Step [900/3000], Loss: 0.0041\n",
      "Epoch[4/10], Step [1000/3000], Loss: 0.1206\n",
      "Epoch[4/10], Step [1100/3000], Loss: 0.0168\n",
      "Epoch[4/10], Step [1200/3000], Loss: 0.0132\n",
      "Epoch[4/10], Step [1300/3000], Loss: 0.0211\n",
      "Epoch[4/10], Step [1400/3000], Loss: 0.0198\n",
      "Epoch[4/10], Step [1500/3000], Loss: 0.0252\n",
      "Epoch[4/10], Step [1600/3000], Loss: 0.1337\n",
      "Epoch[4/10], Step [1700/3000], Loss: 0.0608\n",
      "Epoch[4/10], Step [1800/3000], Loss: 0.1784\n",
      "Epoch[4/10], Step [1900/3000], Loss: 0.0418\n",
      "Epoch[4/10], Step [2000/3000], Loss: 0.0625\n",
      "Epoch[4/10], Step [2100/3000], Loss: 0.0524\n",
      "Epoch[4/10], Step [2200/3000], Loss: 0.0053\n",
      "Epoch[4/10], Step [2300/3000], Loss: 0.0644\n",
      "Epoch[4/10], Step [2400/3000], Loss: 0.0025\n",
      "Epoch[4/10], Step [2500/3000], Loss: 0.3047\n",
      "Epoch[4/10], Step [2600/3000], Loss: 0.0973\n",
      "Epoch[4/10], Step [2700/3000], Loss: 0.0034\n",
      "Epoch[4/10], Step [2800/3000], Loss: 0.052\n",
      "Epoch[4/10], Step [2900/3000], Loss: 0.0336\n",
      "Epoch[4/10], Step [3000/3000], Loss: 0.2068\n",
      "Epoch[5/10], Step [100/3000], Loss: 0.0036\n",
      "Epoch[5/10], Step [200/3000], Loss: 0.0677\n",
      "Epoch[5/10], Step [300/3000], Loss: 0.0177\n",
      "Epoch[5/10], Step [400/3000], Loss: 0.0323\n",
      "Epoch[5/10], Step [500/3000], Loss: 0.1665\n",
      "Epoch[5/10], Step [600/3000], Loss: 0.0105\n",
      "Epoch[5/10], Step [700/3000], Loss: 0.012\n",
      "Epoch[5/10], Step [800/3000], Loss: 0.1151\n",
      "Epoch[5/10], Step [900/3000], Loss: 0.0398\n",
      "Epoch[5/10], Step [1000/3000], Loss: 0.0355\n",
      "Epoch[5/10], Step [1100/3000], Loss: 0.0277\n",
      "Epoch[5/10], Step [1200/3000], Loss: 0.1275\n",
      "Epoch[5/10], Step [1300/3000], Loss: 0.0106\n",
      "Epoch[5/10], Step [1400/3000], Loss: 0.0109\n",
      "Epoch[5/10], Step [1500/3000], Loss: 0.0246\n",
      "Epoch[5/10], Step [1600/3000], Loss: 0.0967\n",
      "Epoch[5/10], Step [1700/3000], Loss: 0.0335\n",
      "Epoch[5/10], Step [1800/3000], Loss: 0.1855\n",
      "Epoch[5/10], Step [1900/3000], Loss: 0.0662\n",
      "Epoch[5/10], Step [2000/3000], Loss: 0.0198\n",
      "Epoch[5/10], Step [2100/3000], Loss: 0.0049\n",
      "Epoch[5/10], Step [2200/3000], Loss: 0.0519\n",
      "Epoch[5/10], Step [2300/3000], Loss: 0.0083\n",
      "Epoch[5/10], Step [2400/3000], Loss: 0.0223\n",
      "Epoch[5/10], Step [2500/3000], Loss: 0.173\n",
      "Epoch[5/10], Step [2600/3000], Loss: 0.0743\n",
      "Epoch[5/10], Step [2700/3000], Loss: 0.0893\n",
      "Epoch[5/10], Step [2800/3000], Loss: 0.0114\n",
      "Epoch[5/10], Step [2900/3000], Loss: 0.03\n",
      "Epoch[5/10], Step [3000/3000], Loss: 0.0812\n",
      "Epoch[6/10], Step [100/3000], Loss: 0.0078\n",
      "Epoch[6/10], Step [200/3000], Loss: 0.0563\n",
      "Epoch[6/10], Step [300/3000], Loss: 0.0665\n",
      "Epoch[6/10], Step [400/3000], Loss: 0.006\n",
      "Epoch[6/10], Step [500/3000], Loss: 0.0182\n",
      "Epoch[6/10], Step [600/3000], Loss: 0.2203\n",
      "Epoch[6/10], Step [700/3000], Loss: 0.0203\n",
      "Epoch[6/10], Step [800/3000], Loss: 0.0253\n",
      "Epoch[6/10], Step [900/3000], Loss: 0.136\n",
      "Epoch[6/10], Step [1000/3000], Loss: 0.013\n",
      "Epoch[6/10], Step [1100/3000], Loss: 0.0923\n",
      "Epoch[6/10], Step [1200/3000], Loss: 0.0092\n",
      "Epoch[6/10], Step [1300/3000], Loss: 0.0095\n",
      "Epoch[6/10], Step [1400/3000], Loss: 0.0138\n",
      "Epoch[6/10], Step [1500/3000], Loss: 0.0163\n",
      "Epoch[6/10], Step [1600/3000], Loss: 0.0184\n",
      "Epoch[6/10], Step [1700/3000], Loss: 0.0054\n",
      "Epoch[6/10], Step [1800/3000], Loss: 0.0313\n",
      "Epoch[6/10], Step [1900/3000], Loss: 0.0215\n",
      "Epoch[6/10], Step [2000/3000], Loss: 0.0093\n",
      "Epoch[6/10], Step [2100/3000], Loss: 0.1698\n",
      "Epoch[6/10], Step [2200/3000], Loss: 0.0478\n",
      "Epoch[6/10], Step [2300/3000], Loss: 0.01\n",
      "Epoch[6/10], Step [2400/3000], Loss: 0.1052\n",
      "Epoch[6/10], Step [2500/3000], Loss: 0.4373\n",
      "Epoch[6/10], Step [2600/3000], Loss: 0.0064\n",
      "Epoch[6/10], Step [2700/3000], Loss: 0.0191\n",
      "Epoch[6/10], Step [2800/3000], Loss: 0.0231\n",
      "Epoch[6/10], Step [2900/3000], Loss: 0.0048\n",
      "Epoch[6/10], Step [3000/3000], Loss: 0.0021\n",
      "Epoch[7/10], Step [100/3000], Loss: 0.0278\n",
      "Epoch[7/10], Step [200/3000], Loss: 0.0911\n",
      "Epoch[7/10], Step [300/3000], Loss: 0.0031\n",
      "Epoch[7/10], Step [400/3000], Loss: 0.0012\n",
      "Epoch[7/10], Step [500/3000], Loss: 0.0134\n",
      "Epoch[7/10], Step [600/3000], Loss: 0.0559\n",
      "Epoch[7/10], Step [700/3000], Loss: 0.0218\n",
      "Epoch[7/10], Step [800/3000], Loss: 0.0163\n",
      "Epoch[7/10], Step [900/3000], Loss: 0.012\n",
      "Epoch[7/10], Step [1000/3000], Loss: 0.0124\n",
      "Epoch[7/10], Step [1100/3000], Loss: 0.0059\n",
      "Epoch[7/10], Step [1200/3000], Loss: 0.0017\n",
      "Epoch[7/10], Step [1300/3000], Loss: 0.0149\n",
      "Epoch[7/10], Step [1400/3000], Loss: 0.0746\n",
      "Epoch[7/10], Step [1500/3000], Loss: 0.0268\n",
      "Epoch[7/10], Step [1600/3000], Loss: 0.0429\n",
      "Epoch[7/10], Step [1700/3000], Loss: 0.1963\n",
      "Epoch[7/10], Step [1800/3000], Loss: 0.0147\n",
      "Epoch[7/10], Step [1900/3000], Loss: 0.0566\n",
      "Epoch[7/10], Step [2000/3000], Loss: 0.0198\n",
      "Epoch[7/10], Step [2100/3000], Loss: 0.0124\n",
      "Epoch[7/10], Step [2200/3000], Loss: 0.0943\n",
      "Epoch[7/10], Step [2300/3000], Loss: 0.0054\n",
      "Epoch[7/10], Step [2400/3000], Loss: 0.0564\n",
      "Epoch[7/10], Step [2500/3000], Loss: 0.0084\n",
      "Epoch[7/10], Step [2600/3000], Loss: 0.0223\n",
      "Epoch[7/10], Step [2700/3000], Loss: 0.0032\n",
      "Epoch[7/10], Step [2800/3000], Loss: 0.0426\n",
      "Epoch[7/10], Step [2900/3000], Loss: 0.018\n",
      "Epoch[7/10], Step [3000/3000], Loss: 0.0016\n",
      "Epoch[8/10], Step [100/3000], Loss: 0.0223\n",
      "Epoch[8/10], Step [200/3000], Loss: 0.0935\n",
      "Epoch[8/10], Step [300/3000], Loss: 0.0288\n",
      "Epoch[8/10], Step [400/3000], Loss: 0.012\n",
      "Epoch[8/10], Step [500/3000], Loss: 0.0037\n",
      "Epoch[8/10], Step [600/3000], Loss: 0.0062\n",
      "Epoch[8/10], Step [700/3000], Loss: 0.0015\n",
      "Epoch[8/10], Step [800/3000], Loss: 0.0137\n",
      "Epoch[8/10], Step [900/3000], Loss: 0.0178\n",
      "Epoch[8/10], Step [1000/3000], Loss: 0.0372\n",
      "Epoch[8/10], Step [1100/3000], Loss: 0.029\n",
      "Epoch[8/10], Step [1200/3000], Loss: 0.0229\n",
      "Epoch[8/10], Step [1300/3000], Loss: 0.0232\n",
      "Epoch[8/10], Step [1400/3000], Loss: 0.0039\n",
      "Epoch[8/10], Step [1500/3000], Loss: 0.0226\n",
      "Epoch[8/10], Step [1600/3000], Loss: 0.0431\n",
      "Epoch[8/10], Step [1700/3000], Loss: 0.0017\n",
      "Epoch[8/10], Step [1800/3000], Loss: 0.063\n",
      "Epoch[8/10], Step [1900/3000], Loss: 0.0046\n",
      "Epoch[8/10], Step [2000/3000], Loss: 0.1009\n",
      "Epoch[8/10], Step [2100/3000], Loss: 0.0065\n",
      "Epoch[8/10], Step [2200/3000], Loss: 0.0033\n",
      "Epoch[8/10], Step [2300/3000], Loss: 0.0075\n",
      "Epoch[8/10], Step [2400/3000], Loss: 0.0417\n",
      "Epoch[8/10], Step [2500/3000], Loss: 0.004\n",
      "Epoch[8/10], Step [2600/3000], Loss: 0.0087\n",
      "Epoch[8/10], Step [2700/3000], Loss: 0.0331\n",
      "Epoch[8/10], Step [2800/3000], Loss: 0.0405\n",
      "Epoch[8/10], Step [2900/3000], Loss: 0.0728\n",
      "Epoch[8/10], Step [3000/3000], Loss: 0.0053\n",
      "Epoch[9/10], Step [100/3000], Loss: 0.0059\n",
      "Epoch[9/10], Step [200/3000], Loss: 0.0069\n",
      "Epoch[9/10], Step [300/3000], Loss: 0.0549\n",
      "Epoch[9/10], Step [400/3000], Loss: 0.0033\n",
      "Epoch[9/10], Step [500/3000], Loss: 0.0049\n",
      "Epoch[9/10], Step [600/3000], Loss: 0.0074\n",
      "Epoch[9/10], Step [700/3000], Loss: 0.0075\n",
      "Epoch[9/10], Step [800/3000], Loss: 0.076\n",
      "Epoch[9/10], Step [900/3000], Loss: 0.0949\n",
      "Epoch[9/10], Step [1000/3000], Loss: 0.0067\n",
      "Epoch[9/10], Step [1100/3000], Loss: 0.0298\n",
      "Epoch[9/10], Step [1200/3000], Loss: 0.0091\n",
      "Epoch[9/10], Step [1300/3000], Loss: 0.0394\n",
      "Epoch[9/10], Step [1400/3000], Loss: 0.0033\n",
      "Epoch[9/10], Step [1500/3000], Loss: 0.0052\n",
      "Epoch[9/10], Step [1600/3000], Loss: 0.0053\n",
      "Epoch[9/10], Step [1700/3000], Loss: 0.0012\n",
      "Epoch[9/10], Step [1800/3000], Loss: 0.0548\n",
      "Epoch[9/10], Step [1900/3000], Loss: 0.0238\n",
      "Epoch[9/10], Step [2000/3000], Loss: 0.002\n",
      "Epoch[9/10], Step [2100/3000], Loss: 0.1316\n",
      "Epoch[9/10], Step [2200/3000], Loss: 0.0051\n",
      "Epoch[9/10], Step [2300/3000], Loss: 0.0224\n",
      "Epoch[9/10], Step [2400/3000], Loss: 0.0693\n",
      "Epoch[9/10], Step [2500/3000], Loss: 0.0245\n",
      "Epoch[9/10], Step [2600/3000], Loss: 0.0058\n",
      "Epoch[9/10], Step [2700/3000], Loss: 0.0106\n",
      "Epoch[9/10], Step [2800/3000], Loss: 0.0569\n",
      "Epoch[9/10], Step [2900/3000], Loss: 0.0112\n",
      "Epoch[9/10], Step [3000/3000], Loss: 0.0923\n",
      "Epoch[10/10], Step [100/3000], Loss: 0.0023\n",
      "Epoch[10/10], Step [200/3000], Loss: 0.0043\n",
      "Epoch[10/10], Step [300/3000], Loss: 0.032\n",
      "Epoch[10/10], Step [400/3000], Loss: 0.0013\n",
      "Epoch[10/10], Step [500/3000], Loss: 0.0014\n",
      "Epoch[10/10], Step [600/3000], Loss: 0.0021\n",
      "Epoch[10/10], Step [700/3000], Loss: 0.0098\n",
      "Epoch[10/10], Step [800/3000], Loss: 0.05\n",
      "Epoch[10/10], Step [900/3000], Loss: 0.0174\n",
      "Epoch[10/10], Step [1000/3000], Loss: 0.0302\n",
      "Epoch[10/10], Step [1100/3000], Loss: 0.0047\n",
      "Epoch[10/10], Step [1200/3000], Loss: 0.0149\n",
      "Epoch[10/10], Step [1300/3000], Loss: 0.0099\n",
      "Epoch[10/10], Step [1400/3000], Loss: 0.0023\n",
      "Epoch[10/10], Step [1500/3000], Loss: 0.0396\n",
      "Epoch[10/10], Step [1600/3000], Loss: 0.0037\n",
      "Epoch[10/10], Step [1700/3000], Loss: 0.1253\n",
      "Epoch[10/10], Step [1800/3000], Loss: 0.0046\n",
      "Epoch[10/10], Step [1900/3000], Loss: 0.0035\n",
      "Epoch[10/10], Step [2000/3000], Loss: 0.0165\n",
      "Epoch[10/10], Step [2100/3000], Loss: 0.0063\n",
      "Epoch[10/10], Step [2200/3000], Loss: 0.0307\n",
      "Epoch[10/10], Step [2300/3000], Loss: 0.0278\n",
      "Epoch[10/10], Step [2400/3000], Loss: 0.0185\n",
      "Epoch[10/10], Step [2500/3000], Loss: 0.0022\n",
      "Epoch[10/10], Step [2600/3000], Loss: 0.0027\n",
      "Epoch[10/10], Step [2700/3000], Loss: 0.0115\n",
      "Epoch[10/10], Step [2800/3000], Loss: 0.0029\n",
      "Epoch[10/10], Step [2900/3000], Loss: 0.0021\n",
      "Epoch[10/10], Step [3000/3000], Loss: 0.0011\n",
      "Accuracy of the network no the 10,000 test images: 98.03%, with learning rate: 0.05, and [1568, 1568] hidden neurons\n",
      "\n",
      "\n",
      "\n",
      "Epoch[1/10], Step [100/3000], Loss: 0.7563\n",
      "Epoch[1/10], Step [200/3000], Loss: 1.4464\n",
      "Epoch[1/10], Step [300/3000], Loss: 0.2129\n",
      "Epoch[1/10], Step [400/3000], Loss: 0.3937\n",
      "Epoch[1/10], Step [500/3000], Loss: 0.2905\n",
      "Epoch[1/10], Step [600/3000], Loss: 0.4329\n",
      "Epoch[1/10], Step [700/3000], Loss: 0.1514\n",
      "Epoch[1/10], Step [800/3000], Loss: 0.041\n",
      "Epoch[1/10], Step [900/3000], Loss: 0.1678\n",
      "Epoch[1/10], Step [1000/3000], Loss: 0.0871\n",
      "Epoch[1/10], Step [1100/3000], Loss: 0.2156\n",
      "Epoch[1/10], Step [1200/3000], Loss: 0.4088\n",
      "Epoch[1/10], Step [1300/3000], Loss: 0.185\n",
      "Epoch[1/10], Step [1400/3000], Loss: 0.2908\n",
      "Epoch[1/10], Step [1500/3000], Loss: 0.0917\n",
      "Epoch[1/10], Step [1600/3000], Loss: 0.0303\n",
      "Epoch[1/10], Step [1700/3000], Loss: 0.4853\n",
      "Epoch[1/10], Step [1800/3000], Loss: 0.2026\n",
      "Epoch[1/10], Step [1900/3000], Loss: 0.2128\n",
      "Epoch[1/10], Step [2000/3000], Loss: 0.0428\n",
      "Epoch[1/10], Step [2100/3000], Loss: 0.1861\n",
      "Epoch[1/10], Step [2200/3000], Loss: 0.0231\n",
      "Epoch[1/10], Step [2300/3000], Loss: 0.0273\n",
      "Epoch[1/10], Step [2400/3000], Loss: 0.0558\n",
      "Epoch[1/10], Step [2500/3000], Loss: 0.0068\n",
      "Epoch[1/10], Step [2600/3000], Loss: 0.074\n",
      "Epoch[1/10], Step [2700/3000], Loss: 0.1078\n",
      "Epoch[1/10], Step [2800/3000], Loss: 0.1013\n",
      "Epoch[1/10], Step [2900/3000], Loss: 0.0524\n",
      "Epoch[1/10], Step [3000/3000], Loss: 0.059\n",
      "Epoch[2/10], Step [100/3000], Loss: 0.0286\n",
      "Epoch[2/10], Step [200/3000], Loss: 0.1139\n",
      "Epoch[2/10], Step [300/3000], Loss: 0.114\n",
      "Epoch[2/10], Step [400/3000], Loss: 0.042\n",
      "Epoch[2/10], Step [500/3000], Loss: 0.1678\n",
      "Epoch[2/10], Step [600/3000], Loss: 0.1651\n",
      "Epoch[2/10], Step [700/3000], Loss: 0.0336\n",
      "Epoch[2/10], Step [800/3000], Loss: 0.061\n",
      "Epoch[2/10], Step [900/3000], Loss: 0.0146\n",
      "Epoch[2/10], Step [1000/3000], Loss: 0.0094\n",
      "Epoch[2/10], Step [1100/3000], Loss: 0.012\n",
      "Epoch[2/10], Step [1200/3000], Loss: 0.0436\n",
      "Epoch[2/10], Step [1300/3000], Loss: 0.146\n",
      "Epoch[2/10], Step [1400/3000], Loss: 0.0174\n",
      "Epoch[2/10], Step [1500/3000], Loss: 0.3767\n",
      "Epoch[2/10], Step [1600/3000], Loss: 0.1483\n",
      "Epoch[2/10], Step [1700/3000], Loss: 0.032\n",
      "Epoch[2/10], Step [1800/3000], Loss: 0.1142\n",
      "Epoch[2/10], Step [1900/3000], Loss: 0.0796\n",
      "Epoch[2/10], Step [2000/3000], Loss: 0.0906\n",
      "Epoch[2/10], Step [2100/3000], Loss: 0.3135\n",
      "Epoch[2/10], Step [2200/3000], Loss: 0.3826\n",
      "Epoch[2/10], Step [2300/3000], Loss: 0.0083\n",
      "Epoch[2/10], Step [2400/3000], Loss: 0.0024\n",
      "Epoch[2/10], Step [2500/3000], Loss: 0.0845\n",
      "Epoch[2/10], Step [2600/3000], Loss: 0.0443\n",
      "Epoch[2/10], Step [2700/3000], Loss: 0.2312\n",
      "Epoch[2/10], Step [2800/3000], Loss: 0.238\n",
      "Epoch[2/10], Step [2900/3000], Loss: 0.2994\n",
      "Epoch[2/10], Step [3000/3000], Loss: 0.1899\n",
      "Epoch[3/10], Step [100/3000], Loss: 0.1652\n",
      "Epoch[3/10], Step [200/3000], Loss: 0.1043\n",
      "Epoch[3/10], Step [300/3000], Loss: 0.1053\n",
      "Epoch[3/10], Step [400/3000], Loss: 0.0413\n",
      "Epoch[3/10], Step [500/3000], Loss: 0.0516\n",
      "Epoch[3/10], Step [600/3000], Loss: 0.0117\n",
      "Epoch[3/10], Step [700/3000], Loss: 0.0801\n",
      "Epoch[3/10], Step [800/3000], Loss: 0.0445\n",
      "Epoch[3/10], Step [900/3000], Loss: 0.0749\n",
      "Epoch[3/10], Step [1000/3000], Loss: 0.1188\n",
      "Epoch[3/10], Step [1100/3000], Loss: 0.0054\n",
      "Epoch[3/10], Step [1200/3000], Loss: 0.0123\n",
      "Epoch[3/10], Step [1300/3000], Loss: 0.2005\n",
      "Epoch[3/10], Step [1400/3000], Loss: 0.1737\n",
      "Epoch[3/10], Step [1500/3000], Loss: 0.0299\n",
      "Epoch[3/10], Step [1600/3000], Loss: 0.0116\n",
      "Epoch[3/10], Step [1700/3000], Loss: 0.1187\n",
      "Epoch[3/10], Step [1800/3000], Loss: 0.1765\n",
      "Epoch[3/10], Step [1900/3000], Loss: 0.1092\n",
      "Epoch[3/10], Step [2000/3000], Loss: 0.0746\n",
      "Epoch[3/10], Step [2100/3000], Loss: 0.0069\n",
      "Epoch[3/10], Step [2200/3000], Loss: 0.2776\n",
      "Epoch[3/10], Step [2300/3000], Loss: 0.0134\n",
      "Epoch[3/10], Step [2400/3000], Loss: 0.0098\n",
      "Epoch[3/10], Step [2500/3000], Loss: 0.01\n",
      "Epoch[3/10], Step [2600/3000], Loss: 0.1409\n",
      "Epoch[3/10], Step [2700/3000], Loss: 0.0031\n",
      "Epoch[3/10], Step [2800/3000], Loss: 0.0092\n",
      "Epoch[3/10], Step [2900/3000], Loss: 0.1991\n",
      "Epoch[3/10], Step [3000/3000], Loss: 0.0017\n",
      "Epoch[4/10], Step [100/3000], Loss: 0.1481\n",
      "Epoch[4/10], Step [200/3000], Loss: 0.0184\n",
      "Epoch[4/10], Step [300/3000], Loss: 0.0592\n",
      "Epoch[4/10], Step [400/3000], Loss: 0.0493\n",
      "Epoch[4/10], Step [500/3000], Loss: 0.0744\n",
      "Epoch[4/10], Step [600/3000], Loss: 0.0108\n",
      "Epoch[4/10], Step [700/3000], Loss: 0.036\n",
      "Epoch[4/10], Step [800/3000], Loss: 0.1252\n",
      "Epoch[4/10], Step [900/3000], Loss: 0.0029\n",
      "Epoch[4/10], Step [1000/3000], Loss: 0.0096\n",
      "Epoch[4/10], Step [1100/3000], Loss: 0.001\n",
      "Epoch[4/10], Step [1200/3000], Loss: 0.0226\n",
      "Epoch[4/10], Step [1300/3000], Loss: 0.0162\n",
      "Epoch[4/10], Step [1400/3000], Loss: 0.08\n",
      "Epoch[4/10], Step [1500/3000], Loss: 0.0144\n",
      "Epoch[4/10], Step [1600/3000], Loss: 0.0074\n",
      "Epoch[4/10], Step [1700/3000], Loss: 0.1512\n",
      "Epoch[4/10], Step [1800/3000], Loss: 0.0339\n",
      "Epoch[4/10], Step [1900/3000], Loss: 0.2621\n",
      "Epoch[4/10], Step [2000/3000], Loss: 0.096\n",
      "Epoch[4/10], Step [2100/3000], Loss: 0.011\n",
      "Epoch[4/10], Step [2200/3000], Loss: 0.1348\n",
      "Epoch[4/10], Step [2300/3000], Loss: 0.2685\n",
      "Epoch[4/10], Step [2400/3000], Loss: 0.0672\n",
      "Epoch[4/10], Step [2500/3000], Loss: 0.0011\n",
      "Epoch[4/10], Step [2600/3000], Loss: 0.0931\n",
      "Epoch[4/10], Step [2700/3000], Loss: 0.0545\n",
      "Epoch[4/10], Step [2800/3000], Loss: 0.0011\n",
      "Epoch[4/10], Step [2900/3000], Loss: 0.1424\n",
      "Epoch[4/10], Step [3000/3000], Loss: 0.0068\n",
      "Epoch[5/10], Step [100/3000], Loss: 0.0014\n",
      "Epoch[5/10], Step [200/3000], Loss: 0.0038\n",
      "Epoch[5/10], Step [300/3000], Loss: 0.0048\n",
      "Epoch[5/10], Step [400/3000], Loss: 0.0225\n",
      "Epoch[5/10], Step [500/3000], Loss: 0.0166\n",
      "Epoch[5/10], Step [600/3000], Loss: 0.0181\n",
      "Epoch[5/10], Step [700/3000], Loss: 0.0693\n",
      "Epoch[5/10], Step [800/3000], Loss: 0.2935\n",
      "Epoch[5/10], Step [900/3000], Loss: 0.156\n",
      "Epoch[5/10], Step [1000/3000], Loss: 0.0768\n",
      "Epoch[5/10], Step [1100/3000], Loss: 0.0005\n",
      "Epoch[5/10], Step [1200/3000], Loss: 0.0047\n",
      "Epoch[5/10], Step [1300/3000], Loss: 0.0056\n",
      "Epoch[5/10], Step [1400/3000], Loss: 0.0006\n",
      "Epoch[5/10], Step [1500/3000], Loss: 0.0031\n",
      "Epoch[5/10], Step [1600/3000], Loss: 0.0046\n",
      "Epoch[5/10], Step [1700/3000], Loss: 0.0328\n",
      "Epoch[5/10], Step [1800/3000], Loss: 0.0741\n",
      "Epoch[5/10], Step [1900/3000], Loss: 0.0606\n",
      "Epoch[5/10], Step [2000/3000], Loss: 0.0008\n",
      "Epoch[5/10], Step [2100/3000], Loss: 0.0072\n",
      "Epoch[5/10], Step [2200/3000], Loss: 0.0455\n",
      "Epoch[5/10], Step [2300/3000], Loss: 0.0035\n",
      "Epoch[5/10], Step [2400/3000], Loss: 0.0775\n",
      "Epoch[5/10], Step [2500/3000], Loss: 0.002\n",
      "Epoch[5/10], Step [2600/3000], Loss: 0.1414\n",
      "Epoch[5/10], Step [2700/3000], Loss: 0.0054\n",
      "Epoch[5/10], Step [2800/3000], Loss: 0.0096\n",
      "Epoch[5/10], Step [2900/3000], Loss: 0.0517\n",
      "Epoch[5/10], Step [3000/3000], Loss: 0.0079\n",
      "Epoch[6/10], Step [100/3000], Loss: 0.0038\n",
      "Epoch[6/10], Step [200/3000], Loss: 0.0151\n",
      "Epoch[6/10], Step [300/3000], Loss: 0.0018\n",
      "Epoch[6/10], Step [400/3000], Loss: 0.0022\n",
      "Epoch[6/10], Step [500/3000], Loss: 0.0681\n",
      "Epoch[6/10], Step [600/3000], Loss: 0.0021\n",
      "Epoch[6/10], Step [700/3000], Loss: 0.0068\n",
      "Epoch[6/10], Step [800/3000], Loss: 0.0011\n",
      "Epoch[6/10], Step [900/3000], Loss: 0.0958\n",
      "Epoch[6/10], Step [1000/3000], Loss: 0.031\n",
      "Epoch[6/10], Step [1100/3000], Loss: 0.0012\n",
      "Epoch[6/10], Step [1200/3000], Loss: 0.0622\n",
      "Epoch[6/10], Step [1300/3000], Loss: 0.0598\n",
      "Epoch[6/10], Step [1400/3000], Loss: 0.0263\n",
      "Epoch[6/10], Step [1500/3000], Loss: 0.0048\n",
      "Epoch[6/10], Step [1600/3000], Loss: 0.0296\n",
      "Epoch[6/10], Step [1700/3000], Loss: 0.0029\n",
      "Epoch[6/10], Step [1800/3000], Loss: 0.0132\n",
      "Epoch[6/10], Step [1900/3000], Loss: 0.0169\n",
      "Epoch[6/10], Step [2000/3000], Loss: 0.004\n",
      "Epoch[6/10], Step [2100/3000], Loss: 0.1738\n",
      "Epoch[6/10], Step [2200/3000], Loss: 0.1186\n",
      "Epoch[6/10], Step [2300/3000], Loss: 0.0081\n",
      "Epoch[6/10], Step [2400/3000], Loss: 0.0349\n",
      "Epoch[6/10], Step [2500/3000], Loss: 0.0019\n",
      "Epoch[6/10], Step [2600/3000], Loss: 0.0026\n",
      "Epoch[6/10], Step [2700/3000], Loss: 0.0028\n",
      "Epoch[6/10], Step [2800/3000], Loss: 0.0044\n",
      "Epoch[6/10], Step [2900/3000], Loss: 0.0304\n",
      "Epoch[6/10], Step [3000/3000], Loss: 0.0409\n",
      "Epoch[7/10], Step [100/3000], Loss: 0.0083\n",
      "Epoch[7/10], Step [200/3000], Loss: 0.0013\n",
      "Epoch[7/10], Step [300/3000], Loss: 0.0233\n",
      "Epoch[7/10], Step [400/3000], Loss: 0.0067\n",
      "Epoch[7/10], Step [500/3000], Loss: 0.025\n",
      "Epoch[7/10], Step [600/3000], Loss: 0.0016\n",
      "Epoch[7/10], Step [700/3000], Loss: 0.003\n",
      "Epoch[7/10], Step [800/3000], Loss: 0.0146\n",
      "Epoch[7/10], Step [900/3000], Loss: 0.0317\n",
      "Epoch[7/10], Step [1000/3000], Loss: 0.004\n",
      "Epoch[7/10], Step [1100/3000], Loss: 0.0412\n",
      "Epoch[7/10], Step [1200/3000], Loss: 0.0836\n",
      "Epoch[7/10], Step [1300/3000], Loss: 0.0008\n",
      "Epoch[7/10], Step [1400/3000], Loss: 0.0152\n",
      "Epoch[7/10], Step [1500/3000], Loss: 0.0448\n",
      "Epoch[7/10], Step [1600/3000], Loss: 0.0214\n",
      "Epoch[7/10], Step [1700/3000], Loss: 0.0413\n",
      "Epoch[7/10], Step [1800/3000], Loss: 0.0105\n",
      "Epoch[7/10], Step [1900/3000], Loss: 0.0028\n",
      "Epoch[7/10], Step [2000/3000], Loss: 0.0511\n",
      "Epoch[7/10], Step [2100/3000], Loss: 0.0035\n",
      "Epoch[7/10], Step [2200/3000], Loss: 0.0821\n",
      "Epoch[7/10], Step [2300/3000], Loss: 0.0007\n",
      "Epoch[7/10], Step [2400/3000], Loss: 0.066\n",
      "Epoch[7/10], Step [2500/3000], Loss: 0.0005\n",
      "Epoch[7/10], Step [2600/3000], Loss: 0.0015\n",
      "Epoch[7/10], Step [2700/3000], Loss: 0.1067\n",
      "Epoch[7/10], Step [2800/3000], Loss: 0.0058\n",
      "Epoch[7/10], Step [2900/3000], Loss: 0.0016\n",
      "Epoch[7/10], Step [3000/3000], Loss: 0.0009\n",
      "Epoch[8/10], Step [100/3000], Loss: 0.0044\n",
      "Epoch[8/10], Step [200/3000], Loss: 0.0083\n",
      "Epoch[8/10], Step [300/3000], Loss: 0.0008\n",
      "Epoch[8/10], Step [400/3000], Loss: 0.0113\n",
      "Epoch[8/10], Step [500/3000], Loss: 0.0098\n",
      "Epoch[8/10], Step [600/3000], Loss: 0.0069\n",
      "Epoch[8/10], Step [700/3000], Loss: 0.0133\n",
      "Epoch[8/10], Step [800/3000], Loss: 0.0002\n",
      "Epoch[8/10], Step [900/3000], Loss: 0.0043\n",
      "Epoch[8/10], Step [1000/3000], Loss: 0.0181\n",
      "Epoch[8/10], Step [1100/3000], Loss: 0.0093\n",
      "Epoch[8/10], Step [1200/3000], Loss: 0.0065\n",
      "Epoch[8/10], Step [1300/3000], Loss: 0.0028\n",
      "Epoch[8/10], Step [1400/3000], Loss: 0.0013\n",
      "Epoch[8/10], Step [1500/3000], Loss: 0.0012\n",
      "Epoch[8/10], Step [1600/3000], Loss: 0.001\n",
      "Epoch[8/10], Step [1700/3000], Loss: 0.0006\n",
      "Epoch[8/10], Step [1800/3000], Loss: 0.0602\n",
      "Epoch[8/10], Step [1900/3000], Loss: 0.0008\n",
      "Epoch[8/10], Step [2000/3000], Loss: 0.0643\n",
      "Epoch[8/10], Step [2100/3000], Loss: 0.0006\n",
      "Epoch[8/10], Step [2200/3000], Loss: 0.0275\n",
      "Epoch[8/10], Step [2300/3000], Loss: 0.0008\n",
      "Epoch[8/10], Step [2400/3000], Loss: 0.0008\n",
      "Epoch[8/10], Step [2500/3000], Loss: 0.0011\n",
      "Epoch[8/10], Step [2600/3000], Loss: 0.0193\n",
      "Epoch[8/10], Step [2700/3000], Loss: 0.0868\n",
      "Epoch[8/10], Step [2800/3000], Loss: 0.0042\n",
      "Epoch[8/10], Step [2900/3000], Loss: 0.0236\n",
      "Epoch[8/10], Step [3000/3000], Loss: 0.0288\n",
      "Epoch[9/10], Step [100/3000], Loss: 0.0029\n",
      "Epoch[9/10], Step [200/3000], Loss: 0.0015\n",
      "Epoch[9/10], Step [300/3000], Loss: 0.003\n",
      "Epoch[9/10], Step [400/3000], Loss: 0.0028\n",
      "Epoch[9/10], Step [500/3000], Loss: 0.0\n",
      "Epoch[9/10], Step [600/3000], Loss: 0.0008\n",
      "Epoch[9/10], Step [700/3000], Loss: 0.0017\n",
      "Epoch[9/10], Step [800/3000], Loss: 0.0004\n",
      "Epoch[9/10], Step [900/3000], Loss: 0.0029\n",
      "Epoch[9/10], Step [1000/3000], Loss: 0.0252\n",
      "Epoch[9/10], Step [1100/3000], Loss: 0.0613\n",
      "Epoch[9/10], Step [1200/3000], Loss: 0.0032\n",
      "Epoch[9/10], Step [1300/3000], Loss: 0.0002\n",
      "Epoch[9/10], Step [1400/3000], Loss: 0.0099\n",
      "Epoch[9/10], Step [1500/3000], Loss: 0.037\n",
      "Epoch[9/10], Step [1600/3000], Loss: 0.0082\n",
      "Epoch[9/10], Step [1700/3000], Loss: 0.0001\n",
      "Epoch[9/10], Step [1800/3000], Loss: 0.018\n",
      "Epoch[9/10], Step [1900/3000], Loss: 0.0002\n",
      "Epoch[9/10], Step [2000/3000], Loss: 0.0013\n",
      "Epoch[9/10], Step [2100/3000], Loss: 0.1847\n",
      "Epoch[9/10], Step [2200/3000], Loss: 0.0031\n",
      "Epoch[9/10], Step [2300/3000], Loss: 0.0023\n",
      "Epoch[9/10], Step [2400/3000], Loss: 0.0008\n",
      "Epoch[9/10], Step [2500/3000], Loss: 0.0056\n",
      "Epoch[9/10], Step [2600/3000], Loss: 0.1161\n",
      "Epoch[9/10], Step [2700/3000], Loss: 0.007\n",
      "Epoch[9/10], Step [2800/3000], Loss: 0.0001\n",
      "Epoch[9/10], Step [2900/3000], Loss: 0.0051\n",
      "Epoch[9/10], Step [3000/3000], Loss: 0.0708\n",
      "Epoch[10/10], Step [100/3000], Loss: 0.07\n",
      "Epoch[10/10], Step [200/3000], Loss: 0.0162\n",
      "Epoch[10/10], Step [300/3000], Loss: 0.0012\n",
      "Epoch[10/10], Step [400/3000], Loss: 0.0001\n",
      "Epoch[10/10], Step [500/3000], Loss: 0.0006\n",
      "Epoch[10/10], Step [600/3000], Loss: 0.0004\n",
      "Epoch[10/10], Step [700/3000], Loss: 0.0015\n",
      "Epoch[10/10], Step [800/3000], Loss: 0.0369\n",
      "Epoch[10/10], Step [900/3000], Loss: 0.0015\n",
      "Epoch[10/10], Step [1000/3000], Loss: 0.001\n",
      "Epoch[10/10], Step [1100/3000], Loss: 0.0008\n",
      "Epoch[10/10], Step [1200/3000], Loss: 0.0058\n",
      "Epoch[10/10], Step [1300/3000], Loss: 0.013\n",
      "Epoch[10/10], Step [1400/3000], Loss: 0.0009\n",
      "Epoch[10/10], Step [1500/3000], Loss: 0.0003\n",
      "Epoch[10/10], Step [1600/3000], Loss: 0.0227\n",
      "Epoch[10/10], Step [1700/3000], Loss: 0.0157\n",
      "Epoch[10/10], Step [1800/3000], Loss: 0.0002\n",
      "Epoch[10/10], Step [1900/3000], Loss: 0.0009\n",
      "Epoch[10/10], Step [2000/3000], Loss: 0.0005\n",
      "Epoch[10/10], Step [2100/3000], Loss: 0.0067\n",
      "Epoch[10/10], Step [2200/3000], Loss: 0.0055\n",
      "Epoch[10/10], Step [2300/3000], Loss: 0.0004\n",
      "Epoch[10/10], Step [2400/3000], Loss: 0.0282\n",
      "Epoch[10/10], Step [2500/3000], Loss: 0.0001\n",
      "Epoch[10/10], Step [2600/3000], Loss: 0.0044\n",
      "Epoch[10/10], Step [2700/3000], Loss: 0.0052\n",
      "Epoch[10/10], Step [2800/3000], Loss: 0.0003\n",
      "Epoch[10/10], Step [2900/3000], Loss: 0.0005\n",
      "Epoch[10/10], Step [3000/3000], Loss: 0.0005\n",
      "Accuracy of the network no the 10,000 test images: 98.06%, with learning rate: 0.1, and [1568, 1568] hidden neurons\n",
      "\n",
      "\n",
      "\n",
      "Epoch[1/10], Step [100/3000], Loss: 0.5745\n",
      "Epoch[1/10], Step [200/3000], Loss: 0.3238\n",
      "Epoch[1/10], Step [300/3000], Loss: 0.3416\n",
      "Epoch[1/10], Step [400/3000], Loss: 0.5417\n",
      "Epoch[1/10], Step [500/3000], Loss: 0.3405\n",
      "Epoch[1/10], Step [600/3000], Loss: 0.3183\n",
      "Epoch[1/10], Step [700/3000], Loss: 0.5529\n",
      "Epoch[1/10], Step [800/3000], Loss: 0.5853\n",
      "Epoch[1/10], Step [900/3000], Loss: 0.1159\n",
      "Epoch[1/10], Step [1000/3000], Loss: 0.6376\n",
      "Epoch[1/10], Step [1100/3000], Loss: 0.4677\n",
      "Epoch[1/10], Step [1200/3000], Loss: 0.2318\n",
      "Epoch[1/10], Step [1300/3000], Loss: 0.3351\n",
      "Epoch[1/10], Step [1400/3000], Loss: 0.3151\n",
      "Epoch[1/10], Step [1500/3000], Loss: 0.1824\n",
      "Epoch[1/10], Step [1600/3000], Loss: 0.3036\n",
      "Epoch[1/10], Step [1700/3000], Loss: 0.1328\n",
      "Epoch[1/10], Step [1800/3000], Loss: 0.3173\n",
      "Epoch[1/10], Step [1900/3000], Loss: 0.2285\n",
      "Epoch[1/10], Step [2000/3000], Loss: 0.5545\n",
      "Epoch[1/10], Step [2100/3000], Loss: 0.1905\n",
      "Epoch[1/10], Step [2200/3000], Loss: 0.031\n",
      "Epoch[1/10], Step [2300/3000], Loss: 0.2294\n",
      "Epoch[1/10], Step [2400/3000], Loss: 0.8542\n",
      "Epoch[1/10], Step [2500/3000], Loss: 0.1441\n",
      "Epoch[1/10], Step [2600/3000], Loss: 0.0498\n",
      "Epoch[1/10], Step [2700/3000], Loss: 0.0826\n",
      "Epoch[1/10], Step [2800/3000], Loss: 0.0784\n",
      "Epoch[1/10], Step [2900/3000], Loss: 0.28\n",
      "Epoch[1/10], Step [3000/3000], Loss: 0.1329\n",
      "Epoch[2/10], Step [100/3000], Loss: 0.2975\n",
      "Epoch[2/10], Step [200/3000], Loss: 0.4008\n",
      "Epoch[2/10], Step [300/3000], Loss: 0.0709\n",
      "Epoch[2/10], Step [400/3000], Loss: 0.0372\n",
      "Epoch[2/10], Step [500/3000], Loss: 0.1271\n",
      "Epoch[2/10], Step [600/3000], Loss: 0.2125\n",
      "Epoch[2/10], Step [700/3000], Loss: 0.0851\n",
      "Epoch[2/10], Step [800/3000], Loss: 0.223\n",
      "Epoch[2/10], Step [900/3000], Loss: 0.2993\n",
      "Epoch[2/10], Step [1000/3000], Loss: 0.2682\n",
      "Epoch[2/10], Step [1100/3000], Loss: 0.133\n",
      "Epoch[2/10], Step [1200/3000], Loss: 0.1111\n",
      "Epoch[2/10], Step [1300/3000], Loss: 0.0673\n",
      "Epoch[2/10], Step [1400/3000], Loss: 0.1324\n",
      "Epoch[2/10], Step [1500/3000], Loss: 0.1006\n",
      "Epoch[2/10], Step [1600/3000], Loss: 0.1151\n",
      "Epoch[2/10], Step [1700/3000], Loss: 0.1039\n",
      "Epoch[2/10], Step [1800/3000], Loss: 0.0174\n",
      "Epoch[2/10], Step [1900/3000], Loss: 0.0491\n",
      "Epoch[2/10], Step [2000/3000], Loss: 0.1639\n",
      "Epoch[2/10], Step [2100/3000], Loss: 0.0858\n",
      "Epoch[2/10], Step [2200/3000], Loss: 0.1142\n",
      "Epoch[2/10], Step [2300/3000], Loss: 0.192\n",
      "Epoch[2/10], Step [2400/3000], Loss: 0.1665\n",
      "Epoch[2/10], Step [2500/3000], Loss: 0.1519\n",
      "Epoch[2/10], Step [2600/3000], Loss: 0.2954\n",
      "Epoch[2/10], Step [2700/3000], Loss: 0.0189\n",
      "Epoch[2/10], Step [2800/3000], Loss: 0.0386\n",
      "Epoch[2/10], Step [2900/3000], Loss: 0.152\n",
      "Epoch[2/10], Step [3000/3000], Loss: 0.2099\n",
      "Epoch[3/10], Step [100/3000], Loss: 0.184\n",
      "Epoch[3/10], Step [200/3000], Loss: 0.1633\n",
      "Epoch[3/10], Step [300/3000], Loss: 0.5423\n",
      "Epoch[3/10], Step [400/3000], Loss: 0.261\n",
      "Epoch[3/10], Step [500/3000], Loss: 0.0459\n",
      "Epoch[3/10], Step [600/3000], Loss: 0.0332\n",
      "Epoch[3/10], Step [700/3000], Loss: 0.0604\n",
      "Epoch[3/10], Step [800/3000], Loss: 0.045\n",
      "Epoch[3/10], Step [900/3000], Loss: 0.1003\n",
      "Epoch[3/10], Step [1000/3000], Loss: 0.6381\n",
      "Epoch[3/10], Step [1100/3000], Loss: 0.0368\n",
      "Epoch[3/10], Step [1200/3000], Loss: 0.073\n",
      "Epoch[3/10], Step [1300/3000], Loss: 0.0103\n",
      "Epoch[3/10], Step [1400/3000], Loss: 0.0089\n",
      "Epoch[3/10], Step [1500/3000], Loss: 0.0102\n",
      "Epoch[3/10], Step [1600/3000], Loss: 0.0792\n",
      "Epoch[3/10], Step [1700/3000], Loss: 0.0111\n",
      "Epoch[3/10], Step [1800/3000], Loss: 0.1144\n",
      "Epoch[3/10], Step [1900/3000], Loss: 0.0257\n",
      "Epoch[3/10], Step [2000/3000], Loss: 0.0779\n",
      "Epoch[3/10], Step [2100/3000], Loss: 0.0661\n",
      "Epoch[3/10], Step [2200/3000], Loss: 0.0093\n",
      "Epoch[3/10], Step [2300/3000], Loss: 0.0071\n",
      "Epoch[3/10], Step [2400/3000], Loss: 0.1319\n",
      "Epoch[3/10], Step [2500/3000], Loss: 0.0632\n",
      "Epoch[3/10], Step [2600/3000], Loss: 0.0391\n",
      "Epoch[3/10], Step [2700/3000], Loss: 0.0164\n",
      "Epoch[3/10], Step [2800/3000], Loss: 0.0566\n",
      "Epoch[3/10], Step [2900/3000], Loss: 0.1129\n",
      "Epoch[3/10], Step [3000/3000], Loss: 0.0259\n",
      "Epoch[4/10], Step [100/3000], Loss: 0.0256\n",
      "Epoch[4/10], Step [200/3000], Loss: 0.0266\n",
      "Epoch[4/10], Step [300/3000], Loss: 0.0066\n",
      "Epoch[4/10], Step [400/3000], Loss: 0.0861\n",
      "Epoch[4/10], Step [500/3000], Loss: 0.0345\n",
      "Epoch[4/10], Step [600/3000], Loss: 0.1467\n",
      "Epoch[4/10], Step [700/3000], Loss: 0.0141\n",
      "Epoch[4/10], Step [800/3000], Loss: 0.0873\n",
      "Epoch[4/10], Step [900/3000], Loss: 0.006\n",
      "Epoch[4/10], Step [1000/3000], Loss: 0.398\n",
      "Epoch[4/10], Step [1100/3000], Loss: 0.252\n",
      "Epoch[4/10], Step [1200/3000], Loss: 0.0331\n",
      "Epoch[4/10], Step [1300/3000], Loss: 0.0846\n",
      "Epoch[4/10], Step [1400/3000], Loss: 0.0872\n",
      "Epoch[4/10], Step [1500/3000], Loss: 0.0071\n",
      "Epoch[4/10], Step [1600/3000], Loss: 0.0138\n",
      "Epoch[4/10], Step [1700/3000], Loss: 0.0679\n",
      "Epoch[4/10], Step [1800/3000], Loss: 0.0916\n",
      "Epoch[4/10], Step [1900/3000], Loss: 0.0159\n",
      "Epoch[4/10], Step [2000/3000], Loss: 0.0081\n",
      "Epoch[4/10], Step [2100/3000], Loss: 0.0532\n",
      "Epoch[4/10], Step [2200/3000], Loss: 0.0121\n",
      "Epoch[4/10], Step [2300/3000], Loss: 0.0043\n",
      "Epoch[4/10], Step [2400/3000], Loss: 0.01\n",
      "Epoch[4/10], Step [2500/3000], Loss: 0.1154\n",
      "Epoch[4/10], Step [2600/3000], Loss: 0.0573\n",
      "Epoch[4/10], Step [2700/3000], Loss: 0.054\n",
      "Epoch[4/10], Step [2800/3000], Loss: 0.0076\n",
      "Epoch[4/10], Step [2900/3000], Loss: 0.2352\n",
      "Epoch[4/10], Step [3000/3000], Loss: 0.0243\n",
      "Epoch[5/10], Step [100/3000], Loss: 0.0657\n",
      "Epoch[5/10], Step [200/3000], Loss: 0.0066\n",
      "Epoch[5/10], Step [300/3000], Loss: 0.0083\n",
      "Epoch[5/10], Step [400/3000], Loss: 0.0562\n",
      "Epoch[5/10], Step [500/3000], Loss: 0.0479\n",
      "Epoch[5/10], Step [600/3000], Loss: 0.018\n",
      "Epoch[5/10], Step [700/3000], Loss: 0.0107\n",
      "Epoch[5/10], Step [800/3000], Loss: 0.0703\n",
      "Epoch[5/10], Step [900/3000], Loss: 0.0024\n",
      "Epoch[5/10], Step [1000/3000], Loss: 0.1158\n",
      "Epoch[5/10], Step [1100/3000], Loss: 0.0275\n",
      "Epoch[5/10], Step [1200/3000], Loss: 0.0199\n",
      "Epoch[5/10], Step [1300/3000], Loss: 0.1042\n",
      "Epoch[5/10], Step [1400/3000], Loss: 0.0079\n",
      "Epoch[5/10], Step [1500/3000], Loss: 0.1614\n",
      "Epoch[5/10], Step [1600/3000], Loss: 0.0069\n",
      "Epoch[5/10], Step [1700/3000], Loss: 0.3774\n",
      "Epoch[5/10], Step [1800/3000], Loss: 0.0105\n",
      "Epoch[5/10], Step [1900/3000], Loss: 0.0047\n",
      "Epoch[5/10], Step [2000/3000], Loss: 0.0931\n",
      "Epoch[5/10], Step [2100/3000], Loss: 0.0234\n",
      "Epoch[5/10], Step [2200/3000], Loss: 0.0793\n",
      "Epoch[5/10], Step [2300/3000], Loss: 0.0163\n",
      "Epoch[5/10], Step [2400/3000], Loss: 0.0402\n",
      "Epoch[5/10], Step [2500/3000], Loss: 0.3934\n",
      "Epoch[5/10], Step [2600/3000], Loss: 0.0311\n",
      "Epoch[5/10], Step [2700/3000], Loss: 0.0256\n",
      "Epoch[5/10], Step [2800/3000], Loss: 0.0256\n",
      "Epoch[5/10], Step [2900/3000], Loss: 0.0019\n",
      "Epoch[5/10], Step [3000/3000], Loss: 0.0119\n",
      "Epoch[6/10], Step [100/3000], Loss: 0.0055\n",
      "Epoch[6/10], Step [200/3000], Loss: 0.0252\n",
      "Epoch[6/10], Step [300/3000], Loss: 0.0739\n",
      "Epoch[6/10], Step [400/3000], Loss: 0.0081\n",
      "Epoch[6/10], Step [500/3000], Loss: 0.0086\n",
      "Epoch[6/10], Step [600/3000], Loss: 0.0201\n",
      "Epoch[6/10], Step [700/3000], Loss: 0.0024\n",
      "Epoch[6/10], Step [800/3000], Loss: 0.0059\n",
      "Epoch[6/10], Step [900/3000], Loss: 0.0013\n",
      "Epoch[6/10], Step [1000/3000], Loss: 0.1525\n",
      "Epoch[6/10], Step [1100/3000], Loss: 0.0145\n",
      "Epoch[6/10], Step [1200/3000], Loss: 0.0073\n",
      "Epoch[6/10], Step [1300/3000], Loss: 0.0029\n",
      "Epoch[6/10], Step [1400/3000], Loss: 0.0252\n",
      "Epoch[6/10], Step [1500/3000], Loss: 0.0156\n",
      "Epoch[6/10], Step [1600/3000], Loss: 0.0028\n",
      "Epoch[6/10], Step [1700/3000], Loss: 0.0042\n",
      "Epoch[6/10], Step [1800/3000], Loss: 0.0241\n",
      "Epoch[6/10], Step [1900/3000], Loss: 0.0404\n",
      "Epoch[6/10], Step [2000/3000], Loss: 0.0006\n",
      "Epoch[6/10], Step [2100/3000], Loss: 0.0032\n",
      "Epoch[6/10], Step [2200/3000], Loss: 0.011\n",
      "Epoch[6/10], Step [2300/3000], Loss: 0.0507\n",
      "Epoch[6/10], Step [2400/3000], Loss: 0.0196\n",
      "Epoch[6/10], Step [2500/3000], Loss: 0.0107\n",
      "Epoch[6/10], Step [2600/3000], Loss: 0.0223\n",
      "Epoch[6/10], Step [2700/3000], Loss: 0.0104\n",
      "Epoch[6/10], Step [2800/3000], Loss: 0.0091\n",
      "Epoch[6/10], Step [2900/3000], Loss: 0.0019\n",
      "Epoch[6/10], Step [3000/3000], Loss: 0.0684\n",
      "Epoch[7/10], Step [100/3000], Loss: 0.0061\n",
      "Epoch[7/10], Step [200/3000], Loss: 0.0054\n",
      "Epoch[7/10], Step [300/3000], Loss: 0.0017\n",
      "Epoch[7/10], Step [400/3000], Loss: 0.0352\n",
      "Epoch[7/10], Step [500/3000], Loss: 0.0107\n",
      "Epoch[7/10], Step [600/3000], Loss: 0.0245\n",
      "Epoch[7/10], Step [700/3000], Loss: 0.1029\n",
      "Epoch[7/10], Step [800/3000], Loss: 0.0092\n",
      "Epoch[7/10], Step [900/3000], Loss: 0.0595\n",
      "Epoch[7/10], Step [1000/3000], Loss: 0.0062\n",
      "Epoch[7/10], Step [1100/3000], Loss: 0.0293\n",
      "Epoch[7/10], Step [1200/3000], Loss: 0.0036\n",
      "Epoch[7/10], Step [1300/3000], Loss: 0.0076\n",
      "Epoch[7/10], Step [1400/3000], Loss: 0.0497\n",
      "Epoch[7/10], Step [1500/3000], Loss: 0.1526\n",
      "Epoch[7/10], Step [1600/3000], Loss: 0.0044\n",
      "Epoch[7/10], Step [1700/3000], Loss: 0.0055\n",
      "Epoch[7/10], Step [1800/3000], Loss: 0.0031\n",
      "Epoch[7/10], Step [1900/3000], Loss: 0.0068\n",
      "Epoch[7/10], Step [2000/3000], Loss: 0.0203\n",
      "Epoch[7/10], Step [2100/3000], Loss: 0.0011\n",
      "Epoch[7/10], Step [2200/3000], Loss: 0.0255\n",
      "Epoch[7/10], Step [2300/3000], Loss: 0.0013\n",
      "Epoch[7/10], Step [2400/3000], Loss: 0.0039\n",
      "Epoch[7/10], Step [2500/3000], Loss: 0.0548\n",
      "Epoch[7/10], Step [2600/3000], Loss: 0.0056\n",
      "Epoch[7/10], Step [2700/3000], Loss: 0.0026\n",
      "Epoch[7/10], Step [2800/3000], Loss: 0.0043\n",
      "Epoch[7/10], Step [2900/3000], Loss: 0.1039\n",
      "Epoch[7/10], Step [3000/3000], Loss: 0.0271\n",
      "Epoch[8/10], Step [100/3000], Loss: 0.0124\n",
      "Epoch[8/10], Step [200/3000], Loss: 0.0341\n",
      "Epoch[8/10], Step [300/3000], Loss: 0.0119\n",
      "Epoch[8/10], Step [400/3000], Loss: 0.0168\n",
      "Epoch[8/10], Step [500/3000], Loss: 0.0014\n",
      "Epoch[8/10], Step [600/3000], Loss: 0.0333\n",
      "Epoch[8/10], Step [700/3000], Loss: 0.0013\n",
      "Epoch[8/10], Step [800/3000], Loss: 0.0135\n",
      "Epoch[8/10], Step [900/3000], Loss: 0.0022\n",
      "Epoch[8/10], Step [1000/3000], Loss: 0.1115\n",
      "Epoch[8/10], Step [1100/3000], Loss: 0.0185\n",
      "Epoch[8/10], Step [1200/3000], Loss: 0.0136\n",
      "Epoch[8/10], Step [1300/3000], Loss: 0.0012\n",
      "Epoch[8/10], Step [1400/3000], Loss: 0.0198\n",
      "Epoch[8/10], Step [1500/3000], Loss: 0.2092\n",
      "Epoch[8/10], Step [1600/3000], Loss: 0.0049\n",
      "Epoch[8/10], Step [1700/3000], Loss: 0.0242\n",
      "Epoch[8/10], Step [1800/3000], Loss: 0.0072\n",
      "Epoch[8/10], Step [1900/3000], Loss: 0.038\n",
      "Epoch[8/10], Step [2000/3000], Loss: 0.0108\n",
      "Epoch[8/10], Step [2100/3000], Loss: 0.0656\n",
      "Epoch[8/10], Step [2200/3000], Loss: 0.0018\n",
      "Epoch[8/10], Step [2300/3000], Loss: 0.0319\n",
      "Epoch[8/10], Step [2400/3000], Loss: 0.0634\n",
      "Epoch[8/10], Step [2500/3000], Loss: 0.0106\n",
      "Epoch[8/10], Step [2600/3000], Loss: 0.0009\n",
      "Epoch[8/10], Step [2700/3000], Loss: 0.0006\n",
      "Epoch[8/10], Step [2800/3000], Loss: 0.0075\n",
      "Epoch[8/10], Step [2900/3000], Loss: 0.2315\n",
      "Epoch[8/10], Step [3000/3000], Loss: 0.034\n",
      "Epoch[9/10], Step [100/3000], Loss: 0.0031\n",
      "Epoch[9/10], Step [200/3000], Loss: 0.0033\n",
      "Epoch[9/10], Step [300/3000], Loss: 0.0074\n",
      "Epoch[9/10], Step [400/3000], Loss: 0.0745\n",
      "Epoch[9/10], Step [500/3000], Loss: 0.0024\n",
      "Epoch[9/10], Step [600/3000], Loss: 0.0227\n",
      "Epoch[9/10], Step [700/3000], Loss: 0.006\n",
      "Epoch[9/10], Step [800/3000], Loss: 0.0074\n",
      "Epoch[9/10], Step [900/3000], Loss: 0.0011\n",
      "Epoch[9/10], Step [1000/3000], Loss: 0.0015\n",
      "Epoch[9/10], Step [1100/3000], Loss: 0.0088\n",
      "Epoch[9/10], Step [1200/3000], Loss: 0.0052\n",
      "Epoch[9/10], Step [1300/3000], Loss: 0.0889\n",
      "Epoch[9/10], Step [1400/3000], Loss: 0.0012\n",
      "Epoch[9/10], Step [1500/3000], Loss: 0.0264\n",
      "Epoch[9/10], Step [1600/3000], Loss: 0.0253\n",
      "Epoch[9/10], Step [1700/3000], Loss: 0.0218\n",
      "Epoch[9/10], Step [1800/3000], Loss: 0.1078\n",
      "Epoch[9/10], Step [1900/3000], Loss: 0.0027\n",
      "Epoch[9/10], Step [2000/3000], Loss: 0.0008\n",
      "Epoch[9/10], Step [2100/3000], Loss: 0.0598\n",
      "Epoch[9/10], Step [2200/3000], Loss: 0.0013\n",
      "Epoch[9/10], Step [2300/3000], Loss: 0.015\n",
      "Epoch[9/10], Step [2400/3000], Loss: 0.0299\n",
      "Epoch[9/10], Step [2500/3000], Loss: 0.0299\n",
      "Epoch[9/10], Step [2600/3000], Loss: 0.1743\n",
      "Epoch[9/10], Step [2700/3000], Loss: 0.0334\n",
      "Epoch[9/10], Step [2800/3000], Loss: 0.0442\n",
      "Epoch[9/10], Step [2900/3000], Loss: 0.0249\n",
      "Epoch[9/10], Step [3000/3000], Loss: 0.1641\n",
      "Epoch[10/10], Step [100/3000], Loss: 0.078\n",
      "Epoch[10/10], Step [200/3000], Loss: 0.0027\n",
      "Epoch[10/10], Step [300/3000], Loss: 0.0009\n",
      "Epoch[10/10], Step [400/3000], Loss: 0.0027\n",
      "Epoch[10/10], Step [500/3000], Loss: 0.0141\n",
      "Epoch[10/10], Step [600/3000], Loss: 0.0109\n",
      "Epoch[10/10], Step [700/3000], Loss: 0.0873\n",
      "Epoch[10/10], Step [800/3000], Loss: 0.0013\n",
      "Epoch[10/10], Step [900/3000], Loss: 0.0002\n",
      "Epoch[10/10], Step [1000/3000], Loss: 0.0027\n",
      "Epoch[10/10], Step [1100/3000], Loss: 0.0042\n",
      "Epoch[10/10], Step [1200/3000], Loss: 0.0034\n",
      "Epoch[10/10], Step [1300/3000], Loss: 0.0041\n",
      "Epoch[10/10], Step [1400/3000], Loss: 0.1591\n",
      "Epoch[10/10], Step [1500/3000], Loss: 0.0042\n",
      "Epoch[10/10], Step [1600/3000], Loss: 0.0081\n",
      "Epoch[10/10], Step [1700/3000], Loss: 0.011\n",
      "Epoch[10/10], Step [1800/3000], Loss: 0.026\n",
      "Epoch[10/10], Step [1900/3000], Loss: 0.0028\n",
      "Epoch[10/10], Step [2000/3000], Loss: 0.0022\n",
      "Epoch[10/10], Step [2100/3000], Loss: 0.0022\n",
      "Epoch[10/10], Step [2200/3000], Loss: 0.0011\n",
      "Epoch[10/10], Step [2300/3000], Loss: 0.0131\n",
      "Epoch[10/10], Step [2400/3000], Loss: 0.0529\n",
      "Epoch[10/10], Step [2500/3000], Loss: 0.0005\n",
      "Epoch[10/10], Step [2600/3000], Loss: 0.0007\n",
      "Epoch[10/10], Step [2700/3000], Loss: 0.0009\n",
      "Epoch[10/10], Step [2800/3000], Loss: 0.0028\n",
      "Epoch[10/10], Step [2900/3000], Loss: 0.031\n",
      "Epoch[10/10], Step [3000/3000], Loss: 0.0065\n",
      "Accuracy of the network no the 10,000 test images: 98.09%, with learning rate: 0.05, and [3136, 1568] hidden neurons\n",
      "\n",
      "\n",
      "\n",
      "Epoch[1/10], Step [100/3000], Loss: 0.7142\n",
      "Epoch[1/10], Step [200/3000], Loss: 0.2548\n",
      "Epoch[1/10], Step [300/3000], Loss: 0.208\n",
      "Epoch[1/10], Step [400/3000], Loss: 0.5742\n",
      "Epoch[1/10], Step [500/3000], Loss: 0.1025\n",
      "Epoch[1/10], Step [600/3000], Loss: 0.108\n",
      "Epoch[1/10], Step [700/3000], Loss: 0.294\n",
      "Epoch[1/10], Step [800/3000], Loss: 0.5694\n",
      "Epoch[1/10], Step [900/3000], Loss: 0.1447\n",
      "Epoch[1/10], Step [1000/3000], Loss: 0.23\n",
      "Epoch[1/10], Step [1100/3000], Loss: 0.0618\n",
      "Epoch[1/10], Step [1200/3000], Loss: 0.2712\n",
      "Epoch[1/10], Step [1300/3000], Loss: 0.0895\n",
      "Epoch[1/10], Step [1400/3000], Loss: 0.3828\n",
      "Epoch[1/10], Step [1500/3000], Loss: 0.3944\n",
      "Epoch[1/10], Step [1600/3000], Loss: 0.1623\n",
      "Epoch[1/10], Step [1700/3000], Loss: 0.1056\n",
      "Epoch[1/10], Step [1800/3000], Loss: 0.0868\n",
      "Epoch[1/10], Step [1900/3000], Loss: 0.4665\n",
      "Epoch[1/10], Step [2000/3000], Loss: 0.0229\n",
      "Epoch[1/10], Step [2100/3000], Loss: 0.1336\n",
      "Epoch[1/10], Step [2200/3000], Loss: 0.0073\n",
      "Epoch[1/10], Step [2300/3000], Loss: 0.3091\n",
      "Epoch[1/10], Step [2400/3000], Loss: 0.1192\n",
      "Epoch[1/10], Step [2500/3000], Loss: 0.0775\n",
      "Epoch[1/10], Step [2600/3000], Loss: 0.1333\n",
      "Epoch[1/10], Step [2700/3000], Loss: 0.1091\n",
      "Epoch[1/10], Step [2800/3000], Loss: 0.0165\n",
      "Epoch[1/10], Step [2900/3000], Loss: 0.0702\n",
      "Epoch[1/10], Step [3000/3000], Loss: 0.023\n",
      "Epoch[2/10], Step [100/3000], Loss: 0.0058\n",
      "Epoch[2/10], Step [200/3000], Loss: 0.0127\n",
      "Epoch[2/10], Step [300/3000], Loss: 0.0121\n",
      "Epoch[2/10], Step [400/3000], Loss: 0.0426\n",
      "Epoch[2/10], Step [500/3000], Loss: 0.0731\n",
      "Epoch[2/10], Step [600/3000], Loss: 0.113\n",
      "Epoch[2/10], Step [700/3000], Loss: 0.1266\n",
      "Epoch[2/10], Step [800/3000], Loss: 0.0636\n",
      "Epoch[2/10], Step [900/3000], Loss: 0.0152\n",
      "Epoch[2/10], Step [1000/3000], Loss: 0.2695\n",
      "Epoch[2/10], Step [1100/3000], Loss: 0.0296\n",
      "Epoch[2/10], Step [1200/3000], Loss: 0.0597\n",
      "Epoch[2/10], Step [1300/3000], Loss: 0.0163\n",
      "Epoch[2/10], Step [1400/3000], Loss: 0.0392\n",
      "Epoch[2/10], Step [1500/3000], Loss: 0.0257\n",
      "Epoch[2/10], Step [1600/3000], Loss: 0.0488\n",
      "Epoch[2/10], Step [1700/3000], Loss: 0.0242\n",
      "Epoch[2/10], Step [1800/3000], Loss: 0.0862\n",
      "Epoch[2/10], Step [1900/3000], Loss: 0.1795\n",
      "Epoch[2/10], Step [2000/3000], Loss: 0.0408\n",
      "Epoch[2/10], Step [2100/3000], Loss: 0.0094\n",
      "Epoch[2/10], Step [2200/3000], Loss: 0.0244\n",
      "Epoch[2/10], Step [2300/3000], Loss: 0.0666\n",
      "Epoch[2/10], Step [2400/3000], Loss: 0.0321\n",
      "Epoch[2/10], Step [2500/3000], Loss: 0.1295\n",
      "Epoch[2/10], Step [2600/3000], Loss: 0.1448\n",
      "Epoch[2/10], Step [2700/3000], Loss: 0.2873\n",
      "Epoch[2/10], Step [2800/3000], Loss: 0.0484\n",
      "Epoch[2/10], Step [2900/3000], Loss: 0.0146\n",
      "Epoch[2/10], Step [3000/3000], Loss: 0.0089\n",
      "Epoch[3/10], Step [100/3000], Loss: 0.0352\n",
      "Epoch[3/10], Step [200/3000], Loss: 0.0148\n",
      "Epoch[3/10], Step [300/3000], Loss: 0.1587\n",
      "Epoch[3/10], Step [400/3000], Loss: 0.0475\n",
      "Epoch[3/10], Step [500/3000], Loss: 0.238\n",
      "Epoch[3/10], Step [600/3000], Loss: 0.1026\n",
      "Epoch[3/10], Step [700/3000], Loss: 0.0151\n",
      "Epoch[3/10], Step [800/3000], Loss: 0.0055\n",
      "Epoch[3/10], Step [900/3000], Loss: 0.0812\n",
      "Epoch[3/10], Step [1000/3000], Loss: 0.0388\n",
      "Epoch[3/10], Step [1100/3000], Loss: 0.0539\n",
      "Epoch[3/10], Step [1200/3000], Loss: 0.0274\n",
      "Epoch[3/10], Step [1300/3000], Loss: 0.0266\n",
      "Epoch[3/10], Step [1400/3000], Loss: 0.0109\n",
      "Epoch[3/10], Step [1500/3000], Loss: 0.0441\n",
      "Epoch[3/10], Step [1600/3000], Loss: 0.023\n",
      "Epoch[3/10], Step [1700/3000], Loss: 0.1293\n",
      "Epoch[3/10], Step [1800/3000], Loss: 0.0208\n",
      "Epoch[3/10], Step [1900/3000], Loss: 0.0168\n",
      "Epoch[3/10], Step [2000/3000], Loss: 0.0671\n",
      "Epoch[3/10], Step [2100/3000], Loss: 0.0247\n",
      "Epoch[3/10], Step [2200/3000], Loss: 0.0354\n",
      "Epoch[3/10], Step [2300/3000], Loss: 0.0024\n",
      "Epoch[3/10], Step [2400/3000], Loss: 0.0954\n",
      "Epoch[3/10], Step [2500/3000], Loss: 0.1618\n",
      "Epoch[3/10], Step [2600/3000], Loss: 0.0559\n",
      "Epoch[3/10], Step [2700/3000], Loss: 0.0308\n",
      "Epoch[3/10], Step [2800/3000], Loss: 0.0132\n",
      "Epoch[3/10], Step [2900/3000], Loss: 0.0051\n",
      "Epoch[3/10], Step [3000/3000], Loss: 0.0543\n",
      "Epoch[4/10], Step [100/3000], Loss: 0.0247\n",
      "Epoch[4/10], Step [200/3000], Loss: 0.008\n",
      "Epoch[4/10], Step [300/3000], Loss: 0.0067\n",
      "Epoch[4/10], Step [400/3000], Loss: 0.0108\n",
      "Epoch[4/10], Step [500/3000], Loss: 0.119\n",
      "Epoch[4/10], Step [600/3000], Loss: 0.0099\n",
      "Epoch[4/10], Step [700/3000], Loss: 0.0415\n",
      "Epoch[4/10], Step [800/3000], Loss: 0.2718\n",
      "Epoch[4/10], Step [900/3000], Loss: 0.1694\n",
      "Epoch[4/10], Step [1000/3000], Loss: 0.0177\n",
      "Epoch[4/10], Step [1100/3000], Loss: 0.0006\n",
      "Epoch[4/10], Step [1200/3000], Loss: 0.0542\n",
      "Epoch[4/10], Step [1300/3000], Loss: 0.206\n",
      "Epoch[4/10], Step [1400/3000], Loss: 0.0591\n",
      "Epoch[4/10], Step [1500/3000], Loss: 0.014\n",
      "Epoch[4/10], Step [1600/3000], Loss: 0.0777\n",
      "Epoch[4/10], Step [1700/3000], Loss: 0.0268\n",
      "Epoch[4/10], Step [1800/3000], Loss: 0.0052\n",
      "Epoch[4/10], Step [1900/3000], Loss: 0.0666\n",
      "Epoch[4/10], Step [2000/3000], Loss: 0.0077\n",
      "Epoch[4/10], Step [2100/3000], Loss: 0.2903\n",
      "Epoch[4/10], Step [2200/3000], Loss: 0.0088\n",
      "Epoch[4/10], Step [2300/3000], Loss: 0.0247\n",
      "Epoch[4/10], Step [2400/3000], Loss: 0.0129\n",
      "Epoch[4/10], Step [2500/3000], Loss: 0.0116\n",
      "Epoch[4/10], Step [2600/3000], Loss: 0.0255\n",
      "Epoch[4/10], Step [2700/3000], Loss: 0.0155\n",
      "Epoch[4/10], Step [2800/3000], Loss: 0.055\n",
      "Epoch[4/10], Step [2900/3000], Loss: 0.0162\n",
      "Epoch[4/10], Step [3000/3000], Loss: 0.0267\n",
      "Epoch[5/10], Step [100/3000], Loss: 0.0029\n",
      "Epoch[5/10], Step [200/3000], Loss: 0.003\n",
      "Epoch[5/10], Step [300/3000], Loss: 0.0032\n",
      "Epoch[5/10], Step [400/3000], Loss: 0.4877\n",
      "Epoch[5/10], Step [500/3000], Loss: 0.015\n",
      "Epoch[5/10], Step [600/3000], Loss: 0.002\n",
      "Epoch[5/10], Step [700/3000], Loss: 0.0205\n",
      "Epoch[5/10], Step [800/3000], Loss: 0.0068\n",
      "Epoch[5/10], Step [900/3000], Loss: 0.0009\n",
      "Epoch[5/10], Step [1000/3000], Loss: 0.0473\n",
      "Epoch[5/10], Step [1100/3000], Loss: 0.0325\n",
      "Epoch[5/10], Step [1200/3000], Loss: 0.0019\n",
      "Epoch[5/10], Step [1300/3000], Loss: 0.0013\n",
      "Epoch[5/10], Step [1400/3000], Loss: 0.0108\n",
      "Epoch[5/10], Step [1500/3000], Loss: 0.0065\n",
      "Epoch[5/10], Step [1600/3000], Loss: 0.0854\n",
      "Epoch[5/10], Step [1700/3000], Loss: 0.0047\n",
      "Epoch[5/10], Step [1800/3000], Loss: 0.0088\n",
      "Epoch[5/10], Step [1900/3000], Loss: 0.0073\n",
      "Epoch[5/10], Step [2000/3000], Loss: 0.0454\n",
      "Epoch[5/10], Step [2100/3000], Loss: 0.018\n",
      "Epoch[5/10], Step [2200/3000], Loss: 0.0305\n",
      "Epoch[5/10], Step [2300/3000], Loss: 0.0123\n",
      "Epoch[5/10], Step [2400/3000], Loss: 0.0343\n",
      "Epoch[5/10], Step [2500/3000], Loss: 0.0011\n",
      "Epoch[5/10], Step [2600/3000], Loss: 0.0036\n",
      "Epoch[5/10], Step [2700/3000], Loss: 0.0832\n",
      "Epoch[5/10], Step [2800/3000], Loss: 0.001\n",
      "Epoch[5/10], Step [2900/3000], Loss: 0.0036\n",
      "Epoch[5/10], Step [3000/3000], Loss: 0.01\n",
      "Epoch[6/10], Step [100/3000], Loss: 0.0125\n",
      "Epoch[6/10], Step [200/3000], Loss: 0.0824\n",
      "Epoch[6/10], Step [300/3000], Loss: 0.0169\n",
      "Epoch[6/10], Step [400/3000], Loss: 0.0045\n",
      "Epoch[6/10], Step [500/3000], Loss: 0.0362\n",
      "Epoch[6/10], Step [600/3000], Loss: 0.0059\n",
      "Epoch[6/10], Step [700/3000], Loss: 0.001\n",
      "Epoch[6/10], Step [800/3000], Loss: 0.012\n",
      "Epoch[6/10], Step [900/3000], Loss: 0.0018\n",
      "Epoch[6/10], Step [1000/3000], Loss: 0.0265\n",
      "Epoch[6/10], Step [1100/3000], Loss: 0.0049\n",
      "Epoch[6/10], Step [1200/3000], Loss: 0.0005\n",
      "Epoch[6/10], Step [1300/3000], Loss: 0.0043\n",
      "Epoch[6/10], Step [1400/3000], Loss: 0.024\n",
      "Epoch[6/10], Step [1500/3000], Loss: 0.0397\n",
      "Epoch[6/10], Step [1600/3000], Loss: 0.0138\n",
      "Epoch[6/10], Step [1700/3000], Loss: 0.0097\n",
      "Epoch[6/10], Step [1800/3000], Loss: 0.0165\n",
      "Epoch[6/10], Step [1900/3000], Loss: 0.0026\n",
      "Epoch[6/10], Step [2000/3000], Loss: 0.011\n",
      "Epoch[6/10], Step [2100/3000], Loss: 0.0938\n",
      "Epoch[6/10], Step [2200/3000], Loss: 0.0181\n",
      "Epoch[6/10], Step [2300/3000], Loss: 0.0267\n",
      "Epoch[6/10], Step [2400/3000], Loss: 0.0095\n",
      "Epoch[6/10], Step [2500/3000], Loss: 0.0003\n",
      "Epoch[6/10], Step [2600/3000], Loss: 0.0011\n",
      "Epoch[6/10], Step [2700/3000], Loss: 0.0533\n",
      "Epoch[6/10], Step [2800/3000], Loss: 0.0054\n",
      "Epoch[6/10], Step [2900/3000], Loss: 0.0162\n",
      "Epoch[6/10], Step [3000/3000], Loss: 0.0716\n",
      "Epoch[7/10], Step [100/3000], Loss: 0.0002\n",
      "Epoch[7/10], Step [200/3000], Loss: 0.0\n",
      "Epoch[7/10], Step [300/3000], Loss: 0.0138\n",
      "Epoch[7/10], Step [400/3000], Loss: 0.0142\n",
      "Epoch[7/10], Step [500/3000], Loss: 0.0081\n",
      "Epoch[7/10], Step [600/3000], Loss: 0.0133\n",
      "Epoch[7/10], Step [700/3000], Loss: 0.0031\n",
      "Epoch[7/10], Step [800/3000], Loss: 0.0557\n",
      "Epoch[7/10], Step [900/3000], Loss: 0.1088\n",
      "Epoch[7/10], Step [1000/3000], Loss: 0.0403\n",
      "Epoch[7/10], Step [1100/3000], Loss: 0.0184\n",
      "Epoch[7/10], Step [1200/3000], Loss: 0.0232\n",
      "Epoch[7/10], Step [1300/3000], Loss: 0.0009\n",
      "Epoch[7/10], Step [1400/3000], Loss: 0.0146\n",
      "Epoch[7/10], Step [1500/3000], Loss: 0.0008\n",
      "Epoch[7/10], Step [1600/3000], Loss: 0.0056\n",
      "Epoch[7/10], Step [1700/3000], Loss: 0.0016\n",
      "Epoch[7/10], Step [1800/3000], Loss: 0.0776\n",
      "Epoch[7/10], Step [1900/3000], Loss: 0.0172\n",
      "Epoch[7/10], Step [2000/3000], Loss: 0.0015\n",
      "Epoch[7/10], Step [2100/3000], Loss: 0.0017\n",
      "Epoch[7/10], Step [2200/3000], Loss: 0.0908\n",
      "Epoch[7/10], Step [2300/3000], Loss: 0.0109\n",
      "Epoch[7/10], Step [2400/3000], Loss: 0.024\n",
      "Epoch[7/10], Step [2500/3000], Loss: 0.0021\n",
      "Epoch[7/10], Step [2600/3000], Loss: 0.0024\n",
      "Epoch[7/10], Step [2700/3000], Loss: 0.0201\n",
      "Epoch[7/10], Step [2800/3000], Loss: 0.0943\n",
      "Epoch[7/10], Step [2900/3000], Loss: 0.0013\n",
      "Epoch[7/10], Step [3000/3000], Loss: 0.0524\n",
      "Epoch[8/10], Step [100/3000], Loss: 0.0003\n",
      "Epoch[8/10], Step [200/3000], Loss: 0.0076\n",
      "Epoch[8/10], Step [300/3000], Loss: 0.0029\n",
      "Epoch[8/10], Step [400/3000], Loss: 0.0011\n",
      "Epoch[8/10], Step [500/3000], Loss: 0.0007\n",
      "Epoch[8/10], Step [600/3000], Loss: 0.0008\n",
      "Epoch[8/10], Step [700/3000], Loss: 0.0005\n",
      "Epoch[8/10], Step [800/3000], Loss: 0.0125\n",
      "Epoch[8/10], Step [900/3000], Loss: 0.0006\n",
      "Epoch[8/10], Step [1000/3000], Loss: 0.0089\n",
      "Epoch[8/10], Step [1100/3000], Loss: 0.0015\n",
      "Epoch[8/10], Step [1200/3000], Loss: 0.0009\n",
      "Epoch[8/10], Step [1300/3000], Loss: 0.0009\n",
      "Epoch[8/10], Step [1400/3000], Loss: 0.0255\n",
      "Epoch[8/10], Step [1500/3000], Loss: 0.0002\n",
      "Epoch[8/10], Step [1600/3000], Loss: 0.0055\n",
      "Epoch[8/10], Step [1700/3000], Loss: 0.002\n",
      "Epoch[8/10], Step [1800/3000], Loss: 0.0008\n",
      "Epoch[8/10], Step [1900/3000], Loss: 0.0034\n",
      "Epoch[8/10], Step [2000/3000], Loss: 0.0002\n",
      "Epoch[8/10], Step [2100/3000], Loss: 0.0126\n",
      "Epoch[8/10], Step [2200/3000], Loss: 0.0011\n",
      "Epoch[8/10], Step [2300/3000], Loss: 0.0035\n",
      "Epoch[8/10], Step [2400/3000], Loss: 0.0033\n",
      "Epoch[8/10], Step [2500/3000], Loss: 0.0045\n",
      "Epoch[8/10], Step [2600/3000], Loss: 0.0004\n",
      "Epoch[8/10], Step [2700/3000], Loss: 0.0001\n",
      "Epoch[8/10], Step [2800/3000], Loss: 0.0819\n",
      "Epoch[8/10], Step [2900/3000], Loss: 0.0056\n",
      "Epoch[8/10], Step [3000/3000], Loss: 0.0325\n",
      "Epoch[9/10], Step [100/3000], Loss: 0.0016\n",
      "Epoch[9/10], Step [200/3000], Loss: 0.0206\n",
      "Epoch[9/10], Step [300/3000], Loss: 0.0\n",
      "Epoch[9/10], Step [400/3000], Loss: 0.0011\n",
      "Epoch[9/10], Step [500/3000], Loss: 0.002\n",
      "Epoch[9/10], Step [600/3000], Loss: 0.0098\n",
      "Epoch[9/10], Step [700/3000], Loss: 0.0002\n",
      "Epoch[9/10], Step [800/3000], Loss: 0.0031\n",
      "Epoch[9/10], Step [900/3000], Loss: 0.0033\n",
      "Epoch[9/10], Step [1000/3000], Loss: 0.001\n",
      "Epoch[9/10], Step [1100/3000], Loss: 0.0006\n",
      "Epoch[9/10], Step [1200/3000], Loss: 0.0027\n",
      "Epoch[9/10], Step [1300/3000], Loss: 0.0\n",
      "Epoch[9/10], Step [1400/3000], Loss: 0.0147\n",
      "Epoch[9/10], Step [1500/3000], Loss: 0.0188\n",
      "Epoch[9/10], Step [1600/3000], Loss: 0.0033\n",
      "Epoch[9/10], Step [1700/3000], Loss: 0.0021\n",
      "Epoch[9/10], Step [1800/3000], Loss: 0.0041\n",
      "Epoch[9/10], Step [1900/3000], Loss: 0.0023\n",
      "Epoch[9/10], Step [2000/3000], Loss: 0.0008\n",
      "Epoch[9/10], Step [2100/3000], Loss: 0.496\n",
      "Epoch[9/10], Step [2200/3000], Loss: 0.0003\n",
      "Epoch[9/10], Step [2300/3000], Loss: 0.0025\n",
      "Epoch[9/10], Step [2400/3000], Loss: 0.0012\n",
      "Epoch[9/10], Step [2500/3000], Loss: 0.0015\n",
      "Epoch[9/10], Step [2600/3000], Loss: 0.003\n",
      "Epoch[9/10], Step [2700/3000], Loss: 0.0015\n",
      "Epoch[9/10], Step [2800/3000], Loss: 0.0007\n",
      "Epoch[9/10], Step [2900/3000], Loss: 0.0001\n",
      "Epoch[9/10], Step [3000/3000], Loss: 0.0043\n",
      "Epoch[10/10], Step [100/3000], Loss: 0.0046\n",
      "Epoch[10/10], Step [200/3000], Loss: 0.001\n",
      "Epoch[10/10], Step [300/3000], Loss: 0.0001\n",
      "Epoch[10/10], Step [400/3000], Loss: 0.0006\n",
      "Epoch[10/10], Step [500/3000], Loss: 0.0009\n",
      "Epoch[10/10], Step [600/3000], Loss: 0.0013\n",
      "Epoch[10/10], Step [700/3000], Loss: 0.0002\n",
      "Epoch[10/10], Step [800/3000], Loss: 0.0008\n",
      "Epoch[10/10], Step [900/3000], Loss: 0.0002\n",
      "Epoch[10/10], Step [1000/3000], Loss: 0.0021\n",
      "Epoch[10/10], Step [1100/3000], Loss: 0.0109\n",
      "Epoch[10/10], Step [1200/3000], Loss: 0.0007\n",
      "Epoch[10/10], Step [1300/3000], Loss: 0.0244\n",
      "Epoch[10/10], Step [1400/3000], Loss: 0.0016\n",
      "Epoch[10/10], Step [1500/3000], Loss: 0.0057\n",
      "Epoch[10/10], Step [1600/3000], Loss: 0.0016\n",
      "Epoch[10/10], Step [1700/3000], Loss: 0.0005\n",
      "Epoch[10/10], Step [1800/3000], Loss: 0.0009\n",
      "Epoch[10/10], Step [1900/3000], Loss: 0.0035\n",
      "Epoch[10/10], Step [2000/3000], Loss: 0.0002\n",
      "Epoch[10/10], Step [2100/3000], Loss: 0.0023\n",
      "Epoch[10/10], Step [2200/3000], Loss: 0.0001\n",
      "Epoch[10/10], Step [2300/3000], Loss: 0.0051\n",
      "Epoch[10/10], Step [2400/3000], Loss: 0.003\n",
      "Epoch[10/10], Step [2500/3000], Loss: 0.0002\n",
      "Epoch[10/10], Step [2600/3000], Loss: 0.0014\n",
      "Epoch[10/10], Step [2700/3000], Loss: 0.0001\n",
      "Epoch[10/10], Step [2800/3000], Loss: 0.0006\n",
      "Epoch[10/10], Step [2900/3000], Loss: 0.0254\n",
      "Epoch[10/10], Step [3000/3000], Loss: 0.0006\n",
      "Accuracy of the network no the 10,000 test images: 97.89%, with learning rate: 0.1, and [3136, 1568] hidden neurons\n",
      "\n",
      "\n",
      "\n",
      "Epoch[1/10], Step [100/3000], Loss: 0.9241\n",
      "Epoch[1/10], Step [200/3000], Loss: 0.5636\n",
      "Epoch[1/10], Step [300/3000], Loss: 0.2314\n",
      "Epoch[1/10], Step [400/3000], Loss: 0.5543\n",
      "Epoch[1/10], Step [500/3000], Loss: 0.4222\n",
      "Epoch[1/10], Step [600/3000], Loss: 0.8851\n",
      "Epoch[1/10], Step [700/3000], Loss: 0.6336\n",
      "Epoch[1/10], Step [800/3000], Loss: 0.136\n",
      "Epoch[1/10], Step [900/3000], Loss: 0.0937\n",
      "Epoch[1/10], Step [1000/3000], Loss: 0.3613\n",
      "Epoch[1/10], Step [1100/3000], Loss: 0.244\n",
      "Epoch[1/10], Step [1200/3000], Loss: 0.0737\n",
      "Epoch[1/10], Step [1300/3000], Loss: 0.1539\n",
      "Epoch[1/10], Step [1400/3000], Loss: 0.1455\n",
      "Epoch[1/10], Step [1500/3000], Loss: 0.1582\n",
      "Epoch[1/10], Step [1600/3000], Loss: 0.0534\n",
      "Epoch[1/10], Step [1700/3000], Loss: 0.1363\n",
      "Epoch[1/10], Step [1800/3000], Loss: 0.0616\n",
      "Epoch[1/10], Step [1900/3000], Loss: 0.1061\n",
      "Epoch[1/10], Step [2000/3000], Loss: 0.0511\n",
      "Epoch[1/10], Step [2100/3000], Loss: 0.1376\n",
      "Epoch[1/10], Step [2200/3000], Loss: 0.1265\n",
      "Epoch[1/10], Step [2300/3000], Loss: 0.0951\n",
      "Epoch[1/10], Step [2400/3000], Loss: 0.2756\n",
      "Epoch[1/10], Step [2500/3000], Loss: 0.0266\n",
      "Epoch[1/10], Step [2600/3000], Loss: 0.0775\n",
      "Epoch[1/10], Step [2700/3000], Loss: 0.09\n",
      "Epoch[1/10], Step [2800/3000], Loss: 0.1427\n",
      "Epoch[1/10], Step [2900/3000], Loss: 0.1655\n",
      "Epoch[1/10], Step [3000/3000], Loss: 0.2644\n",
      "Epoch[2/10], Step [100/3000], Loss: 0.2148\n",
      "Epoch[2/10], Step [200/3000], Loss: 0.0237\n",
      "Epoch[2/10], Step [300/3000], Loss: 0.2271\n",
      "Epoch[2/10], Step [400/3000], Loss: 0.023\n",
      "Epoch[2/10], Step [500/3000], Loss: 0.3541\n",
      "Epoch[2/10], Step [600/3000], Loss: 0.0468\n",
      "Epoch[2/10], Step [700/3000], Loss: 0.0984\n",
      "Epoch[2/10], Step [800/3000], Loss: 0.2307\n",
      "Epoch[2/10], Step [900/3000], Loss: 0.0705\n",
      "Epoch[2/10], Step [1000/3000], Loss: 0.4423\n",
      "Epoch[2/10], Step [1100/3000], Loss: 0.1301\n",
      "Epoch[2/10], Step [1200/3000], Loss: 0.202\n",
      "Epoch[2/10], Step [1300/3000], Loss: 0.2443\n",
      "Epoch[2/10], Step [1400/3000], Loss: 0.2578\n",
      "Epoch[2/10], Step [1500/3000], Loss: 0.0666\n",
      "Epoch[2/10], Step [1600/3000], Loss: 0.1516\n",
      "Epoch[2/10], Step [1700/3000], Loss: 0.1025\n",
      "Epoch[2/10], Step [1800/3000], Loss: 0.0462\n",
      "Epoch[2/10], Step [1900/3000], Loss: 0.2136\n",
      "Epoch[2/10], Step [2000/3000], Loss: 0.0304\n",
      "Epoch[2/10], Step [2100/3000], Loss: 0.2715\n",
      "Epoch[2/10], Step [2200/3000], Loss: 0.0328\n",
      "Epoch[2/10], Step [2300/3000], Loss: 0.2007\n",
      "Epoch[2/10], Step [2400/3000], Loss: 0.0133\n",
      "Epoch[2/10], Step [2500/3000], Loss: 0.2249\n",
      "Epoch[2/10], Step [2600/3000], Loss: 0.0424\n",
      "Epoch[2/10], Step [2700/3000], Loss: 0.0137\n",
      "Epoch[2/10], Step [2800/3000], Loss: 0.0205\n",
      "Epoch[2/10], Step [2900/3000], Loss: 0.0272\n",
      "Epoch[2/10], Step [3000/3000], Loss: 0.0374\n",
      "Epoch[3/10], Step [100/3000], Loss: 0.0618\n",
      "Epoch[3/10], Step [200/3000], Loss: 0.0117\n",
      "Epoch[3/10], Step [300/3000], Loss: 0.0599\n",
      "Epoch[3/10], Step [400/3000], Loss: 0.0632\n",
      "Epoch[3/10], Step [500/3000], Loss: 0.0125\n",
      "Epoch[3/10], Step [600/3000], Loss: 0.0582\n",
      "Epoch[3/10], Step [700/3000], Loss: 0.0122\n",
      "Epoch[3/10], Step [800/3000], Loss: 0.02\n",
      "Epoch[3/10], Step [900/3000], Loss: 0.0449\n",
      "Epoch[3/10], Step [1000/3000], Loss: 0.2186\n",
      "Epoch[3/10], Step [1100/3000], Loss: 0.1829\n",
      "Epoch[3/10], Step [1200/3000], Loss: 0.0693\n",
      "Epoch[3/10], Step [1300/3000], Loss: 0.0688\n",
      "Epoch[3/10], Step [1400/3000], Loss: 0.4183\n",
      "Epoch[3/10], Step [1500/3000], Loss: 0.1093\n",
      "Epoch[3/10], Step [1600/3000], Loss: 0.1881\n",
      "Epoch[3/10], Step [1700/3000], Loss: 0.2238\n",
      "Epoch[3/10], Step [1800/3000], Loss: 0.0193\n",
      "Epoch[3/10], Step [1900/3000], Loss: 0.0098\n",
      "Epoch[3/10], Step [2000/3000], Loss: 0.1939\n",
      "Epoch[3/10], Step [2100/3000], Loss: 0.3214\n",
      "Epoch[3/10], Step [2200/3000], Loss: 0.067\n",
      "Epoch[3/10], Step [2300/3000], Loss: 0.297\n",
      "Epoch[3/10], Step [2400/3000], Loss: 0.0959\n",
      "Epoch[3/10], Step [2500/3000], Loss: 0.0461\n",
      "Epoch[3/10], Step [2600/3000], Loss: 0.2001\n",
      "Epoch[3/10], Step [2700/3000], Loss: 0.11\n",
      "Epoch[3/10], Step [2800/3000], Loss: 0.0387\n",
      "Epoch[3/10], Step [2900/3000], Loss: 0.0193\n",
      "Epoch[3/10], Step [3000/3000], Loss: 0.057\n",
      "Epoch[4/10], Step [100/3000], Loss: 0.2644\n",
      "Epoch[4/10], Step [200/3000], Loss: 0.0166\n",
      "Epoch[4/10], Step [300/3000], Loss: 0.2464\n",
      "Epoch[4/10], Step [400/3000], Loss: 0.0898\n",
      "Epoch[4/10], Step [500/3000], Loss: 0.0342\n",
      "Epoch[4/10], Step [600/3000], Loss: 0.2053\n",
      "Epoch[4/10], Step [700/3000], Loss: 0.0678\n",
      "Epoch[4/10], Step [800/3000], Loss: 0.0246\n",
      "Epoch[4/10], Step [900/3000], Loss: 0.1259\n",
      "Epoch[4/10], Step [1000/3000], Loss: 0.1378\n",
      "Epoch[4/10], Step [1100/3000], Loss: 0.0045\n",
      "Epoch[4/10], Step [1200/3000], Loss: 0.081\n",
      "Epoch[4/10], Step [1300/3000], Loss: 0.4133\n",
      "Epoch[4/10], Step [1400/3000], Loss: 0.2358\n",
      "Epoch[4/10], Step [1500/3000], Loss: 0.0491\n",
      "Epoch[4/10], Step [1600/3000], Loss: 0.1028\n",
      "Epoch[4/10], Step [1700/3000], Loss: 0.0134\n",
      "Epoch[4/10], Step [1800/3000], Loss: 0.0402\n",
      "Epoch[4/10], Step [1900/3000], Loss: 0.0281\n",
      "Epoch[4/10], Step [2000/3000], Loss: 0.0167\n",
      "Epoch[4/10], Step [2100/3000], Loss: 0.0168\n",
      "Epoch[4/10], Step [2200/3000], Loss: 0.0882\n",
      "Epoch[4/10], Step [2300/3000], Loss: 0.205\n",
      "Epoch[4/10], Step [2400/3000], Loss: 0.0077\n",
      "Epoch[4/10], Step [2500/3000], Loss: 0.0937\n",
      "Epoch[4/10], Step [2600/3000], Loss: 0.0035\n",
      "Epoch[4/10], Step [2700/3000], Loss: 0.0041\n",
      "Epoch[4/10], Step [2800/3000], Loss: 0.0554\n",
      "Epoch[4/10], Step [2900/3000], Loss: 0.0074\n",
      "Epoch[4/10], Step [3000/3000], Loss: 0.0194\n",
      "Epoch[5/10], Step [100/3000], Loss: 0.3046\n",
      "Epoch[5/10], Step [200/3000], Loss: 0.0394\n",
      "Epoch[5/10], Step [300/3000], Loss: 0.0198\n",
      "Epoch[5/10], Step [400/3000], Loss: 0.0044\n",
      "Epoch[5/10], Step [500/3000], Loss: 0.3056\n",
      "Epoch[5/10], Step [600/3000], Loss: 0.0772\n",
      "Epoch[5/10], Step [700/3000], Loss: 0.0119\n",
      "Epoch[5/10], Step [800/3000], Loss: 0.0449\n",
      "Epoch[5/10], Step [900/3000], Loss: 0.0452\n",
      "Epoch[5/10], Step [1000/3000], Loss: 0.017\n",
      "Epoch[5/10], Step [1100/3000], Loss: 0.0769\n",
      "Epoch[5/10], Step [1200/3000], Loss: 0.0146\n",
      "Epoch[5/10], Step [1300/3000], Loss: 0.0125\n",
      "Epoch[5/10], Step [1400/3000], Loss: 0.0161\n",
      "Epoch[5/10], Step [1500/3000], Loss: 0.0077\n",
      "Epoch[5/10], Step [1600/3000], Loss: 0.0749\n",
      "Epoch[5/10], Step [1700/3000], Loss: 0.0122\n",
      "Epoch[5/10], Step [1800/3000], Loss: 0.1038\n",
      "Epoch[5/10], Step [1900/3000], Loss: 0.0266\n",
      "Epoch[5/10], Step [2000/3000], Loss: 0.0156\n",
      "Epoch[5/10], Step [2100/3000], Loss: 0.0152\n",
      "Epoch[5/10], Step [2200/3000], Loss: 0.007\n",
      "Epoch[5/10], Step [2300/3000], Loss: 0.2481\n",
      "Epoch[5/10], Step [2400/3000], Loss: 0.0076\n",
      "Epoch[5/10], Step [2500/3000], Loss: 0.0029\n",
      "Epoch[5/10], Step [2600/3000], Loss: 0.0365\n",
      "Epoch[5/10], Step [2700/3000], Loss: 0.0521\n",
      "Epoch[5/10], Step [2800/3000], Loss: 0.1539\n",
      "Epoch[5/10], Step [2900/3000], Loss: 0.0226\n",
      "Epoch[5/10], Step [3000/3000], Loss: 0.1052\n",
      "Epoch[6/10], Step [100/3000], Loss: 0.0197\n",
      "Epoch[6/10], Step [200/3000], Loss: 0.0144\n",
      "Epoch[6/10], Step [300/3000], Loss: 0.0553\n",
      "Epoch[6/10], Step [400/3000], Loss: 0.0134\n",
      "Epoch[6/10], Step [500/3000], Loss: 0.0224\n",
      "Epoch[6/10], Step [600/3000], Loss: 0.0535\n",
      "Epoch[6/10], Step [700/3000], Loss: 0.0422\n",
      "Epoch[6/10], Step [800/3000], Loss: 0.3969\n",
      "Epoch[6/10], Step [900/3000], Loss: 0.0155\n",
      "Epoch[6/10], Step [1000/3000], Loss: 0.0034\n",
      "Epoch[6/10], Step [1100/3000], Loss: 0.0116\n",
      "Epoch[6/10], Step [1200/3000], Loss: 0.0543\n",
      "Epoch[6/10], Step [1300/3000], Loss: 0.026\n",
      "Epoch[6/10], Step [1400/3000], Loss: 0.0143\n",
      "Epoch[6/10], Step [1500/3000], Loss: 0.0157\n",
      "Epoch[6/10], Step [1600/3000], Loss: 0.0358\n",
      "Epoch[6/10], Step [1700/3000], Loss: 0.1089\n",
      "Epoch[6/10], Step [1800/3000], Loss: 0.012\n",
      "Epoch[6/10], Step [1900/3000], Loss: 0.0248\n",
      "Epoch[6/10], Step [2000/3000], Loss: 0.0013\n",
      "Epoch[6/10], Step [2100/3000], Loss: 0.2467\n",
      "Epoch[6/10], Step [2200/3000], Loss: 0.0055\n",
      "Epoch[6/10], Step [2300/3000], Loss: 0.0728\n",
      "Epoch[6/10], Step [2400/3000], Loss: 0.017\n",
      "Epoch[6/10], Step [2500/3000], Loss: 0.272\n",
      "Epoch[6/10], Step [2600/3000], Loss: 0.0316\n",
      "Epoch[6/10], Step [2700/3000], Loss: 0.0083\n",
      "Epoch[6/10], Step [2800/3000], Loss: 0.0021\n",
      "Epoch[6/10], Step [2900/3000], Loss: 0.021\n",
      "Epoch[6/10], Step [3000/3000], Loss: 0.0346\n",
      "Epoch[7/10], Step [100/3000], Loss: 0.0273\n",
      "Epoch[7/10], Step [200/3000], Loss: 0.0022\n",
      "Epoch[7/10], Step [300/3000], Loss: 0.0274\n",
      "Epoch[7/10], Step [400/3000], Loss: 0.0666\n",
      "Epoch[7/10], Step [500/3000], Loss: 0.0442\n",
      "Epoch[7/10], Step [600/3000], Loss: 0.0751\n",
      "Epoch[7/10], Step [700/3000], Loss: 0.0025\n",
      "Epoch[7/10], Step [800/3000], Loss: 0.0051\n",
      "Epoch[7/10], Step [900/3000], Loss: 0.0028\n",
      "Epoch[7/10], Step [1000/3000], Loss: 0.0172\n",
      "Epoch[7/10], Step [1100/3000], Loss: 0.0249\n",
      "Epoch[7/10], Step [1200/3000], Loss: 0.0127\n",
      "Epoch[7/10], Step [1300/3000], Loss: 0.0158\n",
      "Epoch[7/10], Step [1400/3000], Loss: 0.0093\n",
      "Epoch[7/10], Step [1500/3000], Loss: 0.0066\n",
      "Epoch[7/10], Step [1600/3000], Loss: 0.3162\n",
      "Epoch[7/10], Step [1700/3000], Loss: 0.0027\n",
      "Epoch[7/10], Step [1800/3000], Loss: 0.0086\n",
      "Epoch[7/10], Step [1900/3000], Loss: 0.0504\n",
      "Epoch[7/10], Step [2000/3000], Loss: 0.0675\n",
      "Epoch[7/10], Step [2100/3000], Loss: 0.0475\n",
      "Epoch[7/10], Step [2200/3000], Loss: 0.3061\n",
      "Epoch[7/10], Step [2300/3000], Loss: 0.0268\n",
      "Epoch[7/10], Step [2400/3000], Loss: 0.1122\n",
      "Epoch[7/10], Step [2500/3000], Loss: 0.0106\n",
      "Epoch[7/10], Step [2600/3000], Loss: 0.0168\n",
      "Epoch[7/10], Step [2700/3000], Loss: 0.01\n",
      "Epoch[7/10], Step [2800/3000], Loss: 0.0813\n",
      "Epoch[7/10], Step [2900/3000], Loss: 0.0047\n",
      "Epoch[7/10], Step [3000/3000], Loss: 0.0085\n",
      "Epoch[8/10], Step [100/3000], Loss: 0.0043\n",
      "Epoch[8/10], Step [200/3000], Loss: 0.0593\n",
      "Epoch[8/10], Step [300/3000], Loss: 0.0077\n",
      "Epoch[8/10], Step [400/3000], Loss: 0.0246\n",
      "Epoch[8/10], Step [500/3000], Loss: 0.0096\n",
      "Epoch[8/10], Step [600/3000], Loss: 0.0233\n",
      "Epoch[8/10], Step [700/3000], Loss: 0.067\n",
      "Epoch[8/10], Step [800/3000], Loss: 0.0083\n",
      "Epoch[8/10], Step [900/3000], Loss: 0.0249\n",
      "Epoch[8/10], Step [1000/3000], Loss: 0.0056\n",
      "Epoch[8/10], Step [1100/3000], Loss: 0.0029\n",
      "Epoch[8/10], Step [1200/3000], Loss: 0.0094\n",
      "Epoch[8/10], Step [1300/3000], Loss: 0.008\n",
      "Epoch[8/10], Step [1400/3000], Loss: 0.0104\n",
      "Epoch[8/10], Step [1500/3000], Loss: 0.0647\n",
      "Epoch[8/10], Step [1600/3000], Loss: 0.1933\n",
      "Epoch[8/10], Step [1700/3000], Loss: 0.0178\n",
      "Epoch[8/10], Step [1800/3000], Loss: 0.0344\n",
      "Epoch[8/10], Step [1900/3000], Loss: 0.0442\n",
      "Epoch[8/10], Step [2000/3000], Loss: 0.0186\n",
      "Epoch[8/10], Step [2100/3000], Loss: 0.0818\n",
      "Epoch[8/10], Step [2200/3000], Loss: 0.016\n",
      "Epoch[8/10], Step [2300/3000], Loss: 0.0577\n",
      "Epoch[8/10], Step [2400/3000], Loss: 0.0109\n",
      "Epoch[8/10], Step [2500/3000], Loss: 0.0131\n",
      "Epoch[8/10], Step [2600/3000], Loss: 0.0011\n",
      "Epoch[8/10], Step [2700/3000], Loss: 0.0026\n",
      "Epoch[8/10], Step [2800/3000], Loss: 0.0138\n",
      "Epoch[8/10], Step [2900/3000], Loss: 0.0042\n",
      "Epoch[8/10], Step [3000/3000], Loss: 0.0138\n",
      "Epoch[9/10], Step [100/3000], Loss: 0.115\n",
      "Epoch[9/10], Step [200/3000], Loss: 0.0015\n",
      "Epoch[9/10], Step [300/3000], Loss: 0.0086\n",
      "Epoch[9/10], Step [400/3000], Loss: 0.0159\n",
      "Epoch[9/10], Step [500/3000], Loss: 0.0572\n",
      "Epoch[9/10], Step [600/3000], Loss: 0.0513\n",
      "Epoch[9/10], Step [700/3000], Loss: 0.0085\n",
      "Epoch[9/10], Step [800/3000], Loss: 0.001\n",
      "Epoch[9/10], Step [900/3000], Loss: 0.004\n",
      "Epoch[9/10], Step [1000/3000], Loss: 0.013\n",
      "Epoch[9/10], Step [1100/3000], Loss: 0.0108\n",
      "Epoch[9/10], Step [1200/3000], Loss: 0.0046\n",
      "Epoch[9/10], Step [1300/3000], Loss: 0.0021\n",
      "Epoch[9/10], Step [1400/3000], Loss: 0.0034\n",
      "Epoch[9/10], Step [1500/3000], Loss: 0.0297\n",
      "Epoch[9/10], Step [1600/3000], Loss: 0.1177\n",
      "Epoch[9/10], Step [1700/3000], Loss: 0.0176\n",
      "Epoch[9/10], Step [1800/3000], Loss: 0.0206\n",
      "Epoch[9/10], Step [1900/3000], Loss: 0.0032\n",
      "Epoch[9/10], Step [2000/3000], Loss: 0.0006\n",
      "Epoch[9/10], Step [2100/3000], Loss: 0.0023\n",
      "Epoch[9/10], Step [2200/3000], Loss: 0.0085\n",
      "Epoch[9/10], Step [2300/3000], Loss: 0.0058\n",
      "Epoch[9/10], Step [2400/3000], Loss: 0.0055\n",
      "Epoch[9/10], Step [2500/3000], Loss: 0.016\n",
      "Epoch[9/10], Step [2600/3000], Loss: 0.0008\n",
      "Epoch[9/10], Step [2700/3000], Loss: 0.0612\n",
      "Epoch[9/10], Step [2800/3000], Loss: 0.0134\n",
      "Epoch[9/10], Step [2900/3000], Loss: 0.0252\n",
      "Epoch[9/10], Step [3000/3000], Loss: 0.0036\n",
      "Epoch[10/10], Step [100/3000], Loss: 0.0009\n",
      "Epoch[10/10], Step [200/3000], Loss: 0.1198\n",
      "Epoch[10/10], Step [300/3000], Loss: 0.0522\n",
      "Epoch[10/10], Step [400/3000], Loss: 0.0037\n",
      "Epoch[10/10], Step [500/3000], Loss: 0.0399\n",
      "Epoch[10/10], Step [600/3000], Loss: 0.0021\n",
      "Epoch[10/10], Step [700/3000], Loss: 0.0134\n",
      "Epoch[10/10], Step [800/3000], Loss: 0.0022\n",
      "Epoch[10/10], Step [900/3000], Loss: 0.0159\n",
      "Epoch[10/10], Step [1000/3000], Loss: 0.001\n",
      "Epoch[10/10], Step [1100/3000], Loss: 0.0226\n",
      "Epoch[10/10], Step [1200/3000], Loss: 0.0018\n",
      "Epoch[10/10], Step [1300/3000], Loss: 0.0288\n",
      "Epoch[10/10], Step [1400/3000], Loss: 0.005\n",
      "Epoch[10/10], Step [1500/3000], Loss: 0.0223\n",
      "Epoch[10/10], Step [1600/3000], Loss: 0.0211\n",
      "Epoch[10/10], Step [1700/3000], Loss: 0.0115\n",
      "Epoch[10/10], Step [1800/3000], Loss: 0.0043\n",
      "Epoch[10/10], Step [1900/3000], Loss: 0.0269\n",
      "Epoch[10/10], Step [2000/3000], Loss: 0.0019\n",
      "Epoch[10/10], Step [2100/3000], Loss: 0.0382\n",
      "Epoch[10/10], Step [2200/3000], Loss: 0.1034\n",
      "Epoch[10/10], Step [2300/3000], Loss: 0.0076\n",
      "Epoch[10/10], Step [2400/3000], Loss: 0.0097\n",
      "Epoch[10/10], Step [2500/3000], Loss: 0.0251\n",
      "Epoch[10/10], Step [2600/3000], Loss: 0.0021\n",
      "Epoch[10/10], Step [2700/3000], Loss: 0.0016\n",
      "Epoch[10/10], Step [2800/3000], Loss: 0.0866\n",
      "Epoch[10/10], Step [2900/3000], Loss: 0.0612\n",
      "Epoch[10/10], Step [3000/3000], Loss: 0.0002\n",
      "Accuracy of the network no the 10,000 test images: 97.86%, with learning rate: 0.05, and [1568, 784] hidden neurons\n",
      "\n",
      "\n",
      "\n",
      "Epoch[1/10], Step [100/3000], Loss: 0.577\n",
      "Epoch[1/10], Step [200/3000], Loss: 0.9197\n",
      "Epoch[1/10], Step [300/3000], Loss: 0.1744\n",
      "Epoch[1/10], Step [400/3000], Loss: 0.3065\n",
      "Epoch[1/10], Step [500/3000], Loss: 0.2993\n",
      "Epoch[1/10], Step [600/3000], Loss: 0.6765\n",
      "Epoch[1/10], Step [700/3000], Loss: 0.8401\n",
      "Epoch[1/10], Step [800/3000], Loss: 0.1511\n",
      "Epoch[1/10], Step [900/3000], Loss: 0.1824\n",
      "Epoch[1/10], Step [1000/3000], Loss: 0.3623\n",
      "Epoch[1/10], Step [1100/3000], Loss: 0.2277\n",
      "Epoch[1/10], Step [1200/3000], Loss: 0.4095\n",
      "Epoch[1/10], Step [1300/3000], Loss: 0.1095\n",
      "Epoch[1/10], Step [1400/3000], Loss: 0.0498\n",
      "Epoch[1/10], Step [1500/3000], Loss: 0.976\n",
      "Epoch[1/10], Step [1600/3000], Loss: 0.1322\n",
      "Epoch[1/10], Step [1700/3000], Loss: 0.0802\n",
      "Epoch[1/10], Step [1800/3000], Loss: 0.3245\n",
      "Epoch[1/10], Step [1900/3000], Loss: 0.2236\n",
      "Epoch[1/10], Step [2000/3000], Loss: 0.0279\n",
      "Epoch[1/10], Step [2100/3000], Loss: 0.1285\n",
      "Epoch[1/10], Step [2200/3000], Loss: 0.0637\n",
      "Epoch[1/10], Step [2300/3000], Loss: 0.183\n",
      "Epoch[1/10], Step [2400/3000], Loss: 0.0622\n",
      "Epoch[1/10], Step [2500/3000], Loss: 0.2382\n",
      "Epoch[1/10], Step [2600/3000], Loss: 0.2572\n",
      "Epoch[1/10], Step [2700/3000], Loss: 0.0776\n",
      "Epoch[1/10], Step [2800/3000], Loss: 0.0973\n",
      "Epoch[1/10], Step [2900/3000], Loss: 0.0956\n",
      "Epoch[1/10], Step [3000/3000], Loss: 0.2307\n",
      "Epoch[2/10], Step [100/3000], Loss: 0.3014\n",
      "Epoch[2/10], Step [200/3000], Loss: 0.152\n",
      "Epoch[2/10], Step [300/3000], Loss: 0.3679\n",
      "Epoch[2/10], Step [400/3000], Loss: 0.3078\n",
      "Epoch[2/10], Step [500/3000], Loss: 0.1119\n",
      "Epoch[2/10], Step [600/3000], Loss: 0.0787\n",
      "Epoch[2/10], Step [700/3000], Loss: 0.0964\n",
      "Epoch[2/10], Step [800/3000], Loss: 0.2492\n",
      "Epoch[2/10], Step [900/3000], Loss: 0.0431\n",
      "Epoch[2/10], Step [1000/3000], Loss: 0.0456\n",
      "Epoch[2/10], Step [1100/3000], Loss: 0.0489\n",
      "Epoch[2/10], Step [1200/3000], Loss: 0.1238\n",
      "Epoch[2/10], Step [1300/3000], Loss: 0.1228\n",
      "Epoch[2/10], Step [1400/3000], Loss: 0.0162\n",
      "Epoch[2/10], Step [1500/3000], Loss: 0.0266\n",
      "Epoch[2/10], Step [1600/3000], Loss: 0.0434\n",
      "Epoch[2/10], Step [1700/3000], Loss: 0.0167\n",
      "Epoch[2/10], Step [1800/3000], Loss: 0.0707\n",
      "Epoch[2/10], Step [1900/3000], Loss: 0.0287\n",
      "Epoch[2/10], Step [2000/3000], Loss: 0.1282\n",
      "Epoch[2/10], Step [2100/3000], Loss: 0.0474\n",
      "Epoch[2/10], Step [2200/3000], Loss: 0.0301\n",
      "Epoch[2/10], Step [2300/3000], Loss: 0.0967\n",
      "Epoch[2/10], Step [2400/3000], Loss: 0.1845\n",
      "Epoch[2/10], Step [2500/3000], Loss: 0.0289\n",
      "Epoch[2/10], Step [2600/3000], Loss: 0.0356\n",
      "Epoch[2/10], Step [2700/3000], Loss: 0.0069\n",
      "Epoch[2/10], Step [2800/3000], Loss: 0.0039\n",
      "Epoch[2/10], Step [2900/3000], Loss: 0.0042\n",
      "Epoch[2/10], Step [3000/3000], Loss: 0.5088\n",
      "Epoch[3/10], Step [100/3000], Loss: 0.1041\n",
      "Epoch[3/10], Step [200/3000], Loss: 0.1597\n",
      "Epoch[3/10], Step [300/3000], Loss: 0.0053\n",
      "Epoch[3/10], Step [400/3000], Loss: 0.117\n",
      "Epoch[3/10], Step [500/3000], Loss: 0.0186\n",
      "Epoch[3/10], Step [600/3000], Loss: 0.0589\n",
      "Epoch[3/10], Step [700/3000], Loss: 0.0072\n",
      "Epoch[3/10], Step [800/3000], Loss: 0.0074\n",
      "Epoch[3/10], Step [900/3000], Loss: 0.0073\n",
      "Epoch[3/10], Step [1000/3000], Loss: 0.277\n",
      "Epoch[3/10], Step [1100/3000], Loss: 0.1361\n",
      "Epoch[3/10], Step [1200/3000], Loss: 0.1423\n",
      "Epoch[3/10], Step [1300/3000], Loss: 0.1145\n",
      "Epoch[3/10], Step [1400/3000], Loss: 0.0199\n",
      "Epoch[3/10], Step [1500/3000], Loss: 0.0122\n",
      "Epoch[3/10], Step [1600/3000], Loss: 0.0277\n",
      "Epoch[3/10], Step [1700/3000], Loss: 0.022\n",
      "Epoch[3/10], Step [1800/3000], Loss: 0.0597\n",
      "Epoch[3/10], Step [1900/3000], Loss: 0.0173\n",
      "Epoch[3/10], Step [2000/3000], Loss: 0.0173\n",
      "Epoch[3/10], Step [2100/3000], Loss: 0.0591\n",
      "Epoch[3/10], Step [2200/3000], Loss: 0.1947\n",
      "Epoch[3/10], Step [2300/3000], Loss: 0.0164\n",
      "Epoch[3/10], Step [2400/3000], Loss: 0.0503\n",
      "Epoch[3/10], Step [2500/3000], Loss: 0.062\n",
      "Epoch[3/10], Step [2600/3000], Loss: 0.0157\n",
      "Epoch[3/10], Step [2700/3000], Loss: 0.0045\n",
      "Epoch[3/10], Step [2800/3000], Loss: 0.1725\n",
      "Epoch[3/10], Step [2900/3000], Loss: 0.0387\n",
      "Epoch[3/10], Step [3000/3000], Loss: 0.0039\n",
      "Epoch[4/10], Step [100/3000], Loss: 0.0999\n",
      "Epoch[4/10], Step [200/3000], Loss: 0.1371\n",
      "Epoch[4/10], Step [300/3000], Loss: 0.0507\n",
      "Epoch[4/10], Step [400/3000], Loss: 0.1109\n",
      "Epoch[4/10], Step [500/3000], Loss: 0.0194\n",
      "Epoch[4/10], Step [600/3000], Loss: 0.0066\n",
      "Epoch[4/10], Step [700/3000], Loss: 0.0622\n",
      "Epoch[4/10], Step [800/3000], Loss: 0.0165\n",
      "Epoch[4/10], Step [900/3000], Loss: 0.0098\n",
      "Epoch[4/10], Step [1000/3000], Loss: 0.0828\n",
      "Epoch[4/10], Step [1100/3000], Loss: 0.0748\n",
      "Epoch[4/10], Step [1200/3000], Loss: 0.0439\n",
      "Epoch[4/10], Step [1300/3000], Loss: 0.0306\n",
      "Epoch[4/10], Step [1400/3000], Loss: 0.0105\n",
      "Epoch[4/10], Step [1500/3000], Loss: 0.0273\n",
      "Epoch[4/10], Step [1600/3000], Loss: 0.1016\n",
      "Epoch[4/10], Step [1700/3000], Loss: 0.018\n",
      "Epoch[4/10], Step [1800/3000], Loss: 0.3928\n",
      "Epoch[4/10], Step [1900/3000], Loss: 0.0089\n",
      "Epoch[4/10], Step [2000/3000], Loss: 0.1963\n",
      "Epoch[4/10], Step [2100/3000], Loss: 0.0198\n",
      "Epoch[4/10], Step [2200/3000], Loss: 0.0334\n",
      "Epoch[4/10], Step [2300/3000], Loss: 0.2055\n",
      "Epoch[4/10], Step [2400/3000], Loss: 0.0065\n",
      "Epoch[4/10], Step [2500/3000], Loss: 0.0351\n",
      "Epoch[4/10], Step [2600/3000], Loss: 0.0308\n",
      "Epoch[4/10], Step [2700/3000], Loss: 0.0143\n",
      "Epoch[4/10], Step [2800/3000], Loss: 0.1065\n",
      "Epoch[4/10], Step [2900/3000], Loss: 0.006\n",
      "Epoch[4/10], Step [3000/3000], Loss: 0.0284\n",
      "Epoch[5/10], Step [100/3000], Loss: 0.0084\n",
      "Epoch[5/10], Step [200/3000], Loss: 0.0058\n",
      "Epoch[5/10], Step [300/3000], Loss: 0.0306\n",
      "Epoch[5/10], Step [400/3000], Loss: 0.1164\n",
      "Epoch[5/10], Step [500/3000], Loss: 0.0486\n",
      "Epoch[5/10], Step [600/3000], Loss: 0.0052\n",
      "Epoch[5/10], Step [700/3000], Loss: 0.002\n",
      "Epoch[5/10], Step [800/3000], Loss: 0.0209\n",
      "Epoch[5/10], Step [900/3000], Loss: 0.0009\n",
      "Epoch[5/10], Step [1000/3000], Loss: 0.0063\n",
      "Epoch[5/10], Step [1100/3000], Loss: 0.002\n",
      "Epoch[5/10], Step [1200/3000], Loss: 0.1761\n",
      "Epoch[5/10], Step [1300/3000], Loss: 0.1897\n",
      "Epoch[5/10], Step [1400/3000], Loss: 0.0051\n",
      "Epoch[5/10], Step [1500/3000], Loss: 0.0035\n",
      "Epoch[5/10], Step [1600/3000], Loss: 0.0825\n",
      "Epoch[5/10], Step [1700/3000], Loss: 0.005\n",
      "Epoch[5/10], Step [1800/3000], Loss: 0.1861\n",
      "Epoch[5/10], Step [1900/3000], Loss: 0.095\n",
      "Epoch[5/10], Step [2000/3000], Loss: 0.0433\n",
      "Epoch[5/10], Step [2100/3000], Loss: 0.0602\n",
      "Epoch[5/10], Step [2200/3000], Loss: 0.3913\n",
      "Epoch[5/10], Step [2300/3000], Loss: 0.0692\n",
      "Epoch[5/10], Step [2400/3000], Loss: 0.0586\n",
      "Epoch[5/10], Step [2500/3000], Loss: 0.0708\n",
      "Epoch[5/10], Step [2600/3000], Loss: 0.0164\n",
      "Epoch[5/10], Step [2700/3000], Loss: 0.0021\n",
      "Epoch[5/10], Step [2800/3000], Loss: 0.0021\n",
      "Epoch[5/10], Step [2900/3000], Loss: 0.0003\n",
      "Epoch[5/10], Step [3000/3000], Loss: 0.0113\n",
      "Epoch[6/10], Step [100/3000], Loss: 0.0015\n",
      "Epoch[6/10], Step [200/3000], Loss: 0.0007\n",
      "Epoch[6/10], Step [300/3000], Loss: 0.0044\n",
      "Epoch[6/10], Step [400/3000], Loss: 0.1033\n",
      "Epoch[6/10], Step [500/3000], Loss: 0.0049\n",
      "Epoch[6/10], Step [600/3000], Loss: 0.1525\n",
      "Epoch[6/10], Step [700/3000], Loss: 0.0197\n",
      "Epoch[6/10], Step [800/3000], Loss: 0.0019\n",
      "Epoch[6/10], Step [900/3000], Loss: 0.0032\n",
      "Epoch[6/10], Step [1000/3000], Loss: 0.0333\n",
      "Epoch[6/10], Step [1100/3000], Loss: 0.2852\n",
      "Epoch[6/10], Step [1200/3000], Loss: 0.0004\n",
      "Epoch[6/10], Step [1300/3000], Loss: 0.0092\n",
      "Epoch[6/10], Step [1400/3000], Loss: 0.01\n",
      "Epoch[6/10], Step [1500/3000], Loss: 0.024\n",
      "Epoch[6/10], Step [1600/3000], Loss: 0.3044\n",
      "Epoch[6/10], Step [1700/3000], Loss: 0.0051\n",
      "Epoch[6/10], Step [1800/3000], Loss: 0.0129\n",
      "Epoch[6/10], Step [1900/3000], Loss: 0.0031\n",
      "Epoch[6/10], Step [2000/3000], Loss: 0.2773\n",
      "Epoch[6/10], Step [2100/3000], Loss: 0.0201\n",
      "Epoch[6/10], Step [2200/3000], Loss: 0.1235\n",
      "Epoch[6/10], Step [2300/3000], Loss: 0.0088\n",
      "Epoch[6/10], Step [2400/3000], Loss: 0.0267\n",
      "Epoch[6/10], Step [2500/3000], Loss: 0.0082\n",
      "Epoch[6/10], Step [2600/3000], Loss: 0.0495\n",
      "Epoch[6/10], Step [2700/3000], Loss: 0.0026\n",
      "Epoch[6/10], Step [2800/3000], Loss: 0.0041\n",
      "Epoch[6/10], Step [2900/3000], Loss: 0.0091\n",
      "Epoch[6/10], Step [3000/3000], Loss: 0.0269\n",
      "Epoch[7/10], Step [100/3000], Loss: 0.0135\n",
      "Epoch[7/10], Step [200/3000], Loss: 0.0189\n",
      "Epoch[7/10], Step [300/3000], Loss: 0.0177\n",
      "Epoch[7/10], Step [400/3000], Loss: 0.0066\n",
      "Epoch[7/10], Step [500/3000], Loss: 0.0016\n",
      "Epoch[7/10], Step [600/3000], Loss: 0.0062\n",
      "Epoch[7/10], Step [700/3000], Loss: 0.0104\n",
      "Epoch[7/10], Step [800/3000], Loss: 0.0212\n",
      "Epoch[7/10], Step [900/3000], Loss: 0.0036\n",
      "Epoch[7/10], Step [1000/3000], Loss: 0.0304\n",
      "Epoch[7/10], Step [1100/3000], Loss: 0.0183\n",
      "Epoch[7/10], Step [1200/3000], Loss: 0.0005\n",
      "Epoch[7/10], Step [1300/3000], Loss: 0.0031\n",
      "Epoch[7/10], Step [1400/3000], Loss: 0.0062\n",
      "Epoch[7/10], Step [1500/3000], Loss: 0.005\n",
      "Epoch[7/10], Step [1600/3000], Loss: 0.0052\n",
      "Epoch[7/10], Step [1700/3000], Loss: 0.007\n",
      "Epoch[7/10], Step [1800/3000], Loss: 0.0078\n",
      "Epoch[7/10], Step [1900/3000], Loss: 0.0058\n",
      "Epoch[7/10], Step [2000/3000], Loss: 0.0009\n",
      "Epoch[7/10], Step [2100/3000], Loss: 0.0043\n",
      "Epoch[7/10], Step [2200/3000], Loss: 0.0044\n",
      "Epoch[7/10], Step [2300/3000], Loss: 0.2665\n",
      "Epoch[7/10], Step [2400/3000], Loss: 0.002\n",
      "Epoch[7/10], Step [2500/3000], Loss: 0.1273\n",
      "Epoch[7/10], Step [2600/3000], Loss: 0.0879\n",
      "Epoch[7/10], Step [2700/3000], Loss: 0.0004\n",
      "Epoch[7/10], Step [2800/3000], Loss: 0.0013\n",
      "Epoch[7/10], Step [2900/3000], Loss: 0.2948\n",
      "Epoch[7/10], Step [3000/3000], Loss: 0.0435\n",
      "Epoch[8/10], Step [100/3000], Loss: 0.0347\n",
      "Epoch[8/10], Step [200/3000], Loss: 0.0003\n",
      "Epoch[8/10], Step [300/3000], Loss: 0.0043\n",
      "Epoch[8/10], Step [400/3000], Loss: 0.0005\n",
      "Epoch[8/10], Step [500/3000], Loss: 0.0082\n",
      "Epoch[8/10], Step [600/3000], Loss: 0.0235\n",
      "Epoch[8/10], Step [700/3000], Loss: 0.004\n",
      "Epoch[8/10], Step [800/3000], Loss: 0.0087\n",
      "Epoch[8/10], Step [900/3000], Loss: 0.0005\n",
      "Epoch[8/10], Step [1000/3000], Loss: 0.0014\n",
      "Epoch[8/10], Step [1100/3000], Loss: 0.0107\n",
      "Epoch[8/10], Step [1200/3000], Loss: 0.0725\n",
      "Epoch[8/10], Step [1300/3000], Loss: 0.0354\n",
      "Epoch[8/10], Step [1400/3000], Loss: 0.0001\n",
      "Epoch[8/10], Step [1500/3000], Loss: 0.0011\n",
      "Epoch[8/10], Step [1600/3000], Loss: 0.0007\n",
      "Epoch[8/10], Step [1700/3000], Loss: 0.0006\n",
      "Epoch[8/10], Step [1800/3000], Loss: 0.0048\n",
      "Epoch[8/10], Step [1900/3000], Loss: 0.1857\n",
      "Epoch[8/10], Step [2000/3000], Loss: 0.049\n",
      "Epoch[8/10], Step [2100/3000], Loss: 0.0004\n",
      "Epoch[8/10], Step [2200/3000], Loss: 0.0704\n",
      "Epoch[8/10], Step [2300/3000], Loss: 0.0236\n",
      "Epoch[8/10], Step [2400/3000], Loss: 0.0001\n",
      "Epoch[8/10], Step [2500/3000], Loss: 0.0215\n",
      "Epoch[8/10], Step [2600/3000], Loss: 0.0017\n",
      "Epoch[8/10], Step [2700/3000], Loss: 0.0005\n",
      "Epoch[8/10], Step [2800/3000], Loss: 0.0001\n",
      "Epoch[8/10], Step [2900/3000], Loss: 0.0074\n",
      "Epoch[8/10], Step [3000/3000], Loss: 0.1075\n",
      "Epoch[9/10], Step [100/3000], Loss: 0.002\n",
      "Epoch[9/10], Step [200/3000], Loss: 0.0269\n",
      "Epoch[9/10], Step [300/3000], Loss: 0.0244\n",
      "Epoch[9/10], Step [400/3000], Loss: 0.0029\n",
      "Epoch[9/10], Step [500/3000], Loss: 0.0019\n",
      "Epoch[9/10], Step [600/3000], Loss: 0.0105\n",
      "Epoch[9/10], Step [700/3000], Loss: 0.0054\n",
      "Epoch[9/10], Step [800/3000], Loss: 0.002\n",
      "Epoch[9/10], Step [900/3000], Loss: 0.0396\n",
      "Epoch[9/10], Step [1000/3000], Loss: 0.1514\n",
      "Epoch[9/10], Step [1100/3000], Loss: 0.0014\n",
      "Epoch[9/10], Step [1200/3000], Loss: 0.074\n",
      "Epoch[9/10], Step [1300/3000], Loss: 0.0009\n",
      "Epoch[9/10], Step [1400/3000], Loss: 0.0591\n",
      "Epoch[9/10], Step [1500/3000], Loss: 0.0065\n",
      "Epoch[9/10], Step [1600/3000], Loss: 0.0012\n",
      "Epoch[9/10], Step [1700/3000], Loss: 0.0004\n",
      "Epoch[9/10], Step [1800/3000], Loss: 0.1956\n",
      "Epoch[9/10], Step [1900/3000], Loss: 0.0002\n",
      "Epoch[9/10], Step [2000/3000], Loss: 0.0028\n",
      "Epoch[9/10], Step [2100/3000], Loss: 0.0033\n",
      "Epoch[9/10], Step [2200/3000], Loss: 0.0019\n",
      "Epoch[9/10], Step [2300/3000], Loss: 0.0051\n",
      "Epoch[9/10], Step [2400/3000], Loss: 0.0038\n",
      "Epoch[9/10], Step [2500/3000], Loss: 0.0041\n",
      "Epoch[9/10], Step [2600/3000], Loss: 0.0015\n",
      "Epoch[9/10], Step [2700/3000], Loss: 0.1296\n",
      "Epoch[9/10], Step [2800/3000], Loss: 0.0002\n",
      "Epoch[9/10], Step [2900/3000], Loss: 0.0113\n",
      "Epoch[9/10], Step [3000/3000], Loss: 0.0409\n",
      "Epoch[10/10], Step [100/3000], Loss: 0.0039\n",
      "Epoch[10/10], Step [200/3000], Loss: 0.001\n",
      "Epoch[10/10], Step [300/3000], Loss: 0.012\n",
      "Epoch[10/10], Step [400/3000], Loss: 0.003\n",
      "Epoch[10/10], Step [500/3000], Loss: 0.0003\n",
      "Epoch[10/10], Step [600/3000], Loss: 0.0009\n",
      "Epoch[10/10], Step [700/3000], Loss: 0.0013\n",
      "Epoch[10/10], Step [800/3000], Loss: 0.0119\n",
      "Epoch[10/10], Step [900/3000], Loss: 0.0082\n",
      "Epoch[10/10], Step [1000/3000], Loss: 0.0108\n",
      "Epoch[10/10], Step [1100/3000], Loss: 0.0002\n",
      "Epoch[10/10], Step [1200/3000], Loss: 0.0021\n",
      "Epoch[10/10], Step [1300/3000], Loss: 0.0003\n",
      "Epoch[10/10], Step [1400/3000], Loss: 0.1124\n",
      "Epoch[10/10], Step [1500/3000], Loss: 0.0003\n",
      "Epoch[10/10], Step [1600/3000], Loss: 0.0047\n",
      "Epoch[10/10], Step [1700/3000], Loss: 0.1008\n",
      "Epoch[10/10], Step [1800/3000], Loss: 0.0057\n",
      "Epoch[10/10], Step [1900/3000], Loss: 0.006\n",
      "Epoch[10/10], Step [2000/3000], Loss: 0.0003\n",
      "Epoch[10/10], Step [2100/3000], Loss: 0.0544\n",
      "Epoch[10/10], Step [2200/3000], Loss: 0.0034\n",
      "Epoch[10/10], Step [2300/3000], Loss: 0.0406\n",
      "Epoch[10/10], Step [2400/3000], Loss: 0.0006\n",
      "Epoch[10/10], Step [2500/3000], Loss: 0.0108\n",
      "Epoch[10/10], Step [2600/3000], Loss: 0.0011\n",
      "Epoch[10/10], Step [2700/3000], Loss: 0.0006\n",
      "Epoch[10/10], Step [2800/3000], Loss: 0.0018\n",
      "Epoch[10/10], Step [2900/3000], Loss: 0.0001\n",
      "Epoch[10/10], Step [3000/3000], Loss: 0.0147\n",
      "Accuracy of the network no the 10,000 test images: 97.88%, with learning rate: 0.1, and [1568, 784] hidden neurons\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learning_rates = [0.05, 0.1]\n",
    "hidden_sizes = [[1568, 1568], [3136, 1568], [1568, 784]] # These values should all produce 97+% accuracy\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        total_step = len(train_loader)\n",
    "        # Define the model object and the optimizer\n",
    "        model = NeuralNet(input_size, hidden_size[0], hidden_size[1], num_classes).to(device)\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                # Move tensors to the configured device\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model.forward(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print progress every 100 steps\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {round(float(loss.item()), 4)}')\n",
    "\n",
    "    # Test the model once you finish training\n",
    "        with torch.no_grad(): # In test phase we don't need to compute gradients (for memory efficiency)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # get network outputs\n",
    "                outputs = model.forward(images)\n",
    "                throwaway, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            print(f\"Accuracy of the network no the 10,000 test images: {(100 * correct / total)}%, with learning rate: {learning_rate}, and {hidden_size} hidden neurons\")\n",
    "            print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally tested a range from 0.001 to 10 with order of magnitude increases for lr, 1578*1578, 784*1568, and 1568*784 hidden layer sizes with batch size of 20\n",
    "# 98.16%, 0.1, [1568,784]\n",
    "# 96.57%, 0.01, [1568,784]\n",
    "# 91.27%, 0.001, [1568,784]\n",
    "# 97.62%, 0.1, [784, 1568]\n",
    "# 96.53%, 0.01, [784, 1568]\n",
    "# 91.20%, 0.001, [784, 1568]\n",
    "# 98.09%, 0.1, [1568, 1568]\n",
    "# 96.89%, 0.01, [1568, 1568]\n",
    "# 91.24%, 0.001, [1568, 1568]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint for future use\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1707839a078309ba22a367600023c6395c025ee64f9c106cf0985467e3ca3301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

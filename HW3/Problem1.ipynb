{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T18:04:45.280526Z",
     "start_time": "2023-10-09T18:04:44.281383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x109895a90>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# This new configuration allows for the use of GPU acceleration on Apple Silicon Macs\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.has_mps:\n",
    "    device = 'mps'\n",
    "print(device)\n",
    "torch.manual_seed(189898) # Last 6 digits of my A# without the leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T18:04:45.399167Z",
     "start_time": "2023-10-09T18:04:45.278969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jagardiner/Desktop/STAT-6685/HW3\r\n"
     ]
    }
   ],
   "source": [
    "# Check your Current Working Directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T18:04:45.426092Z",
     "start_time": "2023-10-09T18:04:45.400794Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# Download the MNIST dataset to local drive. A new folder \"data\" will be created in teh current directory to store data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Use a data loader to shuffle and batch\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T18:04:45.429566Z",
     "start_time": "2023-10-09T18:04:45.424852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Network Architecture\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 6\n",
    "\n",
    "# Fully connected neural network with two hidden layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, in_size, h1, h2, n_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(h2, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Define the Loss Function and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T18:08:24.689396Z",
     "start_time": "2023-10-09T18:04:45.431352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/6], Step [100/1875], Loss: 0.7621\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.3125\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.4455\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.3216\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.5484\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.2955\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.18\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.4373\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.1879\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.1108\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.2354\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.2111\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.4518\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.3937\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.27\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.0849\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.2242\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.2007\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.129\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.1488\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.1768\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.2273\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.1346\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.074\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.122\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.1832\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.1178\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.0774\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.1474\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.2043\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.26\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.1834\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.0486\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.095\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.0688\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.0419\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1261\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.1394\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.1785\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.2644\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.3276\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.1585\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0868\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0839\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.1148\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.0447\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.117\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.0589\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.0951\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.1473\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.0766\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.0237\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.03\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.1233\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.1744\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.0584\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0261\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.1554\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.014\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.0426\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.078\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0299\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0827\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0211\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0702\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.0232\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.129\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.1879\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0055\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.0508\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.1228\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.0613\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0655\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.0957\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0674\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.091\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0228\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0506\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.0894\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0121\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.1827\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0258\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.1307\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.1431\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.1492\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.0484\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.093\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.2463\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.0536\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.2066\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.0539\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.081\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0104\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0812\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0166\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.1335\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0404\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0519\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.17\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0644\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0968\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0532\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.0303\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.026\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0285\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.2827\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0333\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0232\n",
      "Accuracy of the network for the 10,000 test images: 97.63%, with learning rate: 0.05, and [1568, 1568] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.3468\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.2455\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.2658\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.1609\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.2591\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.211\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.2589\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.3491\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.0888\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.3058\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.2074\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.1947\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.0379\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.1703\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.3879\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.1995\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.079\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.2159\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.1097\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.133\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.0441\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.3418\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.191\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.1361\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.1277\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.0271\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.0524\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.2315\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.0467\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.226\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.093\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.2464\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.0311\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.0688\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.0433\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.1599\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1121\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.0382\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.0519\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.0613\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.1472\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.0089\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0085\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0582\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.0232\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.2288\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.0914\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.1241\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.0589\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.0545\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.1784\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.1154\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0154\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.2201\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.1383\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.0336\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0197\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0159\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0193\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.2156\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0116\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0604\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0225\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0179\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0561\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.1257\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.0629\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.0205\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0253\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.0483\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.0498\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.1991\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0043\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.0465\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.1133\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0107\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0069\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.008\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.1683\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0106\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0198\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0826\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.0401\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.0712\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0734\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.0408\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0323\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.2148\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.2138\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.0527\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.004\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0843\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0038\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0072\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.015\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.0642\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0089\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0053\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0111\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0198\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0062\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0088\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.0224\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0739\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.7154\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0132\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0284\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.1391\n",
      "Accuracy of the network for the 10,000 test images: 97.66%, with learning rate: 0.1, and [1568, 1568] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.8158\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.4614\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.3932\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.5542\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.2791\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.2896\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.3429\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.2316\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.2619\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.1622\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.0863\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.1301\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.2005\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.2723\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.142\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.2863\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.1459\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.1475\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.1356\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.1481\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.2645\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.0718\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.2085\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.2267\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.0941\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.1934\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.1011\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.1097\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.2266\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.4094\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.0408\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.1329\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.1868\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.079\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.1789\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.1572\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1869\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.361\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.1003\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.087\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.0319\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.1738\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0849\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0572\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.1095\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.1311\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.0716\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.2181\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.1027\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.0485\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.0086\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.2208\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.157\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.1107\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.0893\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.1235\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.1414\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0157\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0551\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.0118\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0846\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0269\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0138\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0849\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.1298\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.0569\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.0666\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.0411\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0153\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.1542\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.0145\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.0266\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0581\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.0181\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0116\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0996\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.2094\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0567\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.0208\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0701\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0331\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0512\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.0362\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.2393\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0898\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.181\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0422\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.0437\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.0085\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.0496\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.1154\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0208\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0171\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0127\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0272\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.1021\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0129\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0126\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0201\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0349\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.045\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.1316\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.2614\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0133\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0116\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0569\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0404\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0095\n",
      "Accuracy of the network for the 10,000 test images: 97.45%, with learning rate: 0.05, and [3136, 1568] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.4878\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.2244\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.2753\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.2542\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.1433\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.1735\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.5238\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.2774\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.0762\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.1617\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.428\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.1449\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.2531\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.1225\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.1168\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.3292\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.108\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.1279\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.2803\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.4255\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.0416\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.0306\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.0587\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.2005\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.1244\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.0248\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.2272\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.0815\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.071\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.1482\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.1727\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.1029\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.3176\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.0922\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.0159\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.2236\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1818\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.0079\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.0588\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.0737\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.102\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.1019\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0373\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0164\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.0534\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.0694\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.0551\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.0222\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.0416\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.0998\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.0404\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.0189\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0117\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.0398\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.0162\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.0279\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.023\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0053\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0247\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.0279\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.1661\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0069\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0051\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0248\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0159\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.0587\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.0425\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.0238\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0064\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.0748\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.2521\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.0184\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0078\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.0124\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0169\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0798\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0068\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0359\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.0059\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0983\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0827\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0251\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.02\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.0078\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0265\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.0323\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0272\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.0188\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.238\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.0025\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.0605\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0628\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0648\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0946\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.01\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.0164\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0037\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0117\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0121\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0379\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0039\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0136\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.0035\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0291\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0019\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0035\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0058\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0135\n",
      "Accuracy of the network for the 10,000 test images: 98.07%, with learning rate: 0.1, and [3136, 1568] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.7822\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.4952\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.7039\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.4163\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.2974\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.2612\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.7066\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.3663\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.3982\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.6043\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.3682\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.53\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.2976\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.2378\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.2401\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.1819\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.2833\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.1582\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.2761\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.1744\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.1869\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.3326\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.1355\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.114\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.257\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.3057\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.2567\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.0976\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.1982\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.0552\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.2242\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.2992\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.1666\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.3504\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.1316\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.0573\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1585\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.2295\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.0757\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.2524\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.2892\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.0795\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.156\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.3099\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.3098\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.077\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.2973\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.2213\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.2132\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.146\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.1252\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.0799\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0603\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.0632\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.0127\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.0525\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0294\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0427\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0758\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.1671\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0951\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0719\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.1043\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.1821\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0635\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.1492\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.1405\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.2797\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.1898\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.1095\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.0666\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.071\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.2057\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.1448\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0255\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0526\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0281\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0599\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.1429\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.2392\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0256\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0706\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.0232\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.1494\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0338\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.1328\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0475\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.0681\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.0797\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.0134\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.1117\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0212\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0629\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.3203\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0113\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.032\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0706\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0079\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0405\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0065\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0261\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0275\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.1542\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0385\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0085\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0724\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0182\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0976\n",
      "Accuracy of the network for the 10,000 test images: 97.36%, with learning rate: 0.05, and [1568, 784] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.6979\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.4031\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.224\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.1085\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.1839\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.2159\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.1219\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.1895\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.2013\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.1777\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.1356\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.2585\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.2103\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.1208\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.2074\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.0843\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.0531\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.3609\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.0699\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.026\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.0622\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.025\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.122\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.166\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.0828\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.1227\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.2236\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.0508\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.3534\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.1369\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.0878\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.0392\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.2029\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.1503\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.1111\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.2754\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.0161\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.0325\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.1771\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.2063\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.0639\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.0607\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0361\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0822\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.0291\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.1355\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.0091\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.2582\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.0548\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.2677\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.1538\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.0257\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0394\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.0673\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.0079\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.1566\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0603\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0061\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.101\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.0064\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0251\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0744\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0181\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0283\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0216\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.0985\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.1244\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.3154\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0501\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.0287\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.1194\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.1497\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0678\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.228\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0212\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.01\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0215\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0419\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.0165\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0953\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0188\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0232\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.0653\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.0127\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0088\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.1578\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.1376\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.0183\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.0056\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.1474\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.016\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0076\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0239\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0085\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0252\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.0133\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0029\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0286\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0478\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0144\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0292\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0444\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.069\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.014\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0045\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0145\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.1601\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0747\n",
      "Accuracy of the network for the 10,000 test images: 97.74%, with learning rate: 0.1, and [1568, 784] hidden neurons\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learning_rates = [0.05, 0.1]\n",
    "hidden_sizes = [[1568, 1568], [3136, 1568], [1568, 784]] # These values should all produce 97+% accuracy\n",
    "best_performer = 0\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        total_step = len(train_loader)\n",
    "        # Define the model object and the optimizer\n",
    "        model = NeuralNet(input_size, hidden_size[0], hidden_size[1], num_classes).to(device)\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                # Move tensors to the configured device\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model.forward(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print progress every 100 steps\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {round(float(loss.item()), 4)}')\n",
    "\n",
    "    # Test the model once you finish training\n",
    "        with torch.no_grad(): # In test phase we don't need to compute gradients (for memory efficiency)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # get network outputs\n",
    "                outputs = model.forward(images)\n",
    "                throwaway, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if correct > best_performer:\n",
    "                # Save the best performing model for future use\n",
    "                best_performer = correct\n",
    "                torch.save(model.state_dict(), 'model.ckpt')\n",
    "            print(f\"Accuracy of the network for the 10,000 test images: {(100 * correct / total)}%, with learning rate: {learning_rate}, and {hidden_size} hidden neurons\")\n",
    "            print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T18:08:24.691744Z",
     "start_time": "2023-10-09T18:08:24.688066Z"
    }
   },
   "outputs": [],
   "source": [
    "# originally tested a range from 0.001 to 10 with order of magnitude increases for lr, 1578*1578, 784*1568, and 1568*784 hidden layer sizes with batch size of 20 and trained for 10 epochs\n",
    "# For lr >= 1, loss=nan, result=random guessing\n",
    "# 98.16%, 0.1, [1568,784]\n",
    "# 96.57%, 0.01, [1568,784]\n",
    "# 91.27%, 0.001, [1568,784]\n",
    "# 97.62%, 0.1, [784, 1568]\n",
    "# 96.53%, 0.01, [784, 1568]\n",
    "# 91.20%, 0.001, [784, 1568]\n",
    "# 98.09%, 0.1, [1568, 1568]\n",
    "# 96.89%, 0.01, [1568, 1568]\n",
    "# 91.24%, 0.001, [1568, 1568]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1707839a078309ba22a367600023c6395c025ee64f9c106cf0985467e3ca3301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T19:53:55.524675Z",
     "start_time": "2023-10-13T19:53:55.510136600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1f2419e5ef0>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# This new configuration allows for the use of GPU acceleration on Apple Silicon Macs\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.has_mps:\n",
    "    device = 'mps'\n",
    "print(device)\n",
    "torch.manual_seed(189898) # Last 6 digits of my A# without the leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T19:53:55.584616600Z",
     "start_time": "2023-10-13T19:53:55.519696500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Check your Current Working Directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T19:53:55.584616600Z",
     "start_time": "2023-10-13T19:53:55.537033400Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# Download the MNIST dataset to local drive. A new folder \"data\" will be created in teh current directory to store data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Use a data loader to shuffle and batch\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T19:53:55.592394800Z",
     "start_time": "2023-10-13T19:53:55.584616600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Network Architecture\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 6\n",
    "\n",
    "# Fully connected neural network with two hidden layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, in_size, h1, h2, n_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(h2, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Define the Loss Function and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T19:59:55.370509500Z",
     "start_time": "2023-10-13T19:53:55.592394800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/6], Step [100/1875], Loss: 0.7621\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.3125\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.4455\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.3216\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.5485\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.2954\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.1801\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.4372\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.1876\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.1108\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.2353\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.2113\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.4513\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.3942\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.2696\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.0851\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.2241\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.2006\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.1291\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.1492\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.1766\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.2276\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.1348\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.0735\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.122\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.1828\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.1178\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.078\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.1473\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.2041\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.2596\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.1832\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.049\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.0954\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.0695\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.0418\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1254\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.1393\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.1791\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.2638\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.3266\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.1595\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0859\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0833\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.1149\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.0453\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.1171\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.0593\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.096\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.1467\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.0764\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.0235\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0301\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.1222\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.1734\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.0582\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0263\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.1555\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0139\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.0425\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0781\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0302\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0827\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.021\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.071\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.0225\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.1288\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.1889\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0057\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.0503\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.1229\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.0604\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0659\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.0982\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0669\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.091\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.023\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0508\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.0894\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0121\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.181\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0261\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.1304\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.1426\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.1504\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.0494\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0935\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.2438\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.0538\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.2047\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.0544\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0821\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0103\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.081\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0167\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.1309\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0418\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0528\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.1692\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0652\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0946\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0521\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.0312\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0249\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0288\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.2843\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0344\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0242\n",
      "Accuracy of the network for the 10,000 test images: 97.68%, with learning rate: 0.05, and [1568, 1568] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.3468\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.2455\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.2658\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.1611\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.2585\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.211\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.2589\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.3488\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.0887\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.3065\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.2072\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.1942\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.0378\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.17\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.3867\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.1966\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.0792\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.2172\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.1123\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.132\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.0437\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.3411\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.1911\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.1361\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.1269\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.0272\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.0543\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.2293\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.0469\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.2236\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.0861\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.2477\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.031\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.0677\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.043\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.1585\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1148\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.0388\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.0504\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.0615\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.1457\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.0091\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0086\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0571\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.0235\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.2133\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.0915\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.1181\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.057\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.0543\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.1792\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.1237\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0159\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.2221\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.1383\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.0341\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0203\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0154\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0182\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.216\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0119\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0599\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0258\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0185\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0554\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.1262\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.0655\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.0196\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0253\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.0479\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.0489\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.1947\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0044\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.049\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.1212\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0103\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0069\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.008\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.1673\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0118\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0185\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0773\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.0361\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.074\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0747\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.0489\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0334\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.2209\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.2278\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.0619\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.0038\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0893\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0039\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0077\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0151\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.0683\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0088\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0052\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0108\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0207\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0069\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0096\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.0228\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0704\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.7104\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0134\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0286\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.1368\n",
      "Accuracy of the network for the 10,000 test images: 97.59%, with learning rate: 0.1, and [1568, 1568] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.8158\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.4615\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.3934\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.5543\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.279\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.2897\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.3427\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.2316\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.2612\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.1617\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.086\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.1305\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.2012\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.2721\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.1418\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.2859\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.1452\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.1471\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.1352\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.1483\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.265\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.0726\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.2093\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.2249\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.0943\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.1933\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.1005\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.1094\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.2273\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.408\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.041\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.1328\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.1866\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.0789\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.1789\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.1566\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1898\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.359\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.1007\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.0874\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.0318\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.1721\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0842\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0573\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.1083\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.131\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.0724\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.219\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.1036\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.0483\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.0085\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.2222\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.1568\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.1113\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.0898\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.1236\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.1431\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0157\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0556\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.0122\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0847\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0274\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0137\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0849\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.1304\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.0576\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.065\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.0417\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0157\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.155\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.0144\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.0268\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.058\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.0176\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0114\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0985\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.2054\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0571\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.0205\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0689\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0325\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0506\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.0353\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.2368\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0884\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.1822\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0422\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.0439\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.0083\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.049\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.1141\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0218\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0176\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0128\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.027\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.1017\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0132\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0135\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0202\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.034\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0437\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.1291\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.264\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0129\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0118\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.057\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0397\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0098\n",
      "Accuracy of the network for the 10,000 test images: 97.48%, with learning rate: 0.05, and [3136, 1568] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.4878\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.2244\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.2753\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.2538\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.1433\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.1731\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.5243\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.2772\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.0762\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.1626\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.4273\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.1456\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.2524\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.1218\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.1155\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.3288\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.1073\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.1264\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.2811\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.424\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.0417\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.0308\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.0593\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.2021\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.1225\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.0247\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.2249\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.0829\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.0703\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.149\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.1691\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.1064\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.3117\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.0948\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.0156\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.2265\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.1778\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.0076\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.0585\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.0726\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.1029\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.1019\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0358\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0153\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.0537\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.073\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.056\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.0235\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.0409\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.0995\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.0385\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.02\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0119\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.0409\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.0166\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.0278\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0227\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0059\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0242\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.0284\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.1617\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0073\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0046\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0237\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0171\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.0599\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.042\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.0216\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0066\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.0736\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.2597\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.0183\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0083\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.0123\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0145\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0858\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0071\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0357\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.0062\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.1056\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0815\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0234\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.019\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.0073\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0236\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.0303\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0271\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.016\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.2464\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.0026\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.0611\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0672\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0659\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0988\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0135\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.0176\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0029\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.011\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0125\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0357\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0048\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0159\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.0029\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0253\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0021\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0042\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0068\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0133\n",
      "Accuracy of the network for the 10,000 test images: 98.0%, with learning rate: 0.1, and [3136, 1568] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.7822\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.4952\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.7039\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.4163\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.2975\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.2613\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.7065\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.3663\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.398\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.6046\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.3682\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.5301\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.2976\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.2376\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.2405\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.1823\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.2834\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.1586\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.2758\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.1744\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.187\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.3326\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.1358\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.1143\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.2574\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.3046\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.2563\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.0976\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.1982\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.0548\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.2237\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.3001\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.1666\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.3507\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.1315\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.0574\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.159\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.2275\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.0759\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.253\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.2873\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.0792\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.1554\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.3106\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.3087\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.0775\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.295\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.222\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.212\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.1459\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.1239\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.0794\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0598\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.0623\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.0125\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.0523\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0293\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0424\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.0779\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.1627\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0944\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0721\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.1046\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.1809\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0645\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.1487\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.1403\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.2726\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.1927\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.1108\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.0655\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.0684\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.2056\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.1427\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.026\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0537\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0279\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0596\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.1432\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.2449\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0258\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0704\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.0231\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.1471\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0324\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.1307\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.0461\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.0695\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.0801\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.0133\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.1115\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.0212\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0615\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.3263\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0117\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.0313\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0678\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0079\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0403\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.0065\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.027\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0263\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.1563\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0377\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0085\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0724\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.0183\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0984\n",
      "Accuracy of the network for the 10,000 test images: 97.4%, with learning rate: 0.05, and [1568, 784] hidden neurons\n",
      "\n",
      "\n",
      "Epoch[1/6], Step [100/1875], Loss: 0.6979\n",
      "Epoch[1/6], Step [200/1875], Loss: 0.4031\n",
      "Epoch[1/6], Step [300/1875], Loss: 0.224\n",
      "Epoch[1/6], Step [400/1875], Loss: 0.1085\n",
      "Epoch[1/6], Step [500/1875], Loss: 0.184\n",
      "Epoch[1/6], Step [600/1875], Loss: 0.2167\n",
      "Epoch[1/6], Step [700/1875], Loss: 0.1217\n",
      "Epoch[1/6], Step [800/1875], Loss: 0.19\n",
      "Epoch[1/6], Step [900/1875], Loss: 0.2011\n",
      "Epoch[1/6], Step [1000/1875], Loss: 0.1771\n",
      "Epoch[1/6], Step [1100/1875], Loss: 0.132\n",
      "Epoch[1/6], Step [1200/1875], Loss: 0.258\n",
      "Epoch[1/6], Step [1300/1875], Loss: 0.2098\n",
      "Epoch[1/6], Step [1400/1875], Loss: 0.1195\n",
      "Epoch[1/6], Step [1500/1875], Loss: 0.2079\n",
      "Epoch[1/6], Step [1600/1875], Loss: 0.0859\n",
      "Epoch[1/6], Step [1700/1875], Loss: 0.0537\n",
      "Epoch[1/6], Step [1800/1875], Loss: 0.3585\n",
      "Epoch[2/6], Step [100/1875], Loss: 0.0681\n",
      "Epoch[2/6], Step [200/1875], Loss: 0.0261\n",
      "Epoch[2/6], Step [300/1875], Loss: 0.0626\n",
      "Epoch[2/6], Step [400/1875], Loss: 0.0244\n",
      "Epoch[2/6], Step [500/1875], Loss: 0.122\n",
      "Epoch[2/6], Step [600/1875], Loss: 0.1637\n",
      "Epoch[2/6], Step [700/1875], Loss: 0.0843\n",
      "Epoch[2/6], Step [800/1875], Loss: 0.1224\n",
      "Epoch[2/6], Step [900/1875], Loss: 0.2246\n",
      "Epoch[2/6], Step [1000/1875], Loss: 0.0522\n",
      "Epoch[2/6], Step [1100/1875], Loss: 0.345\n",
      "Epoch[2/6], Step [1200/1875], Loss: 0.131\n",
      "Epoch[2/6], Step [1300/1875], Loss: 0.0886\n",
      "Epoch[2/6], Step [1400/1875], Loss: 0.037\n",
      "Epoch[2/6], Step [1500/1875], Loss: 0.2051\n",
      "Epoch[2/6], Step [1600/1875], Loss: 0.1473\n",
      "Epoch[2/6], Step [1700/1875], Loss: 0.1124\n",
      "Epoch[2/6], Step [1800/1875], Loss: 0.2732\n",
      "Epoch[3/6], Step [100/1875], Loss: 0.0165\n",
      "Epoch[3/6], Step [200/1875], Loss: 0.0336\n",
      "Epoch[3/6], Step [300/1875], Loss: 0.184\n",
      "Epoch[3/6], Step [400/1875], Loss: 0.2007\n",
      "Epoch[3/6], Step [500/1875], Loss: 0.0629\n",
      "Epoch[3/6], Step [600/1875], Loss: 0.0626\n",
      "Epoch[3/6], Step [700/1875], Loss: 0.0367\n",
      "Epoch[3/6], Step [800/1875], Loss: 0.0811\n",
      "Epoch[3/6], Step [900/1875], Loss: 0.0286\n",
      "Epoch[3/6], Step [1000/1875], Loss: 0.1325\n",
      "Epoch[3/6], Step [1100/1875], Loss: 0.0093\n",
      "Epoch[3/6], Step [1200/1875], Loss: 0.2612\n",
      "Epoch[3/6], Step [1300/1875], Loss: 0.0533\n",
      "Epoch[3/6], Step [1400/1875], Loss: 0.2736\n",
      "Epoch[3/6], Step [1500/1875], Loss: 0.1574\n",
      "Epoch[3/6], Step [1600/1875], Loss: 0.0268\n",
      "Epoch[3/6], Step [1700/1875], Loss: 0.0409\n",
      "Epoch[3/6], Step [1800/1875], Loss: 0.0724\n",
      "Epoch[4/6], Step [100/1875], Loss: 0.0082\n",
      "Epoch[4/6], Step [200/1875], Loss: 0.1486\n",
      "Epoch[4/6], Step [300/1875], Loss: 0.0662\n",
      "Epoch[4/6], Step [400/1875], Loss: 0.0059\n",
      "Epoch[4/6], Step [500/1875], Loss: 0.1087\n",
      "Epoch[4/6], Step [600/1875], Loss: 0.0062\n",
      "Epoch[4/6], Step [700/1875], Loss: 0.0229\n",
      "Epoch[4/6], Step [800/1875], Loss: 0.0702\n",
      "Epoch[4/6], Step [900/1875], Loss: 0.0194\n",
      "Epoch[4/6], Step [1000/1875], Loss: 0.0305\n",
      "Epoch[4/6], Step [1100/1875], Loss: 0.0192\n",
      "Epoch[4/6], Step [1200/1875], Loss: 0.0941\n",
      "Epoch[4/6], Step [1300/1875], Loss: 0.1262\n",
      "Epoch[4/6], Step [1400/1875], Loss: 0.3007\n",
      "Epoch[4/6], Step [1500/1875], Loss: 0.0481\n",
      "Epoch[4/6], Step [1600/1875], Loss: 0.0271\n",
      "Epoch[4/6], Step [1700/1875], Loss: 0.1278\n",
      "Epoch[4/6], Step [1800/1875], Loss: 0.1502\n",
      "Epoch[5/6], Step [100/1875], Loss: 0.0653\n",
      "Epoch[5/6], Step [200/1875], Loss: 0.2119\n",
      "Epoch[5/6], Step [300/1875], Loss: 0.0189\n",
      "Epoch[5/6], Step [400/1875], Loss: 0.0098\n",
      "Epoch[5/6], Step [500/1875], Loss: 0.0199\n",
      "Epoch[5/6], Step [600/1875], Loss: 0.0404\n",
      "Epoch[5/6], Step [700/1875], Loss: 0.0173\n",
      "Epoch[5/6], Step [800/1875], Loss: 0.0951\n",
      "Epoch[5/6], Step [900/1875], Loss: 0.0201\n",
      "Epoch[5/6], Step [1000/1875], Loss: 0.0238\n",
      "Epoch[5/6], Step [1100/1875], Loss: 0.0743\n",
      "Epoch[5/6], Step [1200/1875], Loss: 0.0132\n",
      "Epoch[5/6], Step [1300/1875], Loss: 0.0086\n",
      "Epoch[5/6], Step [1400/1875], Loss: 0.1663\n",
      "Epoch[5/6], Step [1500/1875], Loss: 0.1371\n",
      "Epoch[5/6], Step [1600/1875], Loss: 0.0172\n",
      "Epoch[5/6], Step [1700/1875], Loss: 0.0056\n",
      "Epoch[5/6], Step [1800/1875], Loss: 0.1574\n",
      "Epoch[6/6], Step [100/1875], Loss: 0.018\n",
      "Epoch[6/6], Step [200/1875], Loss: 0.006\n",
      "Epoch[6/6], Step [300/1875], Loss: 0.0273\n",
      "Epoch[6/6], Step [400/1875], Loss: 0.0082\n",
      "Epoch[6/6], Step [500/1875], Loss: 0.0245\n",
      "Epoch[6/6], Step [600/1875], Loss: 0.0153\n",
      "Epoch[6/6], Step [700/1875], Loss: 0.0021\n",
      "Epoch[6/6], Step [800/1875], Loss: 0.0282\n",
      "Epoch[6/6], Step [900/1875], Loss: 0.0379\n",
      "Epoch[6/6], Step [1000/1875], Loss: 0.013\n",
      "Epoch[6/6], Step [1100/1875], Loss: 0.0263\n",
      "Epoch[6/6], Step [1200/1875], Loss: 0.0419\n",
      "Epoch[6/6], Step [1300/1875], Loss: 0.0489\n",
      "Epoch[6/6], Step [1400/1875], Loss: 0.0147\n",
      "Epoch[6/6], Step [1500/1875], Loss: 0.0036\n",
      "Epoch[6/6], Step [1600/1875], Loss: 0.0171\n",
      "Epoch[6/6], Step [1700/1875], Loss: 0.1642\n",
      "Epoch[6/6], Step [1800/1875], Loss: 0.0831\n",
      "Accuracy of the network for the 10,000 test images: 97.62%, with learning rate: 0.1, and [1568, 784] hidden neurons\n",
      "\n",
      "\n",
      "\n",
      "Best Parameters: hidden_size=[3136, 1568], lr=0.1\n",
      "Best Accuracy: 98.0\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learning_rates = [0.05, 0.1]\n",
    "hidden_sizes = [[1568, 1568], [3136, 1568], [1568, 784]] # These values should all produce 97+% accuracy\n",
    "best_performer = 0\n",
    "best_parameters = []\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        total_step = len(train_loader)\n",
    "        # Define the model object and the optimizer\n",
    "        model = NeuralNet(input_size, hidden_size[0], hidden_size[1], num_classes).to(device)\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                # Move tensors to the configured device\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model.forward(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print progress every 100 steps\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {round(float(loss.item()), 4)}')\n",
    "\n",
    "    # Test the model once you finish training\n",
    "        with torch.no_grad(): # In test phase we don't need to compute gradients (for memory efficiency)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # get network outputs\n",
    "                outputs = model.forward(images)\n",
    "                throwaway, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if correct > best_performer:\n",
    "                # Save the best performing model for future use\n",
    "                best_performer = correct\n",
    "                torch.save(model.state_dict(), 'model.ckpt')\n",
    "                best_parameters = [hidden_size, learning_rate]\n",
    "            print(f\"Accuracy of the network for the 10,000 test images: {(100 * correct / total)}%, with learning rate: {learning_rate}, and {hidden_size} hidden neurons\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "print(f\"Best Parameters: hidden_size={best_parameters[0]}, lr={best_parameters[1]}\")\n",
    "print(f\"Best Accuracy: {100 * best_performer / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T19:59:55.373600700Z",
     "start_time": "2023-10-13T19:59:55.368943200Z"
    }
   },
   "outputs": [],
   "source": [
    "# originally tested a range from 0.001 to 10 with order of magnitude increases for lr, 1578*1578, 784*1568, and 1568*784 hidden layer sizes with batch size of 20 and trained for 10 epochs\n",
    "# For lr >= 1, loss=nan, result=random guessing\n",
    "# 98.16%, 0.1, [1568,784]\n",
    "# 96.57%, 0.01, [1568,784]\n",
    "# 91.27%, 0.001, [1568,784]\n",
    "# 97.62%, 0.1, [784, 1568]\n",
    "# 96.53%, 0.01, [784, 1568]\n",
    "# 91.20%, 0.001, [784, 1568]\n",
    "# 98.09%, 0.1, [1568, 1568]\n",
    "# 96.89%, 0.01, [1568, 1568]\n",
    "# 91.24%, 0.001, [1568, 1568]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1707839a078309ba22a367600023c6395c025ee64f9c106cf0985467e3ca3301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

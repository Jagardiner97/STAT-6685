{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12fa3676f30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.manual_seed(189898) # Last 6 digits of my A# without the leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Check your Current Working Directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:00, 26088148.72it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 8558651.25it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:00, 14189285.66it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 3440377.52it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Set Batch Size\n",
    "'''This is one of the main tuning parameters'''\n",
    "batch_size = 10\n",
    "\n",
    "# Download the MNIST dataset to local drive. A new folder \"data\" will be created in teh current directory to store data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Use a data loader to shuffle and batch\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# Network Architecture\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 100\n",
    "\n",
    "# Fully connected neural netowrk with two hidden layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, h1, h2, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        '''\n",
    "        Step - Define the N/w architecture. Use RELU Activation\n",
    "        '''\n",
    "        self.fc1 = nn.Linear(input_size, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(h2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Step - Forward Propagate through the layers as defined above. Fill in params in place of ...\n",
    "        '''\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self. fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Define the Loss Function and Optimizer\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "'''step - choose different learning ratees and store them in a list and observe the changes'''\n",
    "learning_rates = [...]\n",
    "'''Vary this number and observe the changes, define a list of possible values'''\n",
    "hidden_sizes = [[..., ...]]\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        total_step = len(train_loader)\n",
    "        # Define the model object and the optimizer\n",
    "        model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "        optimizer = ... # Step - Invoke an appropriate optimizer that takes a ...?\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                # Move tensors to the configured device\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                '''Step - Get Network outputs with forward propagation with current network weights'''\n",
    "                outputs = model(...)\n",
    "                '''Step - Get Loss by comparing outputs with True Labels after forward propagation'''\n",
    "                loss = criterion(..., ...)\n",
    "\n",
    "                # Backward and optimize\n",
    "                '''Step - ... below needs to be replaced with functions'''\n",
    "                '''Step - clear the gradients after each pass - Strongly recommended'''\n",
    "                optimizer.\n",
    "                '''Backpropagate the loss to calculate gradient for each weight'''\n",
    "                loss.\n",
    "                '''Update the weight using the learning rate'''\n",
    "                optimizer.\n",
    "\n",
    "                # Print progress every 100 steps\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f'Epoch[{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {round(float(loss.item()), 4)}')\n",
    "\n",
    "    # Test the model once you finish training\n",
    "        with torch.no_grad(): # In test phase we don't need to compute gradients (for memory efficiency)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                '''Step - Move images to device after appropriate reshaping'''\n",
    "\n",
    "                '''Step - Move labels to device'''\n",
    "\n",
    "                # get network outputs\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            print(f\"Accuracy of the network no the 10,000 test images: {(100 * correct / total)} %, with learning rate: {learning_rate}, and {hidden_size} hidden neurons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint for future use\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1707839a078309ba22a367600023c6395c025ee64f9c106cf0985467e3ca3301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T22:19:56.979711800Z",
     "start_time": "2023-10-11T22:19:56.309463300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37baa57740ceef08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T22:19:57.047940500Z",
     "start_time": "2023-10-11T22:19:56.986747600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = loadmat('mnist_49_3000.mat')\n",
    "X = mnist['x']\n",
    "y = mnist['y'][0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.T, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdcf1e19543ebb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T22:19:57.278510200Z",
     "start_time": "2023-10-11T22:19:57.262380600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 1)\n"
     ]
    }
   ],
   "source": [
    "# change -1 to 0 before fitting\n",
    "def converter(inp):\n",
    "    if inp > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "y_train_mod = np.reshape(np.array([converter(val) for val in y_train]), (len(y_train), 1))\n",
    "print(y_train_mod.shape)\n",
    "y_test_mod = np.array([converter(val) for val in y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9675e3557c94b59",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, lamb=10, num_iter=1000, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.lamb = lamb\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        '''Exercise - Define Sigmoid Function here. z is a vector'''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization - all zeros\n",
    "        '''Exercise - Initialize theat. What should be the dimension of theta?'''\n",
    "        self.theta = np.zeros((X.shape[1], 1))\n",
    "        loss = torch.nn.BCELoss()\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            '''Exercise - Implement x^T(theta)'''\n",
    "            z = np.matmul(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "\n",
    "            print(h.shape)\n",
    "            print(y.shape)\n",
    "\n",
    "            loss_step = loss(h, y)\n",
    "            grad = loss_step.backward()\n",
    "            \n",
    "            '''Exercise - [Critical Step] - Using the formula in question prompt define gradient in vector form'''\n",
    "            #gradient = -1 / ((1 + np.exp(-z)) ** 2) * (-np.exp(-z)) * X\n",
    "            \n",
    "            '''Exercise - [Critical Step] - Update theta using the gradient and learning rate'''\n",
    "            self.theta -= grad * self.theta\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13bdc77f2670956",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 1)\n",
      "(2250, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Austin Gardiner\\Desktop\\STAT-6685\\HW3\\Problem4.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin%20Gardiner/Desktop/STAT-6685/HW3/Problem4.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(h\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin%20Gardiner/Desktop/STAT-6685/HW3/Problem4.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Austin%20Gardiner/Desktop/STAT-6685/HW3/Problem4.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss_step \u001b[39m=\u001b[39m loss(h, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin%20Gardiner/Desktop/STAT-6685/HW3/Problem4.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m grad \u001b[39m=\u001b[39m loss_step\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Austin%20Gardiner/Desktop/STAT-6685/HW3/Problem4.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m'''Exercise - [Critical Step] - Using the formula in question prompt define gradient in vector form'''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Austin Gardiner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Austin Gardiner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 612\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Austin Gardiner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:3055\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3053\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3054\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m-> 3055\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39;49msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m   3056\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3057\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3058\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3059\u001b[0m     )\n\u001b[0;32m   3061\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "'''Exercise - Select and appropriate lambda parameter in ...'''\n",
    "model = LogisticRegression(lamb=0.01)\n",
    "\n",
    "%time model.fit(X_train, y_train_mod) #%time prints out the execution time for the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b4f1d4e5607aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "preds = model.predict(X_test, 0.5)\n",
    "(preds == y_test_mod).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

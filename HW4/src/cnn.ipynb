{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Pytorch packages\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "# Set the device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.has_mps:\n",
    "    device = 'mps'\n",
    "print(device)\n",
    "\n",
    "# Set the seed for torch\n",
    "torch.manual_seed(189898)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the Data\n",
    "mnist = torchvision.datasets.MNIST(root='../data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Initialize training set\n",
    "X_train = mnist.data\n",
    "y_train = mnist.targets\n",
    "\n",
    "# Initialize test set\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.float() / 255.0\n",
    "X_test = X_test.float() / 255.0\n",
    "\n",
    "# Reshape to have 4 channels\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42c50072b1636b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_example(X, y):\n",
    "    \"\"\"Plot the first 5 images and their labels in a row\"\"\"\n",
    "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
    "        plt.subplot(151 + i)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(y.item())\n",
    "\n",
    "plot_example(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abaf3f87fc459e4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1\n",
    "10 Pts: Design and train a CNN with at least two convolutional layers, each followed by a maxpooling layer, for the MNIST dataset. Use dropout and L2 regularization on the weights when training the network. Use standard stochastic gradient descent in this part of the problem. Use grid search to tune hyper-parameters. Include your code. Record your final test accuracy and give a description on how you designed the network and briefly on how you made your design choices (e.g., numbers of layers, initialization strategies, parameter tuning, adaptive learning rate or not, momentum or not, etc.). To get full credit, you will need to get an accuracy of at least 98%."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce6f9f18131a3887"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, c1=16, c2=32, l1=128, dropout=0.4):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional and Pooling Layers\n",
    "        self.convolution = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=c1, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=c1, out_channels=c2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Linear Layers and Dropout for Classification\n",
    "        self. classification = nn.Sequential(\n",
    "            nn.Linear(c2 * 7 * 7, l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(l1, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        preds = F.softmax(self.classification(x), dim=1)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3125b2cf354603bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add L1 regularization\n",
    "\n",
    "class RegularizedNetwork(NeuralNetClassifier):\n",
    "    def __init__(self, *args, lambda1=0.01, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lambda1 = lambda1\n",
    "    \n",
    "    \n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        loss = super().get_loss(y_pred, y_true, X=X, training=training)\n",
    "        loss += self.lambda1 * sum([w.abs().sum() for w in self.module_.parameters()])\n",
    "        return loss\n",
    "    \n",
    "    '''\n",
    "    *** In submission PDF, explain what this method is doing ***\n",
    "    '''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d5d8b5f763ac80c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "416c6ad3f02253be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if device != \"cuda\":\n",
    "    print(\"Warning: not using cuda, training performance will be impacted\")\n",
    "    print(f\"Current device is {device}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30046baf692894ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Here we define the RegularizedNet. Make sure you use nn.NLLLoss. Thus, you have to use a correct last activation\n",
    "in the forward method of your network\n",
    "\n",
    "We can specify different parameters such as learning rate (lr), our optimizar (start with standard SGD, in 4.3 we will\n",
    "try another ones), batch size etc.\n",
    "To define the arquitecture parameters for CNN write them as module__<name of your parameter> = ....\n",
    "\n",
    "Since we have to train it first with L2 regularization lambda1 should be equal to 0\n",
    "'''\n",
    "cnn = RegularizedNetwork(module = CNN,\n",
    "                         module__c1 = 16,\n",
    "                         module__c2 = 32,\n",
    "                         module__l1 = 128,\n",
    "                         max_epochs = 20,\n",
    "                         criterion = torch.nn.NLLLoss, \n",
    "                         optimizer = torch.optim.SGD,\n",
    "                         lr = 0.2, \n",
    "                         lambda1 = 0,\n",
    "                         module__dropout = 0.4,\n",
    "                         optimizer__weight_decay = 0,\n",
    "                         device = device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f49befef6767b5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the network\n",
    "cnn.fit(X_train, y_train)\n",
    "y_pred_probs = cnn.predict(X_test)\n",
    "\n",
    "'''\n",
    "Observe the correlation between loss and validation accuracy\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78cf4384ec4a7036"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict for the test set and get accuracy\n",
    "y_pred = cnn.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "240feb76f87534e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can use cnn.get_params().keys() to get all tunable parameters\n",
    "tunable_parameters = cnn.get_params().keys()\n",
    "print(tunable_parameters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd863aca59cf6ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "grid = {\n",
    "    'lr': [0.1, 0.2, 0.5, 1],\n",
    "    'module__c1': [16, 20],\n",
    "    'module__c2': [32, 40],\n",
    "    'module__l1': [128, 256, 64],\n",
    "    'max_epochs': [5, 10, 20],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'module__dropout': [0.2, 0.4, 0.6, 0.8],\n",
    "    'optimizer__weight_decay': [0, 0.1, 0.95, 1]\n",
    "}\n",
    "gs = GridSearchCV(cnn, grid, refit=True, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the best model and report best score and parameters\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_, gs.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9562fb7806b15705"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2\n",
    "10 Pts: Starting with the network you designed in the previous problem, replace L2 regularization with L1 regularization and tune the regularization parameter as well as the learning rate. Explain what the get loss method is doing in the RegularizedNet class. Use two initialization strategies: \n",
    "1) initialize with the weights obtained using L2 regularization and \n",
    "2) initialize randomly. \n",
    " \n",
    " Which initialization strategy worked the best? Based on your results, which regularization worked best on this data?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8e5a81655f4b270"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 3\n",
    "10 Pts: Start with the best network you have obtained so far. Keeping the same hyperparameters of this network, you will now apply several different optimizers to retrain the network. \n",
    "(a) Use the following approaches to optimization: 1) standard SGD; 2) SGD with momentum; 3) AdaGrad; 4) Adam. Tune any of the associated parameters including the global learning rate using the validation accuracy. Report the final parameters selected in each case, and the final test accuracy in each case. Provide two plots with the results from all four approaches: 1) the training cost vs the number of epochs and 2) the validation accuracy vs the number of epochs. Which optimization approach seems to be working the best and why?\n",
    "(b) Pick one of the optimization approaches above. Using the same network, apply batch normalization to each of the hidden layers and retune the (global) learning rate using the validation accuracy. Report the new learning rate and the final test accuracy. Does batch normalization seem to help in this case?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48acc1ea78aaa6d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "from PIL import Image, ImageFilter \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "*** Explain ***\n",
    "What does the following class do, and why we need to do this? \n",
    "'''\n",
    "\n",
    "class mnist_double():\n",
    "    def __init__(self, gausBl = 1, Rota = 30, resh_size = 14):\n",
    "        \n",
    "        ''' Import MNIST\n",
    "        '''\n",
    "        # If you don't download, specify the repo using the following line\n",
    "        # data_path = r\"...your repo for the data\" \n",
    "        Mnist = torchvision.datasets.MNIST(root = '/data', \n",
    "                                           transform = torchvision.transforms.ToTensor(),\n",
    "                                           download = True)\n",
    "        \n",
    "        data = Mnist.data.reshape(60000,784)\n",
    "        train_X = data\n",
    "        train_X = train_X/train_X.max()\n",
    "        true_labels = Mnist.targets\n",
    "        \n",
    "        indc = np.random.choice(range(data.shape[0]), 2000, replace = False)\n",
    "\n",
    "        X = train_X[indc,:]\n",
    "        true_labels = true_labels[indc]\n",
    "        \n",
    "        images_transform = torch.zeros(X.shape[0], 28, 28)\n",
    "        for ind in range(len(X)):\n",
    "            transform = transforms.ToPILImage()\n",
    "            pilimg = transform(X[ind].reshape(28,28))\n",
    "            im2 = pilimg.filter(ImageFilter.GaussianBlur(radius = gausBl))\n",
    "            im2 = im2.rotate(Rota)\n",
    "            transform = transforms.ToTensor()\n",
    "            images_transform[ind] = transform(im2).squeeze()\n",
    "        \n",
    "        images_transform = images_transform.reshape(X.shape[0], 28 * 28)\n",
    "           \n",
    "        \n",
    "        if resh_size < 28:\n",
    "            images_transform_gb = images_transform.clone()\n",
    "            images_transform = torch.zeros(X.shape[0], resh_size, resh_size)        \n",
    "            p = transforms.Resize((resh_size, resh_size))\n",
    "            transform = transforms.ToPILImage()\n",
    "            transform_t = transforms.ToTensor()\n",
    "            for ind in range(len(X)):\n",
    "                images_transform[ind] = transform_t(p(transform(images_transform_gb[ind].reshape(28,28)))).squeeze()\n",
    "            images_transform = images_transform.reshape(X.shape[0], resh_size * resh_size) \n",
    "            \n",
    "        ins = np.argsort(true_labels)\n",
    "        self.domain1 = X[ins]\n",
    "        self.domain2 = images_transform[ins]\n",
    "        self.type1 = true_labels[ins]\n",
    "        self.type2 = true_labels[ins]\n",
    "        self.correspondence = True\n",
    "    def get_data(self):\n",
    "        return self.domain1, self.domain2,  self.type1, self.type2  \n",
    "    def get_embeddings(self):\n",
    "        return self.domain1[:,:2], self.domain2[:,:2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b28878b2d434411d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "domain1, domain2, labels1, labels2 = mnist_double(gausBl = 1).get_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36e28089587a9b71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "indices_plot = np.random.choice(range(domain1.shape[0]), 10, replace = False)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (10, 5))  \n",
    "ax = ax.flatten()\n",
    "for i in range(len(indices_plot)):\n",
    "    ax[i].imshow(domain1[indices_plot[i]].reshape(28,28))\n",
    "fig.suptitle(\"Original Images (domain1)\", size=20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (10, 5))  \n",
    "ax = ax.flatten()\n",
    "for i in range(len(indices_plot)):\n",
    "    ax[i].imshow(domain2[indices_plot[i]].reshape(14, 14))\n",
    "fig.suptitle(\"Transformed Images (domain2)\", size=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6817268d5c7984fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "*** Explain ***\n",
    "Looking at the following class, explain what is a generator?\n",
    "'''\n",
    "\n",
    "class MAGAN_generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        # pdb.set_trace()\n",
    "                \n",
    "        full_list = list(hidden_dims) \n",
    "        # Create generator \n",
    "        self.generator = nn.Sequential()\n",
    "        self.generator.add_module(f'gen_ini', nn.Linear(input_dim, full_list[0]))\n",
    "        # self.generator.add_module(f'batchn_', nn.BatchNorm1d(full_list[0]))\n",
    "        self.generator.add_module(f'act_',nn.LeakyReLU(negative_slope=0.2))\n",
    "        for i in range(len(full_list)-1):\n",
    "            self.generator.add_module(f'gen_{i}' ,nn.Linear(full_list[i], full_list[i+1]))\n",
    "            # self.generator.add_module(f'batchn_{i}', nn.BatchNorm1d(full_list[i+1]))\n",
    "            self.generator.add_module(f'act_{i}',nn.LeakyReLU(negative_slope=0.2))\n",
    "        \n",
    "        self.generator.add_module(f'final_gen', nn.Linear(full_list[i+1], output_dim))\n",
    "        self.generator.add_module('final_act', nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x1):\n",
    "        out = self.generator(x1)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "*** Explain ***\n",
    "Looking at the following class,explain what is a discriminator? \n",
    "Why the dimension of the output of the final layer is equal to 1? \n",
    "'''\n",
    "\n",
    "class MAGAN_discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        full_list = list(hidden_dims) \n",
    "        \n",
    "        # Create discriminator \n",
    "        self.discriminator = nn.Sequential()\n",
    "        self.discriminator.add_module('gen_ini', nn.Linear(input_dim, full_list[0]))\n",
    "        # self.discriminator.add_module('batchn_ini', nn.BatchNorm1d(full_list[0]))\n",
    "        self.discriminator.add_module('act_ini',nn.LeakyReLU(negative_slope=0.2))\n",
    "        for i in range(len(full_list)-1):\n",
    "            self.discriminator.add_module(f'gen_{i}' ,nn.Linear(full_list[i], full_list[i+1]))\n",
    "            # self.discriminator.add_module(f'batchn_{i}', nn.BatchNorm1d(full_list[i+1]))\n",
    "            self.discriminator.add_module(f'act_{i}',nn.LeakyReLU(negative_slope=0.2))\n",
    "        \n",
    "        self.discriminator.add_module('final_gen', nn.Linear(full_list[i+1], 1))\n",
    "        # self.discriminator.add_module('final_act', nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x1):\n",
    "        out = self.discriminator(x1)\n",
    "        return out    \n",
    "\n",
    "class MAGAN_dis(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, hidden_dims):\n",
    "        super().__init__()     \n",
    "        \n",
    "        '''\n",
    "        *** Explain *** what are D1 and D2?\n",
    "        '''\n",
    "                                                                                                \n",
    "        self.D1 = MAGAN_discriminator(input_dim1, hidden_dims)\n",
    "        self.D2 = MAGAN_discriminator(input_dim2, hidden_dims)\n",
    "        \n",
    "    def forward(self, X1, X2, X21, X12):\n",
    "\n",
    "        Dx1 = self.D1(X1)\n",
    "        Dx21 = self.D1(X21)\n",
    "        Dx2 = self.D2(X2)\n",
    "        Dx12 = self.D2(X12) \n",
    "        return Dx1, Dx21, Dx2, Dx12\n",
    "\n",
    "\n",
    "class MAGAN_gen(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, hidden_dims):\n",
    "        super().__init__()     \n",
    "        \n",
    "        '''\n",
    "        *** Explain *** what are G12 and G21?\n",
    "        '''\n",
    "        \n",
    "        self.G12 = MAGAN_generator(input_dim1, hidden_dims, input_dim2)\n",
    "        self.G21 = MAGAN_generator(input_dim2, hidden_dims, input_dim1)\n",
    "\n",
    "    def forward(self, X1, X2, s1, s2):\n",
    "        '''\n",
    "        *** Explain ***\n",
    "        What does this function forward() do?\n",
    "        '''\n",
    "        \n",
    "        X12 = self.G12(X1)\n",
    "        X121 = self.G21(X12)\n",
    "        \n",
    "        X21 = self.G21(X2)\n",
    "        X212 = self.G12(X21)\n",
    "        \n",
    "        S12 = self.G12(s1)\n",
    "        S21 = self.G21(s2)\n",
    "        \n",
    "        return X12, X121, X21, X212, S12, S21 \n",
    "\n",
    "                                                                                \n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "\n",
    "\n",
    "class MAGAN():\n",
    "    def __init__(self, lam = 1000, \n",
    "                 epochs=100, \n",
    "                 hidden_dims = [400, 200, 100],\n",
    "                 torch_module = None,\n",
    "                 optimizer = None,\n",
    "                 lr = 1e-3,\n",
    "                 batch_size = 128,\n",
    "                 weight_decay = 0):\n",
    "        \n",
    "        self.torch_module = torch_module\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.dis_loss = nn.BCELoss(reduction = 'mean')\n",
    "        self.dis_loss = nn.BCEWithLogitsLoss()\n",
    "        self.gen_loss = nn.MSELoss(reduction = 'mean')\n",
    "        self.lam =  lam\n",
    "        \n",
    "    def fit(self, X1, X2, s1 = None, s2 = None):\n",
    "        \n",
    "        input_dim1 = X1.shape[1]\n",
    "        input_dim2 = X2.shape[1]\n",
    "        if self.torch_module is None:\n",
    "            self.torch_module_gen = MAGAN_gen(input_dim1 = input_dim1,\n",
    "                                              input_dim2 = input_dim2,\n",
    "                                              hidden_dims = self.hidden_dims)\n",
    "            self.torch_module_dis = MAGAN_dis(input_dim1 = input_dim1,\n",
    "                                              input_dim2 = input_dim2,\n",
    "                                              hidden_dims = self.hidden_dims)\n",
    "            # self.torch_module_gen.apply(weights_init_uniform)\n",
    "            # self.torch_module_dis.apply(weights_init_uniform)\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            self.optimizer_gen = torch.optim.Adam(self.torch_module_gen.parameters(),\n",
    "                                              lr=self.lr,\n",
    "                                              weight_decay=self.weight_decay)\n",
    "            self.optimizer_dis = torch.optim.Adam(self.torch_module_dis.parameters(),\n",
    "                                                  lr=self.lr,\n",
    "                                                  weight_decay=self.weight_decay)\n",
    "            \n",
    "            \n",
    "        self.X1 = torch.tensor(X1).float() \n",
    "        self.X2 = torch.tensor(X2).float() \n",
    "        s1 = torch.tensor(s1).float().to(self.device)\n",
    "        s2 = torch.tensor(s2).float().to(self.device) \n",
    "        self.torch_module_gen.to(self.device)\n",
    "        self.torch_module_dis.to(self.device)\n",
    "        self.loader = self.get_loader(self.X1, self.X2)\n",
    "        self.noiseb = 0\n",
    "        self.d1_l = [0]\n",
    "        self.d2_l = [0]\n",
    "        self.g1_l = [0]\n",
    "        self.g2_l = [0]\n",
    "        self.train_g = True\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 50 == 1:\n",
    "                print(f'Epoch {epoch} of self.epochs')\n",
    "                print(f\"{self.loss}\")\n",
    "                plt.plot(self.d1_l + self.d2_l, c = \"blue\")\n",
    "                # plt.plot(self.d2_l, c = \"green\")\n",
    "                plt.plot(self.g2_l + self.g1_l, c = \"black\")\n",
    "                # plt.plot(self.g1_l, c = \"red\")\n",
    "                plt.show()\n",
    "                self.noiseb = np.clip(self.noiseb * 0.5, 0, 1)\n",
    "            \n",
    "            if epoch > 10:\n",
    "                if self.d1_l[-1] + self.d2_l[-1] > 1:\n",
    "                    self.train_g = True\n",
    "                else:\n",
    "                    self.train_g = True\n",
    "            for batch in self.loader:\n",
    "                                          \n",
    "                X1, X2 = batch\n",
    "                X1 = X1.to(self.device)\n",
    "                X2 = X2.to(self.device)\n",
    "                # discriminator loss\n",
    "                self.train_generators(X1, X2, s1, s2, self.train_g)\n",
    "                self.train_discriminators(X1, X2, s1, s2) \n",
    "                self.loss = self.g1_l[-1] + self.g2_l[-1] + self.d1_l[-1] + self.d2_l[-1]\n",
    "                \n",
    "\n",
    "        \n",
    "        X12, X121, X21, X212, S12, S21 = self.torch_module_gen(self.X1.to(self.device), self.X2.to(self.device), s1, s2)\n",
    "        nbrs = NearestNeighbors(n_neighbors=X12.shape[0], algorithm='ball_tree').fit(X12.detach().cpu())\n",
    "        _, indices = nbrs.kneighbors(self.X2)\n",
    "        a2 = indices[:,0]\n",
    "        \n",
    "        N1 = self.X1.shape[0]\n",
    "        N2 = self.X2.shape[0]\n",
    "        T = np.zeros((N1, N2))\n",
    "        T[range(N1), a2] = 1  \n",
    "        \n",
    "        return T\n",
    "    \n",
    "    def train_discriminators(self, X1, X2, s1, s2):\n",
    "        self.optimizer_dis.zero_grad()\n",
    "        X12, X121, X21, X212, S12, S21 = self.torch_module_gen(X1, X2, s1, s2)\n",
    "        Dx1, Dx21, Dx2, Dx12 = self.torch_module_dis(X1, X2, X21, X12)\n",
    "        \n",
    "        '''\n",
    "        *** Explain ***\n",
    "        What does this function train_discriminators() do?\n",
    "        '''\n",
    "        \n",
    "        lossD1 = self.dis_loss(Dx1, torch.bernoulli(torch.ones_like(Dx1)*(1-self.noiseb)))\n",
    "        lossD2 = self.dis_loss(Dx2, torch.bernoulli(torch.ones_like(Dx2)*(1-self.noiseb)))\n",
    "        lossD1_f = self.dis_loss(Dx12, torch.bernoulli(torch.ones_like(Dx1)*self.noiseb))\n",
    "        lossD2_f = self.dis_loss(Dx21, torch.bernoulli(torch.ones_like(Dx1)*self.noiseb))\n",
    "                                \n",
    "        # lossD1 = self.dis_loss(Dx1, torch.ones_like(Dx1))\n",
    "        # lossD2 = self.dis_loss(Dx2, torch.ones_like(Dx2))\n",
    "        \n",
    "        # lossD1_f = self.dis_loss(Dx12, torch.zeros_like(Dx12))\n",
    "        # lossD2_f = self.dis_loss(Dx21, torch.zeros_like(Dx21))         \n",
    "        # lossD1 = F.binary_cross_entropy_with_logits(Dx1, torch.ones_like(Dx1))\n",
    "        # lossD2 = F.binary_cross_entropy_with_logits(Dx2, torch.ones_like(Dx2))       \n",
    "        # lossD1_f = F.binary_cross_entropy_with_logits(Dx12, torch.zeros_like(Dx12))\n",
    "        # lossD2_f = F.binary_cross_entropy_with_logits(Dx21, torch.zeros_like(Dx21))  \n",
    "        LossD =  lossD1 + lossD2 + lossD1_f + lossD2_f\n",
    "        LossD.backward()             \n",
    "        self.d1_l.append(lossD1.item() + lossD1_f.item())\n",
    "        self.d2_l.append(lossD2.item() + lossD2_f.item())\n",
    "        self.optimizer_dis.step()\n",
    "        \n",
    "    def train_generators(self, X1, X2, s1, s2, train):\n",
    "        self.optimizer_gen.zero_grad()\n",
    "        X12, X121, X21, X212, S12, S21 = self.torch_module_gen(X1, X2, s1, s2)\n",
    "        Dx1, Dx21, Dx2, Dx12 = self.torch_module_dis(X1, X2, X21, X12)\n",
    "        \n",
    "                \n",
    "        '''\n",
    "        *** Explain ***\n",
    "        What does this function train_generators() do?\n",
    "        '''\n",
    "        \n",
    "        lossG12 = self.dis_loss(Dx12, torch.ones_like(Dx12))\n",
    "        lossG21 = self.dis_loss(Dx21, torch.ones_like(Dx21))\n",
    "        \n",
    "        lossG12_r = self.gen_loss(X1, X121)\n",
    "        lossG21_r = self.gen_loss(X2, X212)\n",
    "        \n",
    "        self.g1_l.append(lossG12.item() + lossG12_r.item())\n",
    "        self.g2_l.append(lossG21.item() + lossG21_r.item())\n",
    "        # Correspondence Loss\n",
    "        C12 = self.gen_loss(s2, S12)\n",
    "        C21 = self.gen_loss(s1, S21)\n",
    "        \n",
    "        LossG =  lossG12 + lossG21 + lossG12_r + lossG21_r + self.lam*(C12 + C21)\n",
    "        LossG.backward()  \n",
    "        if train:\n",
    "            self.optimizer_gen.step()          \n",
    "    \n",
    "    def get_loader(self, X1, X2):\n",
    "        loader1 = torch.utils.data.DataLoader(ConcatDataset(X1, X2), batch_size=self.batch_size, shuffle=False) \n",
    "        return  loader1  \n",
    "    \n",
    "    def transform(self, x1, x2):\n",
    "        X12, X121, X21, X212, s1, s2 = self.torch_module_gen(x1.to(self.device), x2.to(self.device), x1.to(self.device), x2.to(self.device))\n",
    "        return X12.detach().cpu().numpy(), X21.detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75892ccee5543c97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create random correspondences between both domains\n",
    "indices_correspondence = np.random.choice(range(domain1.shape[0]), 100, replace = False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb9c0f4eb23dc9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MAGAN()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33d24c64f080c2e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(domain1, domain2, domain1[indices_correspondence], domain2[indices_correspondence])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baf753be602ff705"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x2g, x1g = model.transform(domain1, domain2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1740fea53529a2f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize = (10, 5))  \n",
    "ax = ax.flatten()\n",
    "for i in range(len(indices_plot)):\n",
    "    ax[i].imshow(domain1[indices_plot[i]].reshape(28,28))\n",
    "fig.suptitle(\"Original Images (domain1)\", size=20)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (10, 5))  \n",
    "ax = ax.flatten()\n",
    "for i in range(len(indices_plot)):\n",
    "    ax[i].imshow(x1g[indices_plot[i]].reshape(28, 28))\n",
    "fig.suptitle(\"Generated Images (domain2)\", size=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9e5270106018651"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize = (10, 5))  \n",
    "ax = ax.flatten()\n",
    "for i in range(len(indices_plot)):\n",
    "    ax[i].imshow(domain1[indices_plot[i]].reshape(28,28))\n",
    "fig.suptitle(\"Original Images (domain1)\", size=20)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize = (10, 5))  \n",
    "ax = ax.flatten()\n",
    "for i in range(len(indices_plot)):\n",
    "    ax[i].imshow(x1g[indices_plot[i]].reshape(28, 28))\n",
    "fig.suptitle(\"Generated Images (domain2)\", size=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f93d680472b113c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
